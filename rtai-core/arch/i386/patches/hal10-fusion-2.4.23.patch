diff -uNrp linux-2.4.23/CREDITS linux-2.4.23-fusion/CREDITS
--- linux-2.4.23/CREDITS	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/CREDITS	2004-01-11 15:29:36.000000000 +0100
@@ -999,8 +999,8 @@ S: Brazil
 
 N: Nigel Gamble
 E: nigel@nrg.org
-E: nigel@sgi.com
 D: Interrupt-driven printer driver
+D: Preemptible kernel
 S: 120 Alley Way
 S: Mountain View, California 94040
 S: USA
diff -uNrp linux-2.4.23/Documentation/Configure.help linux-2.4.23-fusion/Documentation/Configure.help
--- linux-2.4.23/Documentation/Configure.help	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/Documentation/Configure.help	2004-01-11 15:29:36.000000000 +0100
@@ -109,6 +109,23 @@ CONFIG_ADVANCED_OPTIONS
 
   Unless you know what you are doing you *should not* enable this option.
 
+Low latency scheduling
+CONFIG_LOLAT
+  This enables low latency scheduling, with reduces the scheduling
+  latency of the kernel.  This makes the kernel more responsive, and
+  potentially increases its bandwidth; since threads waste less time
+  waiting for execution.
+
+  If you don't know what to do here, say Y.
+
+Control low latency with sysctl
+CONFIG_LOLAT_SYSCTL
+  If you say Y here, you will be able to control low latency
+  scheduling using /proc/sys/kernel/lowlatency.  It will default
+  to '0': low latency disabled.
+
+  If you say N here, then low latency scheduling is always enabled.
+
 Symmetric Multi-Processing support
 CONFIG_SMP
   This enables support for systems with more than one CPU. If you have
@@ -296,6 +313,17 @@ CONFIG_X86_UP_APIC
   If you have a system with several CPUs, you do not need to say Y
   here: the local APIC will be used automatically.
 
+Preemptible Kernel
+CONFIG_PREEMPT
+  This option reduces the latency of the kernel when reacting to
+  real-time or interactive events by allowing a low priority process to
+  be preempted even if it is in kernel mode executing a system call.
+  This allows applications to run more reliably even when the system is
+  under load.
+
+  Say Y here if you are building a kernel for a desktop, embedded or
+  real-time system.  Say N if you are unsure.
+
 Kernel math emulation
 CONFIG_MATH_EMULATION
   Linux can emulate a math coprocessor (used for floating point
diff -uNrp linux-2.4.23/Documentation/adeos.txt linux-2.4.23-fusion/Documentation/adeos.txt
--- linux-2.4.23/Documentation/adeos.txt	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/Documentation/adeos.txt	2004-01-11 15:29:36.000000000 +0100
@@ -0,0 +1,176 @@
+
+The Adeos nanokernel is based on research and publications made in the
+early '90s on the subject of nanokernels. Our basic method was to
+reverse the approach described in most of the papers on the subject.
+Instead of first building the nanokernel and then building the client
+OSes, we started from a live and known-to-be-functional OS, Linux, and
+inserted a nanokernel beneath it. Starting from Adeos, other client
+OSes can now be put side-by-side with the Linux kernel.
+
+To this end, Adeos enables multiple domains to exist simultaneously on
+the same hardware. None of these domains see each other, but all of
+them see Adeos. A domain is most probably a complete OS, but there is
+no assumption being made regarding the sophistication of what's in
+a domain.
+
+To share the hardware among the different OSes, Adeos implements an
+interrupt pipeline (ipipe). Every OS domain has an entry in the ipipe.
+Each interrupt that comes in the ipipe is passed on to every domain
+in the ipipe. Instead of disabling/enabling interrupts, each domain
+in the pipeline only needs to stall/unstall his pipeline stage. If
+an ipipe stage is stalled, then the interrupts do not progress in the
+ipipe until that stage has been unstalled. Each stage of the ipipe
+can, of course, decide to do a number of things with an interrupt.
+Among other things, it can decide that it's the last recipient of the
+interrupt. In that case, the ipipe does not propagate the interrupt
+to the rest of the domains in the ipipe.
+
+Regardless of the operations being done in the ipipe, the Adeos code
+does __not__ play with the interrupt masks. The only case where the
+hardware masks are altered is during the addition/removal of a domain
+from the ipipe. This also means that no OS is allowed to use the real
+hardware cli/sti. But this is OK, since the stall/unstall calls
+achieve the same functionality.
+
+Our approach is based on the following papers (links to these
+papers are provided at the bottom of this message):
+[1] D. Probert, J. Bruno, and M. Karzaorman. "Space: a new approach to
+operating system abstraction." In: International Workshop on Object
+Orientation in Operating Systems, pages 133-137, October 1991.
+[2] D. Probert, J. Bruno. "Building fundamentally extensible application-
+specific operating systems in Space", March 1995.
+[3] D. Cheriton, K. Duda. "A caching model of operating system kernel
+functionality". In: Proc. Symp. on Operating Systems Design and
+Implementation, pages 179-194, Monterey CA (USA), 1994.
+[4] D. Engler, M. Kaashoek, and J. O'Toole Jr. "Exokernel: an operating
+system architecture for application-specific resource management",
+December 1995.
+
+If you don't want to go fetch the complete papers, here's a summary.
+The first 2 discuss the Space nanokernel, the 3rd discussed the cache
+nanokernel, and the last discusses exokernel.
+
+The complete Adeos approach has been thoroughly documented in a whitepaper
+published more than a year ago entitled "Adaptive Domain Environment
+for Operating Systems" and available here: http://www.opersys.com/adeos
+The current implementation is slightly different. Mainly, we do not
+implement the functionality to move Linux out of ring 0. Although of
+interest, this approach is not very portable.
+
+Instead, our patch taps right into Linux's main source of control
+over the hardware, the interrupt dispatching code, and inserts an
+interrupt pipeline which can then serve all the nanokernel's clients,
+including Linux.
+
+This is not a novelty in itself. Other OSes have been modified in such
+a way for a wide range of purposes. One of the most interesting
+examples is described by Stodolsky, Chen, and Bershad in a paper
+entitled "Fast Interrupt Priority Management in Operating System
+Kernels" published in 1993 as part of the Usenix Microkernels and
+Other Kernel Architectures Symposium. In that case, cli/sti were
+replaced by virtual cli/sti which did not modify the real interrupt
+mask in any way. Instead, interrupts were defered and delivered to
+the OS upon a call to the virtualized sti.
+
+Mainly, this resulted in increased performance for the OS. Although
+we haven't done any measurements on Linux's interrupt handling
+performance with Adeos, our nanokernel includes by definition the
+code implementing the technique described in the abovementioned
+Stodolsky paper, which we use to redirect the hardware interrupt flow
+to the pipeline.
+
+i386 and armnommu are currently supported. Most of the
+architecture-dependent code is easily portable to other architectures.
+
+Aside of adding the Adeos module (driver/adeos), we also modified some
+files to tap into Linux interrupt and system event dispatching (all
+the modifications are encapsulated in #ifdef CONFIG_ADEOS_*/#endif).
+
+We modified the idle task so it gives control back to Adeos in order for
+the ipipe to continue propagation.
+
+We modified init/main.c to initialize Adeos very early in the startup.
+
+Of course, we also added the appropriate makefile modifications and
+config options so that you can choose to enable/disable Adeos as
+part of the kernel build configuration.
+
+Adeos' public API is fully documented here:
+http://www.freesoftware.fsf.org/adeos/doc/api/index.html.
+
+In Linux's case, adeos_register_domain() is called very early during
+system startup.
+
+To add your domain to the ipipe, you need to:
+1) Register your domain with Adeos using adeos_register_domain()
+2) Call adeos_virtualize_irq() for all the IRQs you wish to be
+notified about in the ipipe.
+
+That's it. Provided you gave Adeos appropriate handlers in step
+#2, your interrupts will be delivered via the ipipe.
+
+During runtime, you may change your position in the ipipe using
+adeos_renice_domain(). You may also stall/unstall the pipeline
+and change the ipipe's handling of the interrupts according to your
+needs.
+
+Adeos supports SMP, and APIC support on UP.
+
+Here are some of the possible uses for Adeos (this list is far
+from complete):
+1) Much like User-Mode Linux, it should now be possible to have 2
+Linux kernels living side-by-side on the same hardware. In contrast
+to UML, this would not be 2 kernels one ontop of the other, but
+really side-by-side. Since Linux can be told at boot time to use
+only one portion of the available RAM, on a 128MB machine this
+would mean that the first could be made to use the 0-64MB space and
+the second would use the 64-128MB space. We realize that many
+modifications are required. Among other things, one of the 2 kernels
+will not need to conduct hardware initialization. Nevertheless, this
+possibility should be studied closer.
+
+2) It follows from #1 that adding other kernels beside Linux should
+be feasible. BSD is a prime candidate, but it would also be nice to
+see what virtualizers such as VMWare and Plex86 could do with Adeos.
+Proprietary operating systems could potentially also be accomodated.
+
+3) All the previous work that has been done on nanokernels should now
+be easily ported to Linux. Mainly, we would be very interested to
+hear about extensions to Adeos. Primarily, we have no mechanisms
+currently enabling multiple domains to share information. The papers
+mentioned earlier provide such mechanisms, but we'd like to see
+actual practical examples.
+
+4) Kernel debuggers' main problem (tapping into the kernel's
+interrupts) is solved and it should then be possible to provide
+patchless kernel debuggers. They would then become loadable kernel
+modules.
+
+5) Drivers who require absolute priority and dislike other kernel
+portions who use cli/sti can now create a domain of their own
+and place themselves before Linux in the ipipe. This provides a
+mechanism for the implementation of systems that can provide guaranteed
+realtime response.
+
+Philippe Gerum <rpm@xenomai.org>
+Karim Yaghmour <karim@opersys.com>
+
+----------------------------------------------------------------------
+Links to papers:
+1-
+http://citeseer.nj.nec.com/probert91space.html
+ftp://ftp.cs.ucsb.edu/pub/papers/space/iwooos91.ps.gz (not working)
+http://www4.informatik.uni-erlangen.de/~tsthiel/Papers/Space-iwooos91.ps.gz
+
+2-
+http://www.cs.ucsb.edu/research/trcs/abstracts/1995-06.shtml
+http://www4.informatik.uni-erlangen.de/~tsthiel/Papers/Space-trcs95-06.ps.gz
+
+3-
+http://citeseer.nj.nec.com/kenneth94caching.html
+http://guir.cs.berkeley.edu/projects/osprelims/papers/cachmodel-OSkernel.ps.gz
+
+4-
+http://citeseer.nj.nec.com/engler95exokernel.html
+ftp://ftp.cag.lcs.mit.edu/multiscale/exokernel.ps.Z
+----------------------------------------------------------------------
diff -uNrp linux-2.4.23/Documentation/preempt-locking.txt linux-2.4.23-fusion/Documentation/preempt-locking.txt
--- linux-2.4.23/Documentation/preempt-locking.txt	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/Documentation/preempt-locking.txt	2004-01-11 15:29:36.000000000 +0100
@@ -0,0 +1,104 @@
+		  Proper Locking Under a Preemptible Kernel:
+		       Keeping Kernel Code Preempt-Safe
+			  Robert Love <rml@tech9.net>
+			   Last Updated: 22 Jan 2002
+
+
+INTRODUCTION
+
+
+A preemptible kernel creates new locking issues.  The issues are the same as
+those under SMP: concurrency and reentrancy.  Thankfully, the Linux preemptible
+kernel model leverages existing SMP locking mechanisms.  Thus, the kernel
+requires explicit additional locking for very few additional situations.
+
+This document is for all kernel hackers.  Developing code in the kernel
+requires protecting these situations.
+ 
+
+RULE #1: Per-CPU data structures need explicit protection
+
+
+Two similar problems arise. An example code snippet:
+
+	struct this_needs_locking tux[NR_CPUS];
+	tux[smp_processor_id()] = some_value;
+	/* task is preempted here... */
+	something = tux[smp_processor_id()];
+
+First, since the data is per-CPU, it may not have explicit SMP locking, but
+require it otherwise.  Second, when a preempted task is finally rescheduled,
+the previous value of smp_processor_id may not equal the current.  You must
+protect these situations by disabling preemption around them.
+
+
+RULE #2: CPU state must be protected.
+
+
+Under preemption, the state of the CPU must be protected.  This is arch-
+dependent, but includes CPU structures and state not preserved over a context
+switch.  For example, on x86, entering and exiting FPU mode is now a critical
+section that must occur while preemption is disabled.  Think what would happen
+if the kernel is executing a floating-point instruction and is then preempted.
+Remember, the kernel does not save FPU state except for user tasks.  Therefore,
+upon preemption, the FPU registers will be sold to the lowest bidder.  Thus,
+preemption must be disabled around such regions.
+
+Note, some FPU functions are already explicitly preempt safe.  For example,
+kernel_fpu_begin and kernel_fpu_end will disable and enable preemption.
+However, math_state_restore must be called with preemption disabled.
+
+
+RULE #3: Lock acquire and release must be performed by same task
+
+
+A lock acquired in one task must be released by the same task.  This
+means you can't do oddball things like acquire a lock and go off to
+play while another task releases it.  If you want to do something
+like this, acquire and release the task in the same code path and
+have the caller wait on an event by the other task.
+
+
+SOLUTION
+
+
+Data protection under preemption is achieved by disabling preemption for the
+duration of the critical region.
+
+preempt_enable()		decrement the preempt counter
+preempt_disable()		increment the preempt counter
+preempt_enable_no_resched()	decrement, but do not immediately preempt
+preempt_get_count()		return the preempt counter
+
+The functions are nestable.  In other words, you can call preempt_disable
+n-times in a code path, and preemption will not be reenabled until the n-th
+call to preempt_enable.  The preempt statements define to nothing if
+preemption is not enabled.
+
+Note that you do not need to explicitly prevent preemption if you are holding
+any locks or interrupts are disabled, since preemption is implicitly disabled
+in those cases.
+
+Example:
+
+	cpucache_t *cc; /* this is per-CPU */
+	preempt_disable();
+	cc = cc_data(searchp);
+	if (cc && cc->avail) {
+		__free_block(searchp, cc_entry(cc), cc->avail);
+		cc->avail = 0;
+	}
+	preempt_enable();
+	return 0;
+
+Notice how the preemption statements must encompass every reference of the
+critical variables.  Another example:
+
+	int buf[NR_CPUS];
+	set_cpu_val(buf);
+	if (buf[smp_processor_id()] == -1) printf(KERN_INFO "wee!\n");
+	spin_lock(&buf_lock);
+	/* ... */
+
+This code is not preempt-safe, but see how easily we can fix it by simply
+moving the spin_lock up two lines.
diff -uNrp linux-2.4.23/MAINTAINERS linux-2.4.23-fusion/MAINTAINERS
--- linux-2.4.23/MAINTAINERS	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/MAINTAINERS	2004-01-11 15:29:36.000000000 +0100
@@ -1484,6 +1484,14 @@ P:	Michal Ostrowski
 M:	mostrows@styx.uwaterloo.ca
 S:	Maintained
 
+PREEMPTIBLE KERNEL
+P:	Robert M. Love
+M:	rml@tech9.net
+L:	linux-kernel@vger.kernel.org
+L:	kpreempt-tech@lists.sourceforge.net
+W:	http://tech9.net/rml/linux
+S:	Supported
+
 PROMISE DC4030 CACHING DISK CONTROLLER DRIVER
 P:	Peter Denison
 M:	promise@pnd-pc.demon.co.uk
diff -uNrp linux-2.4.23/Makefile linux-2.4.23-fusion/Makefile
--- linux-2.4.23/Makefile	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/Makefile	2004-01-11 21:42:56.000000000 +0100
@@ -1,7 +1,7 @@
 VERSION = 2
 PATCHLEVEL = 4
 SUBLEVEL = 23
-EXTRAVERSION =
+EXTRAVERSION = -fusion
 
 KERNELRELEASE=$(VERSION).$(PATCHLEVEL).$(SUBLEVEL)$(EXTRAVERSION)
 
@@ -74,7 +74,7 @@ endif
 # images.  Uncomment if you want to place them anywhere other than root.
 #
 
-#export	INSTALL_PATH=/boot
+export	INSTALL_PATH=/boot
 
 #
 # INSTALL_MOD_PATH specifies a prefix to MODLIB for module directory
@@ -135,6 +135,7 @@ DRIVERS-y :=
 DRIVERS-m :=
 DRIVERS-  :=
 
+DRIVERS-$(CONFIG_ADEOS) += drivers/adeos/adeos.o
 DRIVERS-$(CONFIG_ACPI_BOOT) += drivers/acpi/acpi.o
 DRIVERS-$(CONFIG_PARPORT) += drivers/parport/driver.o
 DRIVERS-y += drivers/char/char.o \
diff -uNrp linux-2.4.23/arch/alpha/kernel/process.c linux-2.4.23-fusion/arch/alpha/kernel/process.c
--- linux-2.4.23/arch/alpha/kernel/process.c	2003-08-25 13:44:39.000000000 +0200
+++ linux-2.4.23-fusion/arch/alpha/kernel/process.c	2004-01-11 15:29:36.000000000 +0100
@@ -186,6 +186,7 @@ common_shutdown(int mode, char *restart_
 	args.mode = mode;
 	args.restart_cmd = restart_cmd;
 #ifdef CONFIG_SMP
+	preempt_disable();
 	smp_call_function(common_shutdown_1, &args, 1, 0);
 #endif
 	common_shutdown_1(&args);
diff -uNrp linux-2.4.23/arch/i386/config.in linux-2.4.23-fusion/arch/i386/config.in
--- linux-2.4.23/arch/i386/config.in	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/config.in	2004-01-11 15:29:36.000000000 +0100
@@ -25,6 +25,9 @@ endmenu
 
 mainmenu_option next_comment
 comment 'Processor type and features'
+bool 'Low latency scheduling' CONFIG_LOLAT
+dep_bool 'Control low latency with sysctl' CONFIG_LOLAT_SYSCTL $CONFIG_LOLAT
+
 choice 'Processor family' \
 	"386					CONFIG_M386 \
 	 486					CONFIG_M486 \
@@ -225,6 +228,7 @@ fi
 bool 'Math emulation' CONFIG_MATH_EMULATION
 bool 'MTRR (Memory Type Range Register) support' CONFIG_MTRR
 bool 'Symmetric multi-processing support' CONFIG_SMP
+bool 'Preemptible Kernel' CONFIG_PREEMPT
 if [ "$CONFIG_SMP" != "y" ]; then
    bool 'Local APIC support on uniprocessors' CONFIG_X86_UP_APIC
    dep_bool 'IO-APIC support on uniprocessors' CONFIG_X86_UP_IOAPIC $CONFIG_X86_UP_APIC
@@ -258,15 +262,22 @@ if [ "$CONFIG_X86_NUMA" != "y" ]; then
    fi
 fi
 
-if [ "$CONFIG_SMP" = "y" -a "$CONFIG_X86_CMPXCHG" = "y" ]; then
-   define_bool CONFIG_HAVE_DEC_LOCK y
+if [ "$CONFIG_SMP" = "y" -o "$CONFIG_PREEMPT" = "y" ]; then
+   if [ "$CONFIG_X86_CMPXCHG" = "y" ]; then
+      define_bool CONFIG_HAVE_DEC_LOCK y
+   fi
 fi
+
 endmenu
 
 mainmenu_option next_comment
 comment 'General setup'
 
 bool 'Networking support' CONFIG_NET
+tristate 'Adeos support' CONFIG_ADEOS
+if [ "$CONFIG_ADEOS" != "n" ]; then
+   define_bool CONFIG_ADEOS_CORE y
+fi
 
 # Visual Workstation support is utterly broken.
 # If you want to see it working mail an VW540 to hch@infradead.org 8)
diff -uNrp linux-2.4.23/arch/i386/kernel/Makefile linux-2.4.23-fusion/arch/i386/kernel/Makefile
--- linux-2.4.23/arch/i386/kernel/Makefile	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/Makefile	2004-01-11 15:29:36.000000000 +0100
@@ -14,7 +14,7 @@ all: kernel.o head.o init_task.o
 
 O_TARGET := kernel.o
 
-export-objs     := mca.o mtrr.o msr.o cpuid.o microcode.o i386_ksyms.o time.o setup.o
+export-objs     := adeos.o mca.o mtrr.o msr.o cpuid.o microcode.o i386_ksyms.o time.o setup.o
 
 obj-y	:= process.o semaphore.o signal.o entry.o traps.o irq.o vm86.o \
 		ptrace.o i8259.o ioport.o ldt.o setup.o time.o sys_i386.o \
@@ -30,6 +30,7 @@ obj-y			+= pci-pc.o pci-irq.o
 endif
 endif
 
+obj-$(CONFIG_ADEOS_CORE)	+= adeos.o
 obj-$(CONFIG_MCA)		+= mca.o
 obj-$(CONFIG_MTRR)		+= mtrr.o
 obj-$(CONFIG_X86_MSR)		+= msr.o
diff -uNrp linux-2.4.23/arch/i386/kernel/adeos.c linux-2.4.23-fusion/arch/i386/kernel/adeos.c
--- linux-2.4.23/arch/i386/kernel/adeos.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/adeos.c	2004-01-11 15:30:29.000000000 +0100
@@ -0,0 +1,395 @@
+/*
+ *   linux/arch/i386/kernel/adeos.c
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-dependent ADEOS core support for x86.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <linux/irq.h>
+#include <linux/slab.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/hw_irq.h>
+#include <asm/irq.h>
+#include <asm/desc.h>
+#include <asm/io.h>
+#ifdef CONFIG_X86_LOCAL_APIC
+#include <asm/fixmap.h>
+#include <asm/bitops.h>
+#include <asm/mpspec.h>
+#ifdef CONFIG_X86_IO_APIC
+#include <asm/io_apic.h>
+#endif /* CONFIG_X86_IO_APIC */
+#include <asm/apic.h>
+#endif /* CONFIG_X86_LOCAL_APIC */
+
+struct pt_regs __adeos_tick_regs;
+
+int __adeos_tick_irq;
+
+#if defined(CONFIG_ADEOS_MODULE) || defined(CONFIG_X86_IO_APIC)
+/* A global flag telling whether Adeos pipelining is engaged. */
+int adp_pipelined = 0;
+#endif /* CONFIG_ADEOS_MODULE || CONFIG_X86_IO_APIC */
+
+#ifdef CONFIG_SMP
+
+static volatile unsigned long __adeos_cpu_sync_map;
+
+static volatile unsigned long __adeos_cpu_lock_map;
+
+static spinlock_t __adeos_cpu_barrier = SPIN_LOCK_UNLOCKED;
+
+static atomic_t __adeos_critical_count = ATOMIC_INIT(0);
+
+static void (*__adeos_cpu_sync)(void);
+
+#endif /* CONFIG_SMP */
+
+#define __adeos_call_asm_irq_handler(adp,irq) \
+   __asm__ __volatile__ ("pushfl\n\t" \
+                         "push %%cs\n\t" \
+                         "call *%1\n" \
+			 : /* no output */ \
+			 : "a" (irq), "m" ((adp)->irqs[irq].handler))
+
+#define __adeos_call_c_irq_handler(adp,irq) \
+   __asm__ __volatile__ ("pushl %%ebp\n\t" \
+	                 "pushl %%edi\n\t" \
+                      	 "pushl %%esi\n\t" \
+	                 "pushl %%edx\n\t" \
+                         "pushl %%ecx\n\t" \
+	                 "pushl %%ebx\n\t" \
+                         "pushl %%eax\n\t" \
+                         "call *%1\n\t" \
+                         "addl $4,%%esp\n\t" \
+                         "popl %%ebx\n\t" \
+                         "popl %%ecx\n\t" \
+	                 "popl %%edx\n\t" \
+	                 "popl %%esi\n\t" \
+	                 "popl %%edi\n\t" \
+	                 "popl %%ebp\n" \
+			 : /* no output */ \
+			 : "a" (irq), "m" ((adp)->irqs[irq].handler))
+
+#ifdef CONFIG_PREEMPT
+/* With kernel preemption enabled, ret_from_intr might identify a need
+   for rescheduling raised by the C handler, so we should call it
+   after we have bumped the preemption count. Native (i.e. ASM)
+   handlers are expected to do the same. */
+#define __adeos_call_c_root_irq_handler(adp,irq) \
+   __asm__ __volatile__ ("pushfl\n\t" \
+                         "pushl %%cs\n\t" \
+                         "pushl $1f\n\t" \
+	                 "pushl $-1\n\t" /* Negative (fake) orig_eax. */ \
+	                 "pushl %%es\n\t" \
+	                 "pushl %%ds\n\t" \
+	                 "pushl %%eax\n\t" \
+	                 "pushl %%ebp\n\t" \
+	                 "pushl %%edi\n\t" \
+	                 "pushl %%esi\n\t" \
+	                 "pushl %%edx\n\t" \
+	                 "pushl %%ecx\n\t" \
+	                 "pushl %%ebx\n\t" \
+                         __adeos_bump_count \
+                         "pushl %%eax\n\t" \
+                         "call *%1\n\t" \
+			 "addl $4,%%esp\n\t" \
+	                 "jmp "__stringify(ret_from_intr)"\n\t" \
+	                 "1:\n" \
+			 : /* no output */ \
+			 : "a" (irq), "m" ((adp)->irqs[irq].handler))
+#else /* !CONFIG_PREEMPT */
+#define __adeos_call_c_root_irq_handler(adp,irq) __adeos_call_c_irq_handler(adp,irq)
+#endif /* CONFIG_PREEMPT */
+
+static __inline__ unsigned long flnz (unsigned long word) {
+    __asm__("bsrl %1, %0"
+	    : "=r" (word)
+	    : "r"  (word));
+    return word;
+}
+
+void __adeos_check_machine (void)
+
+{
+#ifdef CONFIG_SMP
+    if (!cpu_has_apic)
+	/* Ok, unlike vanilla Linux, Adeos in SMP mode is not going to
+	   work without an APIC. So bail out now. */
+	panic("ADEOS: APIC not detected -- cannot run in SMP mode.");
+#endif /* CONFIG_SMP */
+}
+
+int __adeos_ack_system_irq (unsigned irq) {
+#ifdef CONFIG_X86_LOCAL_APIC
+    ack_APIC_irq();		/* Takes no spin lock, right? */
+#endif /* CONFIG_X86_LOCAL_APIC */
+    return 1;
+}
+
+#ifdef CONFIG_SMP
+
+static void __adeos_do_critical_sync (unsigned irq)
+
+{
+    adeos_declare_cpuid;
+
+    set_bit(cpuid,&__adeos_cpu_sync_map);
+
+    /* Now we are in sync with the lock requestor running on another
+       CPU. Enter a spinning wait until he releases the global
+       lock. */
+    adeos_spin_lock(&__adeos_cpu_barrier);
+
+    /* Got it. Now get out. */
+
+    if (__adeos_cpu_sync)
+	/* Call the sync routine if any. */
+	__adeos_cpu_sync();
+
+    adeos_spin_unlock(&__adeos_cpu_barrier);
+
+    clear_bit(cpuid,&__adeos_cpu_sync_map);
+}
+
+#endif /* CONFIG_SMP */
+
+/* adeos_critical_enter() -- Grab the superlock excluding all CPUs
+   but the current one from a critical section. This lock is used when
+   we must enforce a global critical section for a single CPU in a
+   possibly SMP system whichever context the CPUs are running
+   (i.e. interrupt handler or regular thread). */
+
+unsigned long adeos_critical_enter (void (*syncfn)(void))
+
+{
+    unsigned long flags;
+
+    adeos_hw_local_irq_save(flags);
+
+#ifdef CONFIG_SMP
+    if (smp_num_cpus > 1) /* We might be running a SMP-kernel on a UP box... */
+	{
+	adeos_declare_cpuid;
+
+	if (!test_and_set_bit(cpuid,&__adeos_cpu_lock_map))
+	    {
+	    while (test_and_set_bit(31,&__adeos_cpu_lock_map))
+		{
+		/* Refer to the explanations found in
+		   linux/arch/asm-i386/irq.c about
+		   SUSPECTED_CPU_OR_CHIPSET_BUG_WORKAROUND for more about
+		   this strange loop. */
+		int n = 0;
+		do { cpu_relax(); } while (++n < cpuid);
+		}
+
+	    adeos_spin_lock(&__adeos_cpu_barrier);
+
+	    __adeos_cpu_sync = syncfn;
+
+	    /* Send the sync IPI to all processors but the current one. */
+	    __adeos_send_IPI_allbutself(ADEOS_CRITICAL_VECTOR);
+
+	    while (__adeos_cpu_sync_map != (cpu_online_map & ~__adeos_cpu_lock_map))
+		cpu_relax();
+	    }
+
+	atomic_inc(&__adeos_critical_count);
+	}
+#endif /* CONFIG_SMP */
+
+    return flags;
+}
+
+/* adeos_critical_exit() -- Release the superlock. */
+
+void adeos_critical_exit (unsigned long flags)
+
+{
+#ifdef CONFIG_SMP
+    if (smp_num_cpus > 1) /* We might be running a SMP-kernel on a UP box... */
+	{
+	adeos_declare_cpuid;
+
+	if (atomic_dec_and_test(&__adeos_critical_count))
+	    {
+	    adeos_spin_unlock(&__adeos_cpu_barrier);
+
+	    while (__adeos_cpu_sync_map != 0)
+		cpu_relax();
+
+	    clear_bit(cpuid,&__adeos_cpu_lock_map);
+	    clear_bit(31,&__adeos_cpu_lock_map);
+	    }
+	}
+#endif /* CONFIG_SMP */
+
+    adeos_hw_local_irq_restore(flags);
+}
+
+void __adeos_init_stage (adomain_t *adp)
+
+{
+    int cpuid, n;
+
+    for (cpuid = 0; cpuid < ADEOS_NR_CPUS; cpuid++)
+	{
+	adp->cpudata[cpuid].irq_pending_hi = 0;
+
+	for (n = 0; n < IPIPE_IRQ_IWORDS; n++)
+	    adp->cpudata[cpuid].irq_pending_lo[n] = 0;
+
+	for (n = 0; n < IPIPE_NR_IRQS; n++)
+	    adp->cpudata[cpuid].irq_hits[n] = 0;
+	}
+
+    for (n = 0; n < IPIPE_NR_IRQS; n++)
+	{
+	adp->irqs[n].acknowledge = NULL;
+	adp->irqs[n].handler = NULL;
+	adp->irqs[n].control = IPIPE_PASS_MASK;	/* Pass but don't handle */
+	}
+
+#ifdef CONFIG_SMP
+    adp->irqs[ADEOS_CRITICAL_IPI].acknowledge = &__adeos_ack_system_irq;
+    adp->irqs[ADEOS_CRITICAL_IPI].handler = &__adeos_do_critical_sync;
+    /* Immediately handle in the current domain but *never* pass */
+    adp->irqs[ADEOS_CRITICAL_IPI].control = IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK|IPIPE_SYSTEM_MASK;
+#endif /* CONFIG_SMP */
+}
+
+/* __adeos_sync_stage() -- Flush the pending IRQs for the current
+   domain (and processor).  This routine flushes the interrupt log
+   (see "Optimistic interrupt protection" from D. Stodolsky et al. for
+   more on the deferred interrupt scheme). Every interrupt that
+   occurred while the pipeline was stalled gets played.  WARNING:
+   callers on SMP boxen should always check for CPU migration on
+   return of this routine. */
+
+void __adeos_sync_stage (void)
+
+{
+    unsigned long mask, submask, flags;
+    struct adcpudata *cpudata;
+    adeos_declare_cpuid;
+    int level, rank;
+    adomain_t *adp;
+    unsigned irq;
+
+    adp = adp_cpu_current[cpuid];
+    cpudata = &adp->cpudata[cpuid];
+
+    do
+	{
+	if (unlikely(test_and_set_bit(IPIPE_SYNC_FLAG,&cpudata->status)))
+	    return;
+
+	/* The policy here is to keep the dispatching code
+	   interrupt-free by stalling the current stage. If the upper
+	   domain handler (which we call) wants to re-enable
+	   interrupts while in a safe portion of the code
+	   (e.g. SA_INTERRUPT flag unset for Linux's sigaction()), it
+	   will have to unstall (then stall again before returning to
+	   us!) the stage when it sees fit. */
+
+	while ((mask = cpudata->irq_pending_hi) != 0)
+	    {
+	    /* Give a slight priority advantage to high-numbered IRQs
+	       like the virtual ones. */
+	    level = flnz(mask);
+	    clear_bit(level,&cpudata->irq_pending_hi);
+
+	    while ((submask = cpudata->irq_pending_lo[level]) != 0)
+		{
+		rank = flnz(submask);
+		irq = (level << 5) + rank;
+
+#ifdef CONFIG_SMP
+		if (unlikely(test_bit(IPIPE_LOCK_FLAG,&adp->irqs[irq].control)))
+		    {
+		    clear_bit(rank,&cpudata->irq_pending_lo[level]);
+		    continue;
+		    }
+#endif /* CONFIG_SMP */
+	
+		adeos_hw_local_irq_save(flags);
+
+		if (likely(--cpudata->irq_hits[irq] == 0))
+		    clear_bit(rank,&cpudata->irq_pending_lo[level]);
+
+		/* Allow the sync routine to be reentered on behalf of
+		   the IRQ handler and any execution context switched
+		   in by the IRQ handler. The latter also means that
+		   returning from the switched out context is always
+		   safe even if the sync routine has been reentered in
+		   the meantime. */
+
+		set_bit(IPIPE_STALL_FLAG,&cpudata->status);
+
+		clear_bit(IPIPE_SYNC_FLAG,&cpudata->status);
+
+		adeos_hw_sti();
+
+		if (test_bit(IPIPE_CALLASM_FLAG,&adp->irqs[irq].control))
+		    __adeos_call_asm_irq_handler(adp,irq);
+		else if (adp == adp_root)
+		    __adeos_call_c_root_irq_handler(adp,irq);
+		else
+		    __adeos_call_c_irq_handler(adp,irq);
+
+		adeos_hw_local_irq_restore(flags);
+
+#ifdef CONFIG_SMP
+		adeos_reload_cpuid();
+		cpudata = &adp->cpudata[cpuid];
+#endif /* CONFIG_SMP */
+
+		clear_bit(IPIPE_STALL_FLAG,&cpudata->status);
+
+		if (test_and_set_bit(IPIPE_SYNC_FLAG,&cpudata->status))
+		    return;
+		}
+	    }
+
+	clear_bit(IPIPE_SYNC_FLAG,&cpudata->status);
+	}
+    while (cpudata->irq_pending_hi != 0);
+}
+
+asmlinkage int __adeos_enter_syscall (struct pt_regs regs) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SYSCALL_PROLOGUE] > 0))
+	return __adeos_handle_event(ADEOS_SYSCALL_PROLOGUE,&regs);
+
+    return 0;
+}
+
+asmlinkage int __adeos_exit_syscall (void) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SYSCALL_EPILOGUE] > 0))
+	return __adeos_handle_event(ADEOS_SYSCALL_EPILOGUE,NULL);
+
+    return 0;
+}
diff -uNrp linux-2.4.23/arch/i386/kernel/apic.c linux-2.4.23-fusion/arch/i386/kernel/apic.c
--- linux-2.4.23/arch/i386/kernel/apic.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/apic.c	2004-01-11 15:29:36.000000000 +0100
@@ -1090,7 +1090,7 @@ inline void smp_local_timer_interrupt(st
 	 * Currently this isn't too much of an issue (performance wise),
 	 * we can take more than 100K local irqs per second on a 100 MHz P5.
 	 */
-}
+    }
 
 /*
  * Local APIC timer interrupt. This is the most natural way for doing
@@ -1115,6 +1115,11 @@ void smp_apic_timer_interrupt(struct pt_
 	 * NOTE! We'd better ACK the irq immediately,
 	 * because timer handling can be slow.
 	 */
+#ifdef CONFIG_ADEOS_CORE
+	if (adp_pipelined)
+	    regs = &__adeos_tick_regs;
+	else
+#endif /* CONFIG_ADEOS_CORE */
 	ack_APIC_irq();
 	/*
 	 * update_process_times() expects us to have done irq_enter().
@@ -1162,6 +1167,9 @@ asmlinkage void smp_error_interrupt(void
 	v = apic_read(APIC_ESR);
 	apic_write(APIC_ESR, 0);
 	v1 = apic_read(APIC_ESR);
+#ifdef CONFIG_ADEOS_CORE
+	if (!adp_pipelined)
+#endif /* CONFIG_ADEOS_CORE */
 	ack_APIC_irq();
 	atomic_inc(&irq_err_count);
 
diff -uNrp linux-2.4.23/arch/i386/kernel/cpuid.c linux-2.4.23-fusion/arch/i386/kernel/cpuid.c
--- linux-2.4.23/arch/i386/kernel/cpuid.c	2001-10-11 18:04:57.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/cpuid.c	2004-01-11 15:29:36.000000000 +0100
@@ -60,7 +60,8 @@ static void cpuid_smp_cpuid(void *cmd_bl
 static inline void do_cpuid(int cpu, u32 reg, u32 *data)
 {
   struct cpuid_command cmd;
-  
+
+  preempt_disable();
   if ( cpu == smp_processor_id() ) {
     cpuid(reg, &data[0], &data[1], &data[2], &data[3]);
   } else {
@@ -70,6 +71,7 @@ static inline void do_cpuid(int cpu, u32
     
     smp_call_function(cpuid_smp_cpuid, &cmd, 1, 1);
   }
+  preempt_enable();
 }
 #else /* ! CONFIG_SMP */
 
diff -uNrp linux-2.4.23/arch/i386/kernel/entry.S linux-2.4.23-fusion/arch/i386/kernel/entry.S
--- linux-2.4.23/arch/i386/kernel/entry.S	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/entry.S	2004-01-11 15:29:36.000000000 +0100
@@ -73,7 +73,7 @@ VM_MASK		= 0x00020000
  * these are offsets into the task-struct.
  */
 state		=  0
-flags		=  4
+preempt_count	=  4
 sigpending	=  8
 addr_limit	= 12
 exec_domain	= 16
@@ -81,8 +81,28 @@ need_resched	= 20
 tsk_ptrace	= 24
 processor	= 52
 
+/* These are offsets into the irq_stat structure
+ * There is one per cpu and it is aligned to 32
+ * byte boundry (we put that here as a shift count)
+ */
+irq_array_shift                 = CONFIG_X86_L1_CACHE_SHIFT
+
+irq_stat_local_irq_count        = 4
+irq_stat_local_bh_count         = 8
+
 ENOSYS = 38
 
+#ifdef CONFIG_SMP
+#define GET_CPU_INDX	movl processor(%ebx),%eax;  \
+                        shll $irq_array_shift,%eax
+#define GET_CURRENT_CPU_INDX GET_CURRENT(%ebx); \
+                             GET_CPU_INDX
+#define CPU_INDX (,%eax)
+#else
+#define GET_CPU_INDX
+#define GET_CURRENT_CPU_INDX GET_CURRENT(%ebx)
+#define CPU_INDX
+#endif
 
 #define SAVE_ALL \
 	cld; \
@@ -138,6 +158,11 @@ ENTRY(lcall7)
 	pushfl			# We get a different stack layout with call gates,
 	pushl %eax		# which has to be cleaned up later..
 	SAVE_ALL
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_enter_syscall)
+	testl %eax,%eax
+	jne   restore_all
+#endif /* CONFIG_ADEOS_CORE */
 	movl EIP(%esp),%eax	# due to call gates, this is eflags, not eip..
 	movl CS(%esp),%edx	# this is eip..
 	movl EFLAGS(%esp),%ecx	# and this is cs..
@@ -155,13 +180,25 @@ ENTRY(lcall7)
 	pushl $0x7
 	call *%edx
 	addl $4, %esp
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_exit_syscall)
+	testl %eax,%eax
+	popl %eax
+	jne   restore_all
+#else /* !CONFIG_ADEOS_CORE */
 	popl %eax
+#endif /* CONFIG_ADEOS_CORE */
 	jmp ret_from_sys_call
 
 ENTRY(lcall27)
 	pushfl			# We get a different stack layout with call gates,
 	pushl %eax		# which has to be cleaned up later..
 	SAVE_ALL
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_enter_syscall)
+	testl %eax,%eax
+	jne   restore_all
+#endif /* CONFIG_ADEOS_CORE */
 	movl EIP(%esp),%eax	# due to call gates, this is eflags, not eip..
 	movl CS(%esp),%edx	# this is eip..
 	movl EFLAGS(%esp),%ecx	# and this is cs..
@@ -179,11 +216,21 @@ ENTRY(lcall27)
 	pushl $0x27
 	call *%edx
 	addl $4, %esp
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_exit_syscall)
+	testl %eax,%eax
 	popl %eax
+	jne   restore_all
+#else /* !CONFIG_ADEOS_CORE */
+	popl %eax
+#endif /* CONFIG_ADEOS_CORE */
 	jmp ret_from_sys_call
 
 
 ENTRY(ret_from_fork)
+#ifdef CONFIG_ADEOS_CORE
+	sti
+#endif /* CONFIG_ADEOS_CORE */
 	pushl %ebx
 	call SYMBOL_NAME(schedule_tail)
 	addl $4, %esp
@@ -202,6 +249,12 @@ ENTRY(ret_from_fork)
 ENTRY(system_call)
 	pushl %eax			# save orig_eax
 	SAVE_ALL
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_enter_syscall)
+	testl %eax,%eax
+	jne   restore_all
+	movl ORIG_EAX(%esp),%eax
+#endif /* CONFIG_ADEOS_CORE */
 	GET_CURRENT(%ebx)
 	testb $0x02,tsk_ptrace(%ebx)	# PT_TRACESYS
 	jne tracesys
@@ -209,18 +262,37 @@ ENTRY(system_call)
 	jae badsys
 	call *SYMBOL_NAME(sys_call_table)(,%eax,4)
 	movl %eax,EAX(%esp)		# save the return value
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_exit_syscall)
+	testl %eax,%eax
+	jne   restore_all
+#endif /* CONFIG_ADEOS_CORE */
 ENTRY(ret_from_sys_call)
-	cli				# need_resched and signals atomic test
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_stall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	cli
+#endif /* CONFIG_ADEOS_CORE */
 	cmpl $0,need_resched(%ebx)
 	jne reschedule
 	cmpl $0,sigpending(%ebx)
 	jne signal_return
+#ifdef CONFIG_PREEMPT
+unstall_and_restore_all:
+#endif /* CONFIG_PREEMPT */
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_unstall_root)
+#endif /* CONFIG_ADEOS_CORE */
 restore_all:
 	RESTORE_ALL
 
 	ALIGN
 signal_return:
-	sti				# we can get here from an interrupt handler
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_unstall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	sti
+#endif /* CONFIG_ADEOS_CORE */
 	testl $(VM_MASK),EFLAGS(%esp)
 	movl %esp,%eax
 	jne v86_signal_return
@@ -255,12 +327,38 @@ badsys:
 	ALIGN
 ENTRY(ret_from_intr)
 	GET_CURRENT(%ebx)
+#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_stall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	cli
+#endif /* CONFIG_ADEOS_CORE */
+	decl preempt_count(%ebx)
+#endif /* CONFIG_PREEMPT */
 ret_from_exception:
 	movl EFLAGS(%esp),%eax		# mix EFLAGS and CS
 	movb CS(%esp),%al
 	testl $(VM_MASK | 3),%eax	# return to VM86 mode or non-supervisor?
 	jne ret_from_sys_call
+#ifdef CONFIG_PREEMPT
+	cmpl $0,preempt_count(%ebx)
+	jnz unstall_and_restore_all
+	cmpl $0,need_resched(%ebx)
+	jz unstall_and_restore_all
+	movl SYMBOL_NAME(irq_stat)+irq_stat_local_bh_count CPU_INDX,%ecx
+	addl SYMBOL_NAME(irq_stat)+irq_stat_local_irq_count CPU_INDX,%ecx
+	jnz unstall_and_restore_all
+	incl preempt_count(%ebx)
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_unstall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	sti
+#endif /* CONFIG_ADEOS_CORE */
+	call SYMBOL_NAME(preempt_schedule)
+	jmp ret_from_intr
+#else /* !CONFIG_PREEMPT */
 	jmp restore_all
+#endif /* CONFIG_PREEMPT */
 
 	ALIGN
 reschedule:
@@ -297,6 +395,13 @@ error_code:
 	GET_CURRENT(%ebx)
 	call *%edi
 	addl $8,%esp
+#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_stall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	cli
+#endif /* CONFIG_ADEOS_CORE */
+#endif /* CONFIG_PREEMPT */
 	jmp ret_from_exception
 
 ENTRY(coprocessor_error)
@@ -316,12 +421,26 @@ ENTRY(device_not_available)
 	movl %cr0,%eax
 	testl $0x4,%eax			# EM (math emulation bit)
 	jne device_not_available_emulate
+#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_stall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	cli
+#endif /* CONFIG_ADEOS_CORE */
+#endif /* CONFIG_PREEMPT */
 	call SYMBOL_NAME(math_state_restore)
 	jmp ret_from_exception
 device_not_available_emulate:
 	pushl $0		# temporary storage for ORIG_EIP
 	call  SYMBOL_NAME(math_emulate)
 	addl $4,%esp
+#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_ADEOS_CORE
+	call SYMBOL_NAME(__adeos_stall_root)
+#else /* !CONFIG_ADEOS_CORE */
+	cli
+#endif /* CONFIG_ADEOS_CORE */
+#endif /* CONFIG_PREEMPT */
 	jmp ret_from_exception
 
 ENTRY(debug)
diff -uNrp linux-2.4.23/arch/i386/kernel/i386_ksyms.c linux-2.4.23-fusion/arch/i386/kernel/i386_ksyms.c
--- linux-2.4.23/arch/i386/kernel/i386_ksyms.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/i386_ksyms.c	2004-01-11 15:29:36.000000000 +0100
@@ -33,6 +33,58 @@
 extern void dump_thread(struct pt_regs *, struct user *);
 extern spinlock_t rtc_lock;
 
+#ifdef CONFIG_ADEOS_CORE
+EXPORT_SYMBOL(adeos_critical_enter);
+EXPORT_SYMBOL(adeos_critical_exit);
+EXPORT_SYMBOL(__adeos_sync_stage);
+EXPORT_SYMBOL(__adeos_ack_system_irq);
+EXPORT_SYMBOL(__adeos_tick_regs);
+EXPORT_SYMBOL(__adeos_tick_irq);
+#ifdef CONFIG_SMP
+EXPORT_SYMBOL(__adeos_send_IPI_allbutself);
+EXPORT_SYMBOL(__adeos_send_IPI_other);
+EXPORT_SYMBOL(__adeos_set_irq_affinity);
+#endif /* CONFIG_SMP */
+#ifdef CONFIG_ADEOS_MODULE
+#ifdef CONFIG_X86_LOCAL_APIC
+extern int using_apic_timer;
+EXPORT_SYMBOL_NOVERS(using_apic_timer);
+#endif /* CONFIG_X86_LOCAL_APIC */
+#endif /* CONFIG_ADEOS_MODULE */
+/* The following are per-platform convenience exports which are needed
+   by some Adeos domains loaded as kernel modules. */
+extern irq_desc_t irq_desc[];
+EXPORT_SYMBOL_NOVERS(irq_desc);
+extern struct desc_struct idt_table[];
+EXPORT_SYMBOL_NOVERS(idt_table);
+extern void ret_from_intr(void);
+EXPORT_SYMBOL_NOVERS(ret_from_intr);
+extern struct tty_driver console_driver;
+EXPORT_SYMBOL_NOVERS(console_driver);
+extern void (*kd_mksound)(unsigned int hz, unsigned int ticks);
+EXPORT_SYMBOL_NOVERS(kd_mksound);
+#ifdef CONFIG_SMP
+EXPORT_SYMBOL_NOVERS(cpu_tlbstate);
+#ifdef CONFIG_MULTIQUAD
+EXPORT_SYMBOL_NOVERS(logical_apicid_2_cpu);
+#else  /* CONFIG_MULTIQUAD */
+EXPORT_SYMBOL_NOVERS(physical_apicid_2_cpu);
+#endif /* CONFIG_MULTIQUAD */
+#endif /* CONFIG_SMP */
+#if defined(CONFIG_ADEOS_MODULE) && defined(CONFIG_X86_IO_APIC)
+EXPORT_SYMBOL_NOVERS(io_apic_irqs);
+EXPORT_SYMBOL_NOVERS(irq_vector);
+#endif /* CONFIG_ADEOS_MODULE && CONFIG_X86_IO_APIC */
+EXPORT_SYMBOL_NOVERS(set_ldt_desc);
+EXPORT_SYMBOL_NOVERS(default_ldt);
+EXPORT_SYMBOL_NOVERS(__switch_to);
+EXPORT_SYMBOL_NOVERS(cpu_khz);
+extern void show_stack(unsigned long *esp);
+EXPORT_SYMBOL_NOVERS(show_stack);
+extern void show_registers(struct pt_regs *regs);
+EXPORT_SYMBOL_NOVERS(show_registers);
+#endif /* CONFIG_ADEOS_CORE */
+
 #if defined(CONFIG_APM) || defined(CONFIG_APM_MODULE)
 extern void machine_real_restart(unsigned char *, int);
 EXPORT_SYMBOL(machine_real_restart);
diff -uNrp linux-2.4.23/arch/i386/kernel/i387.c linux-2.4.23-fusion/arch/i386/kernel/i387.c
--- linux-2.4.23/arch/i386/kernel/i387.c	2003-08-25 13:44:39.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/i387.c	2004-01-11 15:29:36.000000000 +0100
@@ -10,6 +10,7 @@
 
 #include <linux/config.h>
 #include <linux/sched.h>
+#include <linux/spinlock.h>
 #include <linux/init.h>
 #include <asm/processor.h>
 #include <asm/i387.h>
@@ -89,6 +90,8 @@ void kernel_fpu_begin(void)
 {
 	struct task_struct *tsk = current;
 
+	preempt_disable();
+	
 	if (tsk->flags & PF_USEDFPU) {
 		__save_init_fpu(tsk);
 		return;
diff -uNrp linux-2.4.23/arch/i386/kernel/i8259.c linux-2.4.23-fusion/arch/i386/kernel/i8259.c
--- linux-2.4.23/arch/i386/kernel/i8259.c	2001-09-18 08:03:09.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/i8259.c	2004-01-11 15:29:36.000000000 +0100
@@ -265,6 +265,36 @@ static inline int i8259A_irq_real(unsign
  */
 void mask_and_ack_8259A(unsigned int irq)
 {
+#ifdef CONFIG_ADEOS_CORE
+	unsigned int irqmask = 1 << irq;
+
+	spin_lock(&i8259A_lock);
+
+	if (cached_irq_mask & irqmask)
+		goto spurious_8259A_irq;
+
+	if (irq == 0) {
+	    /* Fast timer ack -- don't mask
+	      (unless supposedly spurious) */
+	    outb(0x20,0x20);
+	    spin_unlock(&i8259A_lock);
+	    return;
+	}
+
+	cached_irq_mask |= irqmask;
+
+handle_real_irq:
+	if (irq & 8) {
+		outb(cached_A1,0xA1);
+		outb(0x60+(irq&7),0xA0);/* 'Specific EOI' to slave */
+		outb(0x62,0x20);	/* 'Specific EOI' to master-IRQ2 */
+	} else {
+		outb(cached_21,0x21);
+		outb(0x60+irq,0x20);	/* 'Specific EOI' to master */
+	}
+	spin_unlock(&i8259A_lock);
+	return;
+#else /* !CONFIG_ADEOS_CORE */
 	unsigned int irqmask = 1 << irq;
 	unsigned long flags;
 
@@ -301,6 +331,7 @@ handle_real_irq:
 	}
 	spin_unlock_irqrestore(&i8259A_lock, flags);
 	return;
+#endif /* CONFIG_ADEOS_CORE */
 
 spurious_8259A_irq:
 	/*
diff -uNrp linux-2.4.23/arch/i386/kernel/io_apic.c linux-2.4.23-fusion/arch/i386/kernel/io_apic.c
--- linux-2.4.23/arch/i386/kernel/io_apic.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/io_apic.c	2004-01-11 15:29:36.000000000 +0100
@@ -1250,6 +1250,25 @@ static unsigned int startup_edge_ioapic_
  * interrupt for real. This prevents IRQ storms from unhandled
  * devices.
  */
+
+#ifdef CONFIG_ADEOS_CORE
+
+static void ack_edge_ioapic_irq (unsigned irq)
+
+{
+    if ((irq_desc[irq].status & (IRQ_PENDING | IRQ_DISABLED)) == (IRQ_PENDING | IRQ_DISABLED))
+	{
+	unsigned long flags;
+	adeos_spin_lock_irqsave(&ioapic_lock,flags);
+	__mask_IO_APIC_irq(irq);
+	adeos_spin_unlock_irqrestore(&ioapic_lock,flags);
+	}
+
+    ack_APIC_irq();
+}
+
+#else /* !CONFIG_ADEOS_CORE */
+
 static void ack_edge_ioapic_irq(unsigned int irq)
 {
 	if ((irq_desc[irq].status & (IRQ_PENDING | IRQ_DISABLED))
@@ -1258,6 +1277,8 @@ static void ack_edge_ioapic_irq(unsigned
 	ack_APIC_irq();
 }
 
+#endif /* CONFIG_ADEOS_CORE */
+
 static void end_edge_ioapic_irq (unsigned int i) { /* nothing */ }
 
 
@@ -1286,6 +1307,65 @@ static unsigned int startup_level_ioapic
 #define enable_level_ioapic_irq		unmask_IO_APIC_irq
 #define disable_level_ioapic_irq	mask_IO_APIC_irq
 
+#ifdef CONFIG_ADEOS_CORE
+
+/* The standard Linux implementation for acknowledging IO-APIC
+   interrupts has been changed in order to guarantee that low priority
+   IRQs won't be delayed waiting for a high priority interrupt handler
+   to call end_level_ioapic_irq(). Therefore we immediately ack in
+   mask_and_ack_level_ioapic_irq(), still handling the 82093AA bugous
+   edge case. */
+
+static unsigned long bugous_edge_triggers;
+
+static void end_level_ioapic_irq (unsigned irq)
+
+{
+    if (test_and_clear_bit(irq,&bugous_edge_triggers))
+	{
+#ifdef APIC_MISMATCH_DEBUG
+	atomic_inc(&irq_mis_count);
+#endif
+	spin_lock(&ioapic_lock);
+	__unmask_and_level_IO_APIC_irq(irq);
+	spin_unlock(&ioapic_lock);
+	}
+    else
+	{
+	spin_lock(&ioapic_lock);
+	__unmask_IO_APIC_irq(irq);
+	spin_unlock(&ioapic_lock);
+	}
+}
+
+static void mask_and_ack_level_ioapic_irq (unsigned irq)
+
+{
+    unsigned long v;
+    int i;
+
+    i = IO_APIC_VECTOR(irq);
+    v = apic_read(APIC_TMR + ((i & ~0x1f) >> 1));
+
+    if (!(v & (1 << (i & 0x1f))))
+	{
+	set_bit(irq,&bugous_edge_triggers);
+	spin_lock(&ioapic_lock);
+	__mask_and_edge_IO_APIC_irq(irq);
+	spin_unlock(&ioapic_lock);
+	}
+    else
+	{
+	spin_lock(&ioapic_lock);
+	__mask_IO_APIC_irq(irq);
+	spin_unlock(&ioapic_lock);
+	}
+
+    ack_APIC_irq();
+}
+
+#else /* !CONFIG_ADEOS_CORE */
+
 static void end_level_ioapic_irq (unsigned int irq)
 {
 	unsigned long v;
@@ -1347,6 +1427,8 @@ static void end_level_ioapic_irq (unsign
 
 static void mask_and_ack_level_ioapic_irq (unsigned int irq) { /* nothing */ }
 
+#endif /* CONFIG_ADEOS_CORE */
+
 #ifndef CONFIG_SMP
 
 void send_IPI_self(int vector)
diff -uNrp linux-2.4.23/arch/i386/kernel/ioport.c linux-2.4.23-fusion/arch/i386/kernel/ioport.c
--- linux-2.4.23/arch/i386/kernel/ioport.c	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/ioport.c	2004-01-11 15:29:36.000000000 +0100
@@ -55,7 +55,7 @@ static void set_bitmap(unsigned long *bi
 asmlinkage int sys_ioperm(unsigned long from, unsigned long num, int turn_on)
 {
 	struct thread_struct * t = &current->thread;
-	struct tss_struct * tss = init_tss + smp_processor_id();
+	struct tss_struct * tss;
 
 	if ((from + num <= from) || (from + num > IO_BITMAP_SIZE*32))
 		return -EINVAL;
@@ -66,6 +66,8 @@ asmlinkage int sys_ioperm(unsigned long 
 	 * IO bitmap up. ioperm() is much less timing critical than clone(),
 	 * this is why we delay this operation until now:
 	 */
+	preempt_disable();
+	tss = init_tss + smp_processor_id();
 	if (!t->ioperm) {
 		/*
 		 * just in case ...
@@ -84,6 +86,7 @@ asmlinkage int sys_ioperm(unsigned long 
 		memcpy(tss->io_bitmap, t->io_bitmap, IO_BITMAP_BYTES);
 		tss->bitmap = IO_BITMAP_OFFSET; /* Activate it in the TSS */
 	}
+	preempt_enable();
 
 	return 0;
 }
diff -uNrp linux-2.4.23/arch/i386/kernel/irq.c linux-2.4.23-fusion/arch/i386/kernel/irq.c
--- linux-2.4.23/arch/i386/kernel/irq.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/irq.c	2004-01-11 15:29:36.000000000 +0100
@@ -284,9 +284,11 @@ static inline void wait_on_irq(int cpu)
 				show("wait_on_irq");
 				count = ~0;
 			}
+			preempt_disable();
 			__sti();
 			SYNC_OTHER_CORES(cpu);
 			__cli();
+			preempt_enable_no_resched();
 			if (irqs_running())
 				continue;
 			if (global_irq_lock)
@@ -360,8 +362,9 @@ void __global_cli(void)
 
 	__save_flags(flags);
 	if (flags & (1 << EFLAGS_IF_SHIFT)) {
-		int cpu = smp_processor_id();
+		int cpu;
 		__cli();
+		cpu = smp_processor_id();
 		if (!local_irq_count(cpu))
 			get_irqlock(cpu);
 	}
@@ -369,11 +372,14 @@ void __global_cli(void)
 
 void __global_sti(void)
 {
-	int cpu = smp_processor_id();
+	int cpu;
 
+	preempt_disable();
+	cpu = smp_processor_id();
 	if (!local_irq_count(cpu))
 		release_irqlock(cpu);
 	__sti();
+	preempt_enable();
 }
 
 /*
@@ -388,13 +394,15 @@ unsigned long __global_save_flags(void)
 	int retval;
 	int local_enabled;
 	unsigned long flags;
-	int cpu = smp_processor_id();
+	int cpu;
 
 	__save_flags(flags);
 	local_enabled = (flags >> EFLAGS_IF_SHIFT) & 1;
 	/* default to local */
 	retval = 2 + local_enabled;
 
+	preempt_disable();
+	cpu = smp_processor_id();
 	/* check for global flags if we're not in an interrupt */
 	if (!local_irq_count(cpu)) {
 		if (local_enabled)
@@ -402,6 +410,7 @@ unsigned long __global_save_flags(void)
 		if (global_irq_holder == cpu)
 			retval = 0;
 	}
+	preempt_enable();
 	return retval;
 }
 
@@ -595,6 +604,9 @@ asmlinkage unsigned int do_IRQ(struct pt
 
 	kstat.irqs[cpu][irq]++;
 	spin_lock(&desc->lock);
+#ifdef CONFIG_ADEOS_CORE
+	if (!adp_pipelined)
+#endif /* CONFIG_ADEOS_CORE */
 	desc->handler->ack(irq);
 	/*
 	   REPLAY is when Linux resends an IRQ that was dropped earlier
@@ -636,6 +648,11 @@ asmlinkage unsigned int do_IRQ(struct pt
 	 */
 	for (;;) {
 		spin_unlock(&desc->lock);
+#ifdef CONFIG_ADEOS_CORE
+		if (likely(adp_pipelined && __adeos_tick_irq == irq))
+		    handle_IRQ_event(irq,&__adeos_tick_regs,action);
+		else
+#endif /* CONFIG_ADEOS_CORE */
 		handle_IRQ_event(irq, &regs, action);
 		spin_lock(&desc->lock);
 		
@@ -1125,6 +1142,29 @@ static int irq_affinity_write_proc (stru
 	return full_count;
 }
 
+#ifdef CONFIG_ADEOS_CORE
+
+unsigned long __adeos_set_irq_affinity (unsigned irq, unsigned long cpumask)
+
+{
+    unsigned long oldmask = irq_affinity[irq];
+
+    if (cpumask == 0)
+	return oldmask; /* Return mask value -- no change. */
+
+    cpumask &= cpu_online_map;
+
+    if (cpumask == 0 || irq_desc[irq].handler->set_affinity == NULL)
+	return 0;	/* Error -- bad mask value or non-routable IRQ. */
+
+    irq_affinity[irq] = cpumask;
+    irq_desc[irq].handler->set_affinity(irq,cpumask);
+
+    return oldmask;
+}
+
+#endif /* CONFIG_ADEOS_CORE */
+
 #endif
 
 static int prof_cpu_mask_read_proc (char *page, char **start, off_t off,
@@ -1212,4 +1252,3 @@ void init_irq_proc (void)
 	for (i = 0; i < NR_IRQS; i++)
 		register_irq_proc(i);
 }
-
diff -uNrp linux-2.4.23/arch/i386/kernel/ldt.c linux-2.4.23-fusion/arch/i386/kernel/ldt.c
--- linux-2.4.23/arch/i386/kernel/ldt.c	2001-10-17 23:46:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/ldt.c	2004-01-11 15:29:36.000000000 +0100
@@ -92,6 +92,7 @@ static int write_ldt(void * ptr, unsigne
 	 * the GDT index of the LDT is allocated dynamically, and is
 	 * limited by MAX_LDT_DESCRIPTORS.
 	 */
+	preempt_disable();
 	down_write(&mm->mmap_sem);
 	if (!mm->context.segments) {
 		void * segments = vmalloc(LDT_ENTRIES*LDT_ENTRY_SIZE);
@@ -144,6 +145,7 @@ install:
 
 out_unlock:
 	up_write(&mm->mmap_sem);
+	preempt_enable();
 out:
 	return error;
 }
diff -uNrp linux-2.4.23/arch/i386/kernel/microcode.c linux-2.4.23-fusion/arch/i386/kernel/microcode.c
--- linux-2.4.23/arch/i386/kernel/microcode.c	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/microcode.c	2004-01-11 15:29:36.000000000 +0100
@@ -182,11 +182,14 @@ static int do_microcode_update(void)
 	int i, error = 0, err;
 	struct microcode *m;
 
+	preempt_disable();
 	if (smp_call_function(do_update_one, NULL, 1, 1) != 0) {
 		printk(KERN_ERR "microcode: IPI timeout, giving up\n");
+		preempt_enable();
 		return -EIO;
 	}
 	do_update_one(NULL);
+	preempt_enable();
 
 	for (i=0; i<smp_num_cpus; i++) {
 		err = update_req[i].err;
diff -uNrp linux-2.4.23/arch/i386/kernel/msr.c linux-2.4.23-fusion/arch/i386/kernel/msr.c
--- linux-2.4.23/arch/i386/kernel/msr.c	2001-10-11 18:04:57.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/msr.c	2004-01-11 15:29:36.000000000 +0100
@@ -114,8 +114,9 @@ static inline int do_wrmsr(int cpu, u32 
 {
   struct msr_command cmd;
 
+  preempt_disable();
   if ( cpu == smp_processor_id() ) {
-    return wrmsr_eio(reg, eax, edx);
+    cmd.err = wrmsr_eio(reg, eax, edx);
   } else {
     cmd.cpu = cpu;
     cmd.reg = reg;
@@ -123,16 +124,19 @@ static inline int do_wrmsr(int cpu, u32 
     cmd.data[1] = edx;
     
     smp_call_function(msr_smp_wrmsr, &cmd, 1, 1);
-    return cmd.err;
   }
+
+  preempt_enable();
+  return cmd.err;
 }
 
 static inline int do_rdmsr(int cpu, u32 reg, u32 *eax, u32 *edx)
 {
   struct msr_command cmd;
 
+  preempt_disable();
   if ( cpu == smp_processor_id() ) {
-    return rdmsr_eio(reg, eax, edx);
+    cmd.err = rdmsr_eio(reg, eax, edx);
   } else {
     cmd.cpu = cpu;
     cmd.reg = reg;
@@ -141,9 +145,10 @@ static inline int do_rdmsr(int cpu, u32 
     
     *eax = cmd.data[0];
     *edx = cmd.data[1];
-
-    return cmd.err;
   }
+
+  preempt_enable();
+  return cmd.err;
 }
 
 #else /* ! CONFIG_SMP */
diff -uNrp linux-2.4.23/arch/i386/kernel/mtrr.c linux-2.4.23-fusion/arch/i386/kernel/mtrr.c
--- linux-2.4.23/arch/i386/kernel/mtrr.c	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/mtrr.c	2004-01-11 15:29:36.000000000 +0100
@@ -1065,6 +1065,9 @@ static void set_mtrr_smp (unsigned int r
     wait_barrier_execute = TRUE;
     wait_barrier_cache_enable = TRUE;
     atomic_set (&undone_count, smp_num_cpus - 1);
+
+    preempt_disable();
+
     /*  Start the ball rolling on other CPUs  */
     if (smp_call_function (ipi_handler, &data, 1, 0) != 0)
 	panic ("mtrr: timed out waiting for other CPUs\n");
@@ -1090,6 +1093,9 @@ static void set_mtrr_smp (unsigned int r
 	then enable the local cache and return  */
     wait_barrier_cache_enable = FALSE;
     set_mtrr_done (&ctxt);
+
+    preempt_enable();
+
 }   /*  End Function set_mtrr_smp  */
 
 
diff -uNrp linux-2.4.23/arch/i386/kernel/nmi.c linux-2.4.23-fusion/arch/i386/kernel/nmi.c
--- linux-2.4.23/arch/i386/kernel/nmi.c	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/nmi.c	2004-01-11 15:29:36.000000000 +0100
@@ -363,6 +363,9 @@ void nmi_watchdog_tick (struct pt_regs *
 			bust_spinlocks(1);
 			printk("NMI Watchdog detected LOCKUP on CPU%d, eip %08lx, registers:\n", cpu, regs->eip);
 			show_registers(regs);
+#ifdef CONFIG_ADEOS_CORE
+			__adeos_dump_state();
+#endif /* CONFIG_ADEOS_CORE */
 			printk("console shuts up ...\n");
 			console_silent();
 			spin_unlock(&nmi_print_lock);
diff -uNrp linux-2.4.23/arch/i386/kernel/process.c linux-2.4.23-fusion/arch/i386/kernel/process.c
--- linux-2.4.23/arch/i386/kernel/process.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/process.c	2004-01-11 15:29:36.000000000 +0100
@@ -134,6 +134,9 @@ void cpu_idle (void)
 		void (*idle)(void) = pm_idle;
 		if (!idle)
 			idle = default_idle;
+#ifdef CONFIG_ADEOS_CORE
+		adeos_suspend_domain();
+#endif /* CONFIG_ADEOS_CORE */
 		while (!current->need_resched)
 			idle();
 		schedule();
diff -uNrp linux-2.4.23/arch/i386/kernel/smp.c linux-2.4.23-fusion/arch/i386/kernel/smp.c
--- linux-2.4.23/arch/i386/kernel/smp.c	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/kernel/smp.c	2004-01-11 15:29:36.000000000 +0100
@@ -134,6 +134,11 @@ static inline void __send_IPI_shortcut(u
 	 */
 	unsigned int cfg;
 
+#ifdef CONFIG_ADEOS_CORE
+	unsigned long flags;
+	adeos_hw_local_irq_save(flags);
+#endif /* !CONFIG_ADEOS_CORE */
+
 	/*
 	 * Wait for idle.
 	 */
@@ -148,6 +153,10 @@ static inline void __send_IPI_shortcut(u
 	 * Send the IPI. The write to APIC_ICR fires this off.
 	 */
 	apic_write_around(APIC_ICR, cfg);
+
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#endif /* !CONFIG_ADEOS_CORE */
 }
 
 void send_IPI_self(int vector)
@@ -160,9 +169,12 @@ static inline void send_IPI_mask_bitmask
 	unsigned long cfg;
 	unsigned long flags;
 
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	__save_flags(flags);
 	__cli();
-
+#endif /* CONFIG_ADEOS_CORE */
 		
 	/*
 	 * Wait for idle.
@@ -185,7 +197,11 @@ static inline void send_IPI_mask_bitmask
 	 */
 	apic_write_around(APIC_ICR, cfg);
 
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	__restore_flags(flags);
+#endif /* CONFIG_ADEOS_CORE */
 }
 
 static inline void send_IPI_mask_sequence(int mask, int vector)
@@ -199,8 +215,12 @@ static inline void send_IPI_mask_sequenc
 	 * should be modified to do 1 message per cluster ID - mbligh
 	 */ 
 
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	__save_flags(flags);
 	__cli();
+#endif /* CONFIG_ADEOS_CORE */
 
 	for (query_cpu = 0; query_cpu < NR_CPUS; ++query_cpu) {
 		query_mask = 1 << query_cpu;
@@ -231,7 +251,11 @@ static inline void send_IPI_mask_sequenc
 			apic_write_around(APIC_ICR, cfg);
 		}
 	}
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	__restore_flags(flags);
+#endif /* CONFIG_ADEOS_CORE */
 }
 
 static inline void send_IPI_mask(int mask, int vector)
@@ -360,10 +384,14 @@ static void inline leave_mm (unsigned lo
 
 asmlinkage void smp_invalidate_interrupt (void)
 {
-	unsigned long cpu = smp_processor_id();
+	unsigned long cpu;
+
+	preempt_disable();
+
+	cpu = smp_processor_id();
 
 	if (!test_bit(cpu, &flush_cpumask))
-		return;
+		goto out;
 		/* 
 		 * This was a BUG() but until someone can quote me the
 		 * line from the intel manual that guarantees an IPI to
@@ -382,8 +410,14 @@ asmlinkage void smp_invalidate_interrupt
 		} else
 			leave_mm(cpu);
 	}
+#ifdef CONFIG_ADEOS_CORE
+	if (!adp_pipelined)
+#endif /* CONFIG_ADEOS_CORE */
 	ack_APIC_irq();
+
 	clear_bit(cpu, &flush_cpumask);
+out:
+	preempt_enable();
 }
 
 static void flush_tlb_others (unsigned long cpumask, struct mm_struct *mm,
@@ -433,17 +467,22 @@ static void flush_tlb_others (unsigned l
 void flush_tlb_current_task(void)
 {
 	struct mm_struct *mm = current->mm;
-	unsigned long cpu_mask = mm->cpu_vm_mask & ~(1 << smp_processor_id());
+	unsigned long cpu_mask;
 
+	preempt_disable();
+	cpu_mask = mm->cpu_vm_mask & ~(1UL << smp_processor_id());
 	local_flush_tlb();
 	if (cpu_mask)
 		flush_tlb_others(cpu_mask, mm, FLUSH_ALL);
+	preempt_enable();
 }
 
 void flush_tlb_mm (struct mm_struct * mm)
 {
-	unsigned long cpu_mask = mm->cpu_vm_mask & ~(1 << smp_processor_id());
+	unsigned long cpu_mask;
 
+	preempt_disable();
+	cpu_mask = mm->cpu_vm_mask & ~(1UL << smp_processor_id());
 	if (current->active_mm == mm) {
 		if (current->mm)
 			local_flush_tlb();
@@ -452,13 +491,16 @@ void flush_tlb_mm (struct mm_struct * mm
 	}
 	if (cpu_mask)
 		flush_tlb_others(cpu_mask, mm, FLUSH_ALL);
+	preempt_enable();
 }
 
 void flush_tlb_page(struct vm_area_struct * vma, unsigned long va)
 {
 	struct mm_struct *mm = vma->vm_mm;
-	unsigned long cpu_mask = mm->cpu_vm_mask & ~(1 << smp_processor_id());
+	unsigned long cpu_mask;
 
+	preempt_disable();
+	cpu_mask = mm->cpu_vm_mask & ~(1UL << smp_processor_id());
 	if (current->active_mm == mm) {
 		if(current->mm)
 			__flush_tlb_one(va);
@@ -468,6 +510,7 @@ void flush_tlb_page(struct vm_area_struc
 
 	if (cpu_mask)
 		flush_tlb_others(cpu_mask, mm, va);
+	preempt_enable();
 }
 
 static inline void do_flush_tlb_all_local(void)
@@ -486,9 +529,11 @@ static void flush_tlb_all_ipi(void* info
 
 void flush_tlb_all(void)
 {
+	preempt_disable();
 	smp_call_function (flush_tlb_all_ipi,0,1,1);
 
 	do_flush_tlb_all_local();
+	preempt_enable();
 }
 
 /*
@@ -572,7 +617,7 @@ int smp_call_function (void (*func) (voi
 static void stop_this_cpu (void * dummy)
 {
 	/*
-	 * Remove this CPU:
+	 * Remove this CPU: assumes preemption is disabled
 	 */
 	clear_bit(smp_processor_id(), &cpu_online_map);
 	__cli();
@@ -603,6 +648,9 @@ void smp_send_stop(void)
  */
 asmlinkage void smp_reschedule_interrupt(void)
 {
+#ifdef CONFIG_ADEOS_CORE
+	if (!adp_pipelined)
+#endif /* CONFIG_ADEOS_CORE */
 	ack_APIC_irq();
 }
 
@@ -612,6 +660,9 @@ asmlinkage void smp_call_function_interr
 	void *info = call_data->info;
 	int wait = call_data->wait;
 
+#ifdef CONFIG_ADEOS_CORE
+	if (!adp_pipelined)
+#endif /* CONFIG_ADEOS_CORE */
 	ack_APIC_irq();
 	/*
 	 * Notify initiating CPU that I've grabbed the data and am
@@ -629,3 +680,14 @@ asmlinkage void smp_call_function_interr
 	}
 }
 
+#ifdef CONFIG_ADEOS_CORE
+
+void __adeos_send_IPI_allbutself (int vector) {
+    send_IPI_allbutself(vector);
+}
+
+void __adeos_send_IPI_other (int cpu, int vector) {
+    send_IPI_mask(1 << cpu,vector);
+}
+
+#endif /* CONFIG_ADEOS_CORE */
diff -uNrp linux-2.4.23/arch/i386/kernel/time.c linux-2.4.23-fusion/arch/i386/kernel/time.c
--- linux-2.4.23/arch/i386/kernel/time.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/time.c	2004-01-11 15:29:36.000000000 +0100
@@ -211,13 +211,22 @@ static unsigned long do_slow_gettimeoffs
 
 			int i;
 
+#ifdef CONFIG_ADEOS_CORE
+			unsigned long flags;
+			adeos_spin_lock_irqsave(&i8259A_lock, flags);
+#else /* !CONFIG_ADEOS_CORE */
 			spin_lock(&i8259A_lock);
+#endif /* CONFIG_ADEOS_CORE */
 			/*
 			 * This is tricky when I/O APICs are used;
 			 * see do_timer_interrupt().
 			 */
 			i = inb(0x20);
+#ifdef CONFIG_ADEOS_CORE
+			adeos_spin_unlock_irqrestore(&i8259A_lock, flags);
+#else /* !CONFIG_ADEOS_CORE */
 			spin_unlock(&i8259A_lock);
+#endif /* CONFIG_ADEOS_CORE */
 
 			/* assumption about timer being IRQ0 */
 			if (i & 0x01) {
@@ -578,11 +587,20 @@ static inline void do_timer_interrupt(in
 		 * This will also deassert NMI lines for the watchdog if run
 		 * on an 82489DX-based system.
 		 */
+#ifdef CONFIG_ADEOS_CORE
+		unsigned long flags;
+		adeos_spin_lock_irqsave(&i8259A_lock, flags);
+#else /* !CONFIG_ADEOS_CORE */
 		spin_lock(&i8259A_lock);
+#endif /* CONFIG_ADEOS_CORE */
 		outb(0x0c, 0x20);
 		/* Ack the IRQ; AEOI will end it automatically. */
 		inb(0x20);
+#ifdef CONFIG_ADEOS_CORE
+		adeos_spin_unlock_irqrestore(&i8259A_lock, flags);
+#else /* !CONFIG_ADEOS_CORE */
 		spin_unlock(&i8259A_lock);
+#endif /* CONFIG_ADEOS_CORE */
 	}
 #endif
 
@@ -675,6 +693,7 @@ static void timer_interrupt(int irq, voi
 
 		rdtscl(last_tsc_low);
 
+#ifndef CONFIG_ADEOS_CORE
 		spin_lock(&i8253_lock);
 		outb_p(0x00, 0x43);     /* latch the count ASAP */
 
@@ -706,6 +725,7 @@ static void timer_interrupt(int irq, voi
 
 		count = ((LATCH-1) - count) * TICK_SIZE;
 		delay_at_last_interrupt = (count + LATCH/2) / LATCH;
+#endif /* !CONFIG_ADEOS_CORE */
 	}
 
 	do_timer_interrupt(irq, NULL, regs);
diff -uNrp linux-2.4.23/arch/i386/kernel/traps.c linux-2.4.23-fusion/arch/i386/kernel/traps.c
--- linux-2.4.23/arch/i386/kernel/traps.c	2002-11-29 00:53:09.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/kernel/traps.c	2004-01-11 15:29:36.000000000 +0100
@@ -751,6 +751,8 @@ asmlinkage void do_spurious_interrupt_bu
  *
  * Careful.. There are problems with IBM-designed IRQ13 behaviour.
  * Don't touch unless you *really* know how it works.
+ *
+ * Must be called with kernel preemption disabled.
  */
 asmlinkage void math_state_restore(struct pt_regs regs)
 {
diff -uNrp linux-2.4.23/arch/i386/lib/dec_and_lock.c linux-2.4.23-fusion/arch/i386/lib/dec_and_lock.c
--- linux-2.4.23/arch/i386/lib/dec_and_lock.c	2000-07-08 03:20:16.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/lib/dec_and_lock.c	2004-01-11 15:29:36.000000000 +0100
@@ -8,6 +8,7 @@
  */
 
 #include <linux/spinlock.h>
+#include <linux/sched.h>
 #include <asm/atomic.h>
 
 int atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock)
diff -uNrp linux-2.4.23/arch/i386/mm/fault.c linux-2.4.23-fusion/arch/i386/mm/fault.c
--- linux-2.4.23/arch/i386/mm/fault.c	2002-11-29 00:53:09.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/mm/fault.c	2004-01-11 15:29:36.000000000 +0100
@@ -153,7 +153,11 @@ asmlinkage void do_page_fault(struct pt_
 
 	/* It's safe to allow irq's after cr2 has been saved */
 	if (regs->eflags & X86_EFLAGS_IF)
-		local_irq_enable();
+#ifdef CONFIG_ADEOS_CORE
+	    adeos_hw_sti();
+#else /* !CONFIG_ADEOS_CORE */
+	    local_irq_enable();
+#endif /* CONFIG_ADEOS_CORE */
 
 	tsk = current;
 
diff -uNrp linux-2.4.23/arch/i386/mm/init.c linux-2.4.23-fusion/arch/i386/mm/init.c
--- linux-2.4.23/arch/i386/mm/init.c	2003-06-13 16:51:29.000000000 +0200
+++ linux-2.4.23-fusion/arch/i386/mm/init.c	2004-01-11 15:29:36.000000000 +0100
@@ -46,6 +46,7 @@ static unsigned long totalhigh_pages;
 int do_check_pgt_cache(int low, int high)
 {
 	int freed = 0;
+	preempt_disable();
 	if(pgtable_cache_size > high) {
 		do {
 			if (pgd_quicklist) {
@@ -62,6 +63,7 @@ int do_check_pgt_cache(int low, int high
 			}
 		} while(pgtable_cache_size > low);
 	}
+	preempt_enable();
 	return freed;
 }
 
diff -uNrp linux-2.4.23/arch/i386/mm/ioremap.c linux-2.4.23-fusion/arch/i386/mm/ioremap.c
--- linux-2.4.23/arch/i386/mm/ioremap.c	2003-11-28 23:18:27.000000000 +0100
+++ linux-2.4.23-fusion/arch/i386/mm/ioremap.c	2004-01-11 15:29:36.000000000 +0100
@@ -81,6 +81,9 @@ static int remap_area_pages(unsigned lon
 		if (remap_area_pmd(pmd, address, end - address,
 					 phys_addr + address, flags))
 			break;
+#ifdef CONFIG_ADEOS_CORE
+		set_pgdir(address, *dir);
+#endif /* CONFIG_ADEOS_CORE */
 		error = 0;
 		address = (address + PGDIR_SIZE) & PGDIR_MASK;
 		dir++;
diff -uNrp linux-2.4.23/arch/mips/config-shared.in linux-2.4.23-fusion/arch/mips/config-shared.in
--- linux-2.4.23/arch/mips/config-shared.in	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/mips/config-shared.in	2004-01-11 15:29:36.000000000 +0100
@@ -867,6 +867,7 @@ else
    define_bool CONFIG_HOTPLUG_PCI n
 fi
 
+dep_bool 'Preemptible Kernel' CONFIG_PREEMPT $CONFIG_NEW_IRQ
 bool 'System V IPC' CONFIG_SYSVIPC
 bool 'BSD Process Accounting' CONFIG_BSD_PROCESS_ACCT
 bool 'Sysctl support' CONFIG_SYSCTL
diff -uNrp linux-2.4.23/arch/mips/kernel/i8259.c linux-2.4.23-fusion/arch/mips/kernel/i8259.c
--- linux-2.4.23/arch/mips/kernel/i8259.c	2003-08-25 13:44:40.000000000 +0200
+++ linux-2.4.23-fusion/arch/mips/kernel/i8259.c	2004-01-11 15:29:36.000000000 +0100
@@ -8,6 +8,7 @@
  * Copyright (C) 1992 Linus Torvalds
  * Copyright (C) 1994 - 2000 Ralf Baechle
  */
+#include <linux/sched.h>
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/ioport.h>
diff -uNrp linux-2.4.23/arch/mips/kernel/irq.c linux-2.4.23-fusion/arch/mips/kernel/irq.c
--- linux-2.4.23/arch/mips/kernel/irq.c	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/mips/kernel/irq.c	2004-01-11 15:29:36.000000000 +0100
@@ -8,6 +8,8 @@
  * Copyright (C) 1992 Linus Torvalds
  * Copyright (C) 1994 - 2000 Ralf Baechle
  */
+
+#include <linux/sched.h>
 #include <linux/config.h>
 #include <linux/kernel.h>
 #include <linux/delay.h>
@@ -19,11 +21,13 @@
 #include <linux/slab.h>
 #include <linux/mm.h>
 #include <linux/random.h>
-#include <linux/sched.h>
+#include <linux/spinlock.h>
+#include <linux/ptrace.h>
 
 #include <asm/atomic.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
+#include <asm/debug.h>
 
 /*
  * Controller mappings for all interrupt sources:
@@ -429,6 +433,8 @@ asmlinkage unsigned int do_IRQ(int irq, 
 	struct irqaction * action;
 	unsigned int status;
 
+	preempt_disable();
+
 	kstat.irqs[cpu][irq]++;
 	spin_lock(&desc->lock);
 	desc->handler->ack(irq);
@@ -490,6 +496,27 @@ out:
 
 	if (softirq_pending(cpu))
 		do_softirq();
+
+#if defined(CONFIG_PREEMPT)
+	while (--current->preempt_count == 0) {
+		db_assert(intr_off());
+		db_assert(!in_interrupt());
+
+		if (current->need_resched == 0) {
+			break;
+		}
+
+		current->preempt_count ++;
+		sti();
+		if (user_mode(regs)) {
+			schedule();
+		} else {
+			preempt_schedule();
+		}
+		cli();
+	}
+#endif
+
 	return 1;
 }
 
diff -uNrp linux-2.4.23/arch/mips/mm/extable.c linux-2.4.23-fusion/arch/mips/mm/extable.c
--- linux-2.4.23/arch/mips/mm/extable.c	2002-11-29 00:53:10.000000000 +0100
+++ linux-2.4.23-fusion/arch/mips/mm/extable.c	2004-01-11 15:29:36.000000000 +0100
@@ -3,6 +3,7 @@
  */
 #include <linux/config.h>
 #include <linux/module.h>
+#include <linux/sched.h>
 #include <linux/spinlock.h>
 #include <asm/uaccess.h>
 
diff -uNrp linux-2.4.23/arch/ppc/config.in linux-2.4.23-fusion/arch/ppc/config.in
--- linux-2.4.23/arch/ppc/config.in	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/config.in	2004-01-11 15:29:36.000000000 +0100
@@ -135,6 +135,8 @@ if [ "$CONFIG_SMP" = "y" ]; then
   int  'Maximum number of CPUs (2-32)' CONFIG_NR_CPUS 32
 fi
 
+bool 'Preemptible kernel support' CONFIG_PREEMPT
+
 if [ "$CONFIG_6xx" = "y" -a "$CONFIG_8260" = "n" ];then
   bool 'AltiVec Support' CONFIG_ALTIVEC
   bool 'Thermal Management Support' CONFIG_TAU
diff -uNrp linux-2.4.23/arch/ppc/kernel/entry.S linux-2.4.23-fusion/arch/ppc/kernel/entry.S
--- linux-2.4.23/arch/ppc/kernel/entry.S	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/kernel/entry.S	2004-01-11 15:29:36.000000000 +0100
@@ -283,6 +283,46 @@ ret_from_intercept:
 	 */
 	cmpi	0,r3,0
 	beq	restore
+#ifdef CONFIG_PREEMPT
+	lwz	r3,PREEMPT_COUNT(r2)
+	cmpi	0,r3,1
+	bge	ret_from_except
+	lwz	r5,_MSR(r1)
+	andi.	r5,r5,MSR_PR
+	bne	do_signal_ret
+	lwz	r5,NEED_RESCHED(r2)
+	cmpi	0,r5,0
+	beq	ret_from_except
+	lis	r3,irq_stat@h
+	ori	r3,r3,irq_stat@l
+#ifdef CONFIG_SMP
+	lwz     r5,CPU(r2)
+	rlwinm  r5,r5,5,0,26
+	add     r3,r3,r5
+#endif
+	lwz	r5,4(r3)
+	lwz	r3,8(r3)
+	add	r3,r3,r5
+	cmpi	0,r3,0
+	bne	ret_from_except
+	lwz	r3,PREEMPT_COUNT(r2)
+	addi	r3,r3,1
+	stw	r3,PREEMPT_COUNT(r2)
+	mfmsr	r0
+	ori	r0,r0,MSR_EE
+	mtmsr	r0
+	sync
+	bl	preempt_schedule
+	mfmsr	r0
+	rlwinm	r0,r0,0,17,15
+	mtmsr	r0
+	sync
+	lwz	r3,PREEMPT_COUNT(r2)
+	subi	r3,r3,1
+	stw	r3,PREEMPT_COUNT(r2)
+	li	r3,1
+	b	ret_from_intercept
+#endif /* CONFIG_PREEMPT */
 	.globl	ret_from_except
 ret_from_except:
 	lwz	r3,_MSR(r1)	/* Returning to user mode? */
diff -uNrp linux-2.4.23/arch/ppc/kernel/irq.c linux-2.4.23-fusion/arch/ppc/kernel/irq.c
--- linux-2.4.23/arch/ppc/kernel/irq.c	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/kernel/irq.c	2004-01-11 15:29:36.000000000 +0100
@@ -551,6 +551,34 @@ int do_IRQ(struct pt_regs *regs)
 	return 1; /* lets ret_from_int know we can do checks */
 }
 
+#ifdef CONFIG_PREEMPT
+int
+preempt_intercept(struct pt_regs *regs)
+{
+	int ret;
+
+	preempt_disable();
+
+	switch(regs->trap) {
+	case 0x500:
+		ret = do_IRQ(regs);
+		break;
+#ifndef CONFIG_4xx
+	case 0x900:
+#else
+	case 0x1000:
+#endif
+		ret = timer_interrupt(regs);
+		break;
+	default:
+		BUG();
+	}
+
+	preempt_enable();
+	return ret;
+}
+#endif /* CONFIG_PREEMPT */
+
 unsigned long probe_irq_on (void)
 {
 	return 0;
@@ -647,11 +675,13 @@ static inline void wait_on_irq(int cpu)
 				show("wait_on_irq");
 				count = ~0;
 			}
+			preempt_disable();
 			__sti();
 			/* don't worry about the lock race Linus found
 			 * on intel here. -- Cort
 			 */
 			__cli();
+			preempt_enable_no_resched();
 			if (atomic_read(&global_irq_count))
 				continue;
 			if (global_irq_lock)
@@ -727,6 +757,8 @@ static inline void get_irqlock(int cpu)
 	global_irq_holder = cpu;
 }
 
+#define	EFLAGS_IF_SHIFT	15
+
 /*
  * A global "cli()" while in an interrupt context
  * turns into just a local cli(). Interrupts
@@ -744,9 +776,10 @@ void __global_cli(void)
 	unsigned long flags;
 
 	__save_flags(flags);
-	if (flags & (1 << 15)) {
-		int cpu = smp_processor_id();
+	if (flags & (1 << EFLAGS_IF_SHIFT)) {
+		int cpu;
 		__cli();
+		cpu = smp_processor_id();
 		if (!local_irq_count(cpu))
 			get_irqlock(cpu);
 	}
@@ -754,11 +787,14 @@ void __global_cli(void)
 
 void __global_sti(void)
 {
-	int cpu = smp_processor_id();
+	int cpu;
 
+	preempt_disable();
+	cpu = smp_processor_id();
 	if (!local_irq_count(cpu))
 		release_irqlock(cpu);
 	__sti();
+	preempt_enable();
 }
 
 /*
@@ -773,19 +809,23 @@ unsigned long __global_save_flags(void)
 	int retval;
 	int local_enabled;
 	unsigned long flags;
+	int cpu;
 
 	__save_flags(flags);
-	local_enabled = (flags >> 15) & 1;
+	local_enabled = (flags >> EFLAGS_IF_SHIFT) & 1;
 	/* default to local */
 	retval = 2 + local_enabled;
 
 	/* check for global flags if we're not in an interrupt */
-	if (!local_irq_count(smp_processor_id())) {
+	preempt_disable();
+	cpu = smp_processor_id();
+	if (!local_irq_count(cpu)) {
 		if (local_enabled)
 			retval = 1;
-		if (global_irq_holder == (unsigned char) smp_processor_id())
+		if (global_irq_holder == cpu)
 			retval = 0;
 	}
+	preempt_enable();
 	return retval;
 }
 
diff -uNrp linux-2.4.23/arch/ppc/kernel/mk_defs.c linux-2.4.23-fusion/arch/ppc/kernel/mk_defs.c
--- linux-2.4.23/arch/ppc/kernel/mk_defs.c	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/kernel/mk_defs.c	2004-01-11 15:29:36.000000000 +0100
@@ -39,6 +39,9 @@ main(void)
 	DEFINE(SIGPENDING, offsetof(struct task_struct, sigpending));
 	DEFINE(THREAD, offsetof(struct task_struct, thread));
 	DEFINE(MM, offsetof(struct task_struct, mm));
+#ifdef CONFIG_PREEMPT
+	DEFINE(PREEMPT_COUNT, offsetof(struct task_struct, preempt_count));
+#endif
 	DEFINE(ACTIVE_MM, offsetof(struct task_struct, active_mm));
 	DEFINE(TASK_STRUCT_SIZE, sizeof(struct task_struct));
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
diff -uNrp linux-2.4.23/arch/ppc/kernel/open_pic.c linux-2.4.23-fusion/arch/ppc/kernel/open_pic.c
--- linux-2.4.23/arch/ppc/kernel/open_pic.c	2003-08-25 13:44:40.000000000 +0200
+++ linux-2.4.23-fusion/arch/ppc/kernel/open_pic.c	2004-01-11 15:29:36.000000000 +0100
@@ -594,19 +594,24 @@ void openpic_request_IPIs(void)
 void __init do_openpic_setup_cpu(void)
 {
  	int i;
-	u32 msk = 1 << smp_hw_index[smp_processor_id()];
+#ifdef CONFIG_IRQ_ALL_CPUS
+	u32 msk;
+#endif /* CONFIG_IRQ_ALL_CPUS */
 
 	spin_lock(&openpic_setup_lock);
 
 #ifdef CONFIG_IRQ_ALL_CPUS
+	msk = 1 << smp_hw_index[smp_processor_id()];
+
  	/* let the openpic know we want intrs. default affinity
  	 * is 0xffffffff until changed via /proc
  	 * That's how it's done on x86. If we want it differently, then
  	 * we should make sure we also change the default values of irq_affinity
  	 * in irq.c.
  	 */
- 	for (i = 0; i < NumSources; i++)
+ 	for (i = 0; i < NumSources; i++) {
 		openpic_mapirq(i, msk, ~0U);
+	}
 #endif /* CONFIG_IRQ_ALL_CPUS */
  	openpic_set_priority(0);
 
diff -uNrp linux-2.4.23/arch/ppc/kernel/setup.c linux-2.4.23-fusion/arch/ppc/kernel/setup.c
--- linux-2.4.23/arch/ppc/kernel/setup.c	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/kernel/setup.c	2004-01-11 15:29:36.000000000 +0100
@@ -502,6 +502,20 @@ machine_init(unsigned long r3, unsigned 
 	strcpy(cmd_line, CONFIG_CMDLINE);
 #endif /* CONFIG_CMDLINE */
 
+#ifdef CONFIG_PREEMPT
+	/* Override the irq routines for external & timer interrupts here,
+	 * as the MMU has only been minimally setup at this point and
+	 * there are no protections on page zero.
+	 */
+	{
+		extern int preempt_intercept(struct pt_regs *);
+	
+		do_IRQ_intercept = (unsigned long) &preempt_intercept;
+		timer_interrupt_intercept = (unsigned long) &preempt_intercept;
+
+	}
+#endif /* CONFIG_PREEMPT */
+
 	platform_init(r3, r4, r5, r6, r7);
 
 	if (ppc_md.progress)
diff -uNrp linux-2.4.23/arch/ppc/kernel/temp.c linux-2.4.23-fusion/arch/ppc/kernel/temp.c
--- linux-2.4.23/arch/ppc/kernel/temp.c	2003-08-25 13:44:40.000000000 +0200
+++ linux-2.4.23-fusion/arch/ppc/kernel/temp.c	2004-01-11 15:29:36.000000000 +0100
@@ -138,7 +138,7 @@ void TAUException(struct pt_regs * regs)
 
 static void tau_timeout(void * info)
 {
-	unsigned long cpu = smp_processor_id();
+	unsigned long cpu;
 	unsigned long flags;
 	int size;
 	int shrink;
@@ -146,6 +146,8 @@ static void tau_timeout(void * info)
 	/* disabling interrupts *should* be okay */
 	save_flags(flags); cli();
 
+	cpu = smp_processor_id();
+
 #ifndef CONFIG_TAU_INT
 	TAUupdate(cpu);
 #endif
@@ -191,13 +193,15 @@ static void tau_timeout(void * info)
 
 static void tau_timeout_smp(unsigned long unused)
 {
-
 	/* schedule ourselves to be run again */
 	mod_timer(&tau_timer, jiffies + shrink_timer) ;
+
+	preempt_disable();
 #ifdef CONFIG_SMP
 	smp_call_function(tau_timeout, NULL, 1, 0);
 #endif
 	tau_timeout(NULL);
+	preempt_enable();
 }
 
 /*
diff -uNrp linux-2.4.23/arch/ppc/lib/dec_and_lock.c linux-2.4.23-fusion/arch/ppc/lib/dec_and_lock.c
--- linux-2.4.23/arch/ppc/lib/dec_and_lock.c	2001-11-16 19:10:08.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/lib/dec_and_lock.c	2004-01-11 15:29:36.000000000 +0100
@@ -1,4 +1,5 @@
 #include <linux/module.h>
+#include <linux/sched.h>
 #include <linux/spinlock.h>
 #include <asm/atomic.h>
 #include <asm/system.h>
diff -uNrp linux-2.4.23/arch/ppc/mm/init.c linux-2.4.23-fusion/arch/ppc/mm/init.c
--- linux-2.4.23/arch/ppc/mm/init.c	2003-11-28 23:18:28.000000000 +0100
+++ linux-2.4.23-fusion/arch/ppc/mm/init.c	2004-01-11 15:29:36.000000000 +0100
@@ -126,6 +126,9 @@ unsigned long __max_memory;
 int do_check_pgt_cache(int low, int high)
 {
 	int freed = 0;
+
+	preempt_disable();
+
 	if (pgtable_cache_size > high) {
 		do {
                         if (pgd_quicklist) {
@@ -138,6 +141,9 @@ int do_check_pgt_cache(int low, int high
 			}
 		} while (pgtable_cache_size > low);
 	}
+
+	preempt_enable();
+
 	return freed;
 }
 
diff -uNrp linux-2.4.23/arch/ppc/mm/tlb.c linux-2.4.23-fusion/arch/ppc/mm/tlb.c
--- linux-2.4.23/arch/ppc/mm/tlb.c	2003-08-25 13:44:40.000000000 +0200
+++ linux-2.4.23-fusion/arch/ppc/mm/tlb.c	2004-01-11 15:29:36.000000000 +0100
@@ -58,11 +58,14 @@ local_flush_tlb_all(void)
 	 * we can and should dispense with flush_tlb_all().
 	 *  -- paulus.
 	 */
+
+	preempt_disable();
 	local_flush_tlb_range(&init_mm, TASK_SIZE, ~0UL);
 
 #ifdef CONFIG_SMP
 	smp_send_tlb_invalidate(0);
 #endif /* CONFIG_SMP */
+	preempt_enable();
 }
 
 /*
@@ -73,8 +76,10 @@ local_flush_tlb_all(void)
 void
 local_flush_tlb_mm(struct mm_struct *mm)
 {
+	preempt_disable();
 	if (Hash == 0) {
 		_tlbia();
+		preempt_enable();
 		return;
 	}
 
@@ -88,6 +93,7 @@ local_flush_tlb_mm(struct mm_struct *mm)
 #ifdef CONFIG_SMP
 	smp_send_tlb_invalidate(0);
 #endif
+	preempt_enable();
 }
 
 void
@@ -97,8 +103,10 @@ local_flush_tlb_page(struct vm_area_stru
 	pmd_t *pmd;
 	pte_t *pte;
 
+	preempt_disable();
 	if (Hash == 0) {
 		_tlbie(vmaddr);
+		preempt_enable();
 		return;
 	}
 	mm = (vmaddr < TASK_SIZE)? vma->vm_mm: &init_mm;
@@ -111,6 +119,7 @@ local_flush_tlb_page(struct vm_area_stru
 #ifdef CONFIG_SMP
 	smp_send_tlb_invalidate(0);
 #endif
+	preempt_enable();
 }
 
 
@@ -127,13 +136,17 @@ local_flush_tlb_range(struct mm_struct *
 	unsigned long pmd_end;
 	unsigned int ctx = mm->context;
 
+	preempt_disable();
 	if (Hash == 0) {
 		_tlbia();
+		preempt_enable();
 		return;
 	}
 	start &= PAGE_MASK;
-	if (start >= end)
+	if (start >= end) {
+		preempt_enable();
 		return;
+	}
 	pmd = pmd_offset(pgd_offset(mm, start), start);
 	do {
 		pmd_end = (start + PGDIR_SIZE) & PGDIR_MASK;
@@ -156,4 +169,5 @@ local_flush_tlb_range(struct mm_struct *
 #ifdef CONFIG_SMP
 	smp_send_tlb_invalidate(0);
 #endif
+	preempt_enable();
 }
diff -uNrp linux-2.4.23/drivers/Makefile linux-2.4.23-fusion/drivers/Makefile
--- linux-2.4.23/drivers/Makefile	2003-11-28 23:18:29.000000000 +0100
+++ linux-2.4.23-fusion/drivers/Makefile	2004-01-11 15:29:36.000000000 +0100
@@ -8,7 +8,7 @@
 
 mod-subdirs :=	dio hil mtd sbus video macintosh usb input telephony ide \
 		message/i2o message/fusion scsi md ieee1394 pnp isdn atm \
-		fc4 net/hamradio i2c acpi bluetooth usb/gadget
+		fc4 net/hamradio i2c acpi bluetooth usb/gadget adeos
 
 subdir-y :=	parport char block net sound misc media cdrom hotplug
 subdir-m :=	$(subdir-y)
@@ -49,4 +49,6 @@ subdir-$(CONFIG_ACPI_BOOT)	+= acpi
 
 subdir-$(CONFIG_BLUEZ)		+= bluetooth
 
+subdir-$(CONFIG_ADEOS)		+= adeos
+
 include $(TOPDIR)/Rules.make
diff -uNrp linux-2.4.23/drivers/adeos/Makefile linux-2.4.23-fusion/drivers/adeos/Makefile
--- linux-2.4.23/drivers/adeos/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/drivers/adeos/Makefile	2004-01-11 15:29:36.000000000 +0100
@@ -0,0 +1,26 @@
+#
+# Makefile for the Adeos driver.
+#
+
+O_TARGET := driver.o
+
+export-objs	:= generic.o
+list-multi	:= adeos.o
+adeos-objs	:= generic.o
+
+ifdef CONFIG_X86
+adeos-objs	+= x86.o
+endif
+
+ifdef CONFIG_ARM
+ifdef CONFIG_UCLINUX
+adeos-objs	+= armnommu.o
+endif
+endif
+
+obj-$(CONFIG_ADEOS)	+= adeos.o
+
+include $(TOPDIR)/Rules.make
+
+adeos.o: $(adeos-objs)
+	$(LD) -r -o $@ $(adeos-objs)
diff -uNrp linux-2.4.23/drivers/adeos/generic.c linux-2.4.23-fusion/drivers/adeos/generic.c
--- linux-2.4.23/drivers/adeos/generic.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/drivers/adeos/generic.c	2004-01-11 21:40:09.000000000 +0100
@@ -0,0 +1,771 @@
+/*
+ *   linux/drivers/adeos/generic.c
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-independent ADEOS services.
+ */
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/wrapper.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/irq.h>
+
+MODULE_DESCRIPTION("Adeos nanokernel");
+MODULE_AUTHOR("Philippe Gerum");
+MODULE_LICENSE("GPL");
+
+extern spinlock_t __adeos_pipelock;
+
+extern unsigned long __adeos_virtual_irq_map;
+
+extern struct list_head __adeos_pipeline;
+
+/* adeos_register_domain() -- Add a new domain to the system. All
+   client domains must call this routine to register themselves to
+   ADEOS before using its services. */
+
+int adeos_register_domain (adomain_t *adp, adattr_t *attr)
+
+{
+    struct list_head *pos;
+    unsigned long flags;
+    adeos_declare_cpuid;
+    int n;
+
+    if (adp_current != adp_root)
+	{
+	printk(KERN_WARNING "ADEOS: Only the root domain may register a new domain.\n");
+	return -EPERM;
+	}
+
+    flags = adeos_critical_enter(NULL);
+
+    list_for_each(pos,&__adeos_pipeline) {
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+	if (_adp->domid == attr->domid)
+            break;
+    }
+
+    adeos_critical_exit(flags);
+
+    if (pos != &__adeos_pipeline)
+	/* A domain with the given id already exists -- fail. */
+	return -EBUSY;
+
+    for (n = 0; n < ADEOS_NR_CPUS; n++)
+	adp->cpudata[n].status = 0;
+
+    /* A special case for domains who won't process events (i.e. no
+       entry). We have to mark them as suspended so that
+       adeos_suspend_domain() won't consider them, unless they
+       _actually_ receive events, which would lead to a panic
+       situation since they have no stack context... :o> */
+
+    if (attr->entry == NULL)
+	for (n = 0; n < ADEOS_NR_CPUS; n++)
+	    set_bit(IPIPE_SLEEP_FLAG,&adp->cpudata[n].status);
+
+    adp->name = attr->name;
+    adp->priority = attr->priority;
+    adp->domid = attr->domid;
+    adp->dswitch = attr->dswitch;
+    adp->ptd_setfun = attr->ptdset;
+    adp->ptd_getfun = attr->ptdget;
+    adp->ptd_keymap = 0;
+    adp->ptd_keycount = 0;
+    adp->ptd_keymax = attr->nptdkeys;
+
+    for (n = 0; n < ADEOS_NR_EVENTS; n++)
+	/* Event handlers must be cleared before the i-pipe stage is
+	   inserted since an exception may occur on behalf of the new
+	   emerging domain. */
+	adp->events[n].handler = NULL;
+
+    if (attr->entry != NULL)
+	__adeos_init_domain(adp,attr);
+
+    /* Insert the domain in the interrupt pipeline last, so it won't
+       be resumed for processing interrupts until it has a valid stack
+       context. */
+
+    __adeos_init_stage(adp);
+
+    INIT_LIST_HEAD(&adp->p_link);
+
+    flags = adeos_critical_enter(NULL);
+
+    list_for_each(pos,&__adeos_pipeline) {
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+	if (adp->priority > _adp->priority)
+            break;
+    }
+
+    list_add_tail(&adp->p_link,pos);
+
+    adeos_critical_exit(flags);
+
+    printk("ADEOS: Domain %s registered.\n",adp->name);
+
+    /* Finally, allow the new domain to perform its initialization
+       chores on behalf of its own stack context. */
+
+    if (attr->entry != NULL)
+	{
+	__adeos_switch_to(adp,cpuid);
+
+	adeos_reload_cpuid();	/* Processor might have changed. */
+
+	if (!test_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status) &&
+	    adp_root->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+	}
+
+    return 0;
+}
+
+/* adeos_unregister_domain() -- Remove a domain from the system. All
+   client domains must call this routine to unregister themselves from
+   the ADEOS layer. */
+
+int adeos_unregister_domain (adomain_t *adp)
+
+{
+    unsigned long flags;
+    unsigned event;
+
+    if (adp_current != adp_root)
+	{
+	printk(KERN_WARNING "ADEOS: Only the root domain may unregister a domain.\n");
+	return -EPERM;
+	}
+
+    if (adp == adp_root)
+	{
+	printk(KERN_WARNING "ADEOS: Cannot unregister the root domain.\n");
+	return -EPERM;
+	}
+
+    for (event = 0; event < ADEOS_NR_EVENTS; event++)
+	/* Need this to update the monitor count. */
+	adeos_catch_event(event,NULL);
+
+    /* Simply remove the domain from the pipeline and we are almost
+       done. */
+
+    flags = adeos_critical_enter(NULL);
+    list_del_init(&adp->p_link);
+    adeos_critical_exit(flags);
+
+    __adeos_cleanup_domain(adp);
+
+    printk("ADEOS: Domain %s unregistered.\n",adp->name);
+
+    return 0;
+}
+
+/* adeos_renice_domain() -- Change the priority of the current
+   domain. This affects the position of the domain in the interrupt
+   pipeline. The greater the priority value, the earlier the domain is
+   informed of incoming events when the pipeline is processed. */
+
+void adeos_renice_domain (int newpri)
+
+{
+    adomain_t *adp, *nadp = NULL;
+    unsigned long lflags, xflags;
+    struct list_head *pos;
+    adeos_declare_cpuid;
+
+    /* Allows round-robin effect if newpri == oldpri. */
+
+    adp = adp_cpu_current[cpuid];
+
+    /* We do want adeos_critical_exit() to leave the IRQs masked, so
+       we first clear the interrupt bit before calling
+       adeos_critical_enter(). */
+ 
+    adeos_hw_local_irq_save(lflags);
+
+    xflags = adeos_critical_enter(NULL);
+
+    list_del_init(&adp->p_link);
+
+    list_for_each(pos,&__adeos_pipeline) {
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	if (newpri > _adp->priority)
+            break;
+
+	/* While scanning the pipeline from its head to the current
+	   domain's new position, pick the first domain which is
+	   entitled to preempt us. Such domain must be either:
+	   o in a preempted state (i.e. !sleeping),
+	   o or sleeping and unstalled with events to process. */
+
+	if (nadp == NULL &&
+	    (!test_bit(IPIPE_SLEEP_FLAG,&_adp->cpudata[cpuid].status) ||
+	     (!test_bit(IPIPE_STALL_FLAG,&_adp->cpudata[cpuid].status) &&
+	      _adp->cpudata[cpuid].irq_pending_hi != 0) ||
+	     test_bit(IPIPE_XPEND_FLAG,&_adp->cpudata[cpuid].status)))
+	    nadp = _adp;
+    }
+
+    list_add_tail(&adp->p_link,pos);
+    adp->priority = newpri;
+    
+    /* On SMP systems, we release the other CPUs but we still keep the
+       local IRQs masked so that we can't jump to a stale domain. */
+
+    adeos_critical_exit(xflags);
+
+    if (nadp == NULL)
+	goto unmask_and_exit;
+
+    __adeos_switch_to(nadp,cpuid);
+
+    adeos_reload_cpuid(); /* Processor might have changed. */
+
+    /* Try sync'ing pending interrupts on return from our preemption
+       point. */
+
+    if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+	adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+	__adeos_sync_stage();
+
+    /* Note that we only need to sync interrupts here, since other
+       kind of events (i.e. synchronous ones) cannot flow across the
+       domain which triggers them down the pipeline. Since a more
+       prioritary domain was running up to now, there is no chance for
+       us to have such event pending. */
+
+ unmask_and_exit:
+
+    adeos_hw_local_irq_restore(lflags);
+}
+
+int __adeos_schedule_irq (unsigned irq, struct list_head *head)
+
+{
+    struct list_head *ln;
+    unsigned long flags;
+    adeos_declare_cpuid;
+
+    if (irq >= IPIPE_NR_IRQS ||
+	(adeos_virtual_irq_p(irq) && !test_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map)))
+	return -EINVAL;
+
+    adeos_hw_local_irq_save(flags);
+
+    ln = head;
+
+    while (ln != &__adeos_pipeline)
+	{
+	adomain_t *adp = list_entry(ln,adomain_t,p_link);
+
+	if (test_bit(IPIPE_HANDLE_FLAG,&adp->irqs[irq].control))
+	    {
+	    adp->cpudata[cpuid].irq_hits[irq]++;
+	    __adeos_set_irq_bit(adp,cpuid,irq);
+	    adeos_hw_local_irq_restore(flags);
+	    return 1;
+	    }
+
+	ln = adp->p_link.next;
+	}
+
+    adeos_hw_local_irq_restore(flags);
+
+    return 0;
+}
+
+/* adeos_propagate_irq() -- Force a given IRQ propagation on behalf of
+   a running interrupt handler to the next domain down the pipeline.
+   Returns non-zero if a domain has received the interrupt
+   notification, zero otherwise.
+   This call is useful for handling shared interrupts among domains.
+   e.g. pipeline = [domain-A]---[domain-B]...
+   Both domains share IRQ #X.
+   - domain-A handles IRQ #X but does not pass it down (i.e. Terminate
+   or Dynamic interrupt control mode)
+   - domain-B handles IRQ #X (i.e. Terminate or Accept interrupt
+   control modes).
+   When IRQ #X is raised, domain-A's handler determines whether it
+   should process the interrupt by identifying its source. If not,
+   adeos_propagate_irq() is called so that the next domain down the
+   pipeline which handles IRQ #X is given a chance to process it. This
+   process can be repeated until the end of the pipeline is
+   reached. */
+
+int adeos_propagate_irq (unsigned irq) {
+
+    return __adeos_schedule_irq(irq,adp_current->p_link.next);
+}
+
+/* adeos_schedule_irq() -- Almost the same as adeos_propagate_irq(),
+   but attempts to pend the interrupt for the current domain first. */
+
+int adeos_schedule_irq (unsigned irq) {
+
+    return __adeos_schedule_irq(irq,&adp_current->p_link);
+}
+
+unsigned long adeos_set_irq_affinity (unsigned irq, unsigned long cpumask)
+
+{
+#ifdef CONFIG_SMP
+     if (irq >= IPIPE_NR_XIRQS)
+	 /* Allow changing affinity of external IRQs only. */
+	 return 0;
+
+     if (smp_num_cpus > 1)
+	 return __adeos_set_irq_affinity(irq,cpumask);
+#endif /* CONFIG_SMP */
+
+    return 0;
+}
+
+/* adeos_alloc_irq() -- Allocate a virtual/soft pipelined interrupt.
+   Virtual interrupts are handled in exactly the same way than their
+   hw-generated counterparts. This is a very basic, one-way only,
+   inter-domain communication system (see adeos_trigger_irq()).  Note:
+   it is not necessary for a domain to allocate a virtual interrupt to
+   trap it using adeos_virtualize_irq(). The newly allocated VIRQ
+   number which can be passed to other IRQ-related services is
+   returned on success, zero otherwise (i.e. no more virtual interrupt
+   channel is available). */
+
+unsigned adeos_alloc_irq (void)
+
+{
+    unsigned long flags, irq = 0;
+    int ipos;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (__adeos_virtual_irq_map != ~0)
+	{
+	ipos = ffz(__adeos_virtual_irq_map);
+	set_bit(ipos,&__adeos_virtual_irq_map);
+	irq = ipos + IPIPE_VIRQ_BASE;
+	}
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return irq;
+}
+
+/* adeos_free_irq() -- Return a previously allocated virtual/soft
+   pipelined interrupt to the pool of allocatable interrupts. */
+
+int adeos_free_irq (unsigned irq)
+
+{
+    if (irq >= IPIPE_NR_IRQS)
+	return -EINVAL;
+
+    clear_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map);
+
+    return 0;
+}
+
+/* adeos_unstall_pipeline_from() -- Unstall the interrupt pipeline and
+   synchronize pending events from a given domain. */
+
+void adeos_unstall_pipeline_from (adomain_t *adp)
+
+{
+    struct list_head *pos;
+    adeos_declare_cpuid;
+
+    clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+    if (adp == adp_cpu_current[cpuid])
+	{
+	if (adp->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+
+	return;
+	}
+
+    /* Attempt to flush all events that might be pending at the
+       unstalled domain level. This code is roughly lifted from
+       drivers/adeos/x86.c:__adeos_walk_pipeline(). */
+
+    list_for_each(pos,&__adeos_pipeline) {
+
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	if (test_bit(IPIPE_STALL_FLAG,&_adp->cpudata[cpuid].status))
+	    break; /* Stalled stage -- do not go further. */
+
+	if (_adp->cpudata[cpuid].irq_pending_hi != 0)
+	    {
+	    /* Since the critical IPI might be triggered by the
+	       following actions, the current domain might not be
+	       linked to the pipeline anymore after its handler
+	       returns on SMP boxen, even if the domain remains valid
+	       (see adeos_unregister_domain()), so don't make any
+	       hazardous assumptions here. */
+
+	    if (_adp == adp_cpu_current[cpuid])
+		__adeos_sync_stage();
+	    else
+		{
+		__adeos_switch_to(_adp,cpuid);
+
+		adeos_reload_cpuid(); /* Processor might have changed. */
+
+		if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+		    adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+		    __adeos_sync_stage();
+		}
+	    
+	    break;
+	    }
+	else if (_adp == adp_cpu_current[cpuid])
+	    break;
+    }
+}
+
+/* adeos_catch_event_from() -- Interpose an event handler starting
+   from a given domain. */
+
+int adeos_catch_event_from (adomain_t *adp, unsigned event, void (*handler)(adevinfo_t *))
+
+{
+    if (event >= ADEOS_NR_EVENTS)
+	return -EINVAL;
+
+    if (!xchg(&adp->events[event].handler,handler))
+	{
+	if (handler)
+	    __adeos_event_monitors[event]++;
+	}
+    else if (!handler)
+	__adeos_event_monitors[event]--;
+
+    return 0;
+}
+
+void (*adeos_hook_dswitch(void (*handler)(void))) (void) {
+
+    return (void (*)(void))xchg(&adp_current->dswitch,handler);
+}
+
+void adeos_init_attr (adattr_t *attr)
+
+{
+    attr->name = "Anonymous";
+    attr->domid = 1;
+    attr->entry = NULL;
+    attr->estacksz = 0;	/* Let ADEOS choose a reasonable stack size */
+    attr->priority = ADEOS_ROOT_PRI;
+    attr->dswitch = NULL;
+    attr->nptdkeys = 0;
+    attr->ptdset = NULL;
+    attr->ptdget = NULL;
+}
+
+int adeos_alloc_ptdkey (void)
+
+{
+    unsigned long flags;
+    int key = -1;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (adp_current->ptd_keycount < adp_current->ptd_keymax)
+	{
+	key = ffz(adp_current->ptd_keymap);
+	set_bit(key,&adp_current->ptd_keymap);
+	adp_current->ptd_keycount++;
+	}
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return key;
+}
+
+int adeos_free_ptdkey (int key)
+
+{
+    unsigned long flags; 
+
+    if (key < 0 || key >= adp_current->ptd_keymax)
+	return -EINVAL;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (test_and_clear_bit(key,&adp_current->ptd_keymap))
+	adp_current->ptd_keycount--;
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return 0;
+}
+
+int adeos_set_ptd (int key, void *value)
+
+{
+    if (key < 0 || key >= adp_current->ptd_keymax)
+	return -EINVAL;
+
+    if (!adp_current->ptd_setfun)
+	{
+	printk(KERN_WARNING "ADEOS: no ptdset hook for %s\n",adp_current->name);
+	return -EINVAL;
+	}
+
+    adp_current->ptd_setfun(key,value);
+
+    return 0;
+}
+
+void *adeos_get_ptd (int key)
+
+{
+    if (key < 0 || key >= adp_current->ptd_keymax)
+	return NULL;
+
+    if (!adp_current->ptd_getfun)
+	{
+	printk(KERN_WARNING "ADEOS: no ptdget hook for %s\n",adp_current->name);
+	return NULL;
+	}
+
+    return adp_current->ptd_getfun(key);
+}
+
+int adeos_init_mutex (admutex_t *mutex)
+
+{
+    admutex_t initm = ADEOS_MUTEX_UNLOCKED;
+    *mutex = initm;
+    return 0;
+}
+
+int adeos_destroy_mutex (admutex_t *mutex)
+
+{
+    if (!adeos_spin_trylock(&mutex->lock) &&
+	adp_current != adp_root &&
+	mutex->owner != adp_current)
+	return -EBUSY;
+
+    return 0;
+}
+
+static inline void __adeos_sleepon_mutex (admutex_t *mutex, adomain_t *sleeper, int cpuid)
+
+{
+    adomain_t *owner = mutex->owner;
+
+    /* Make the current domain (== sleeper) wait for the mutex to be
+       released. Adeos' pipelined scheme guarantees that the new
+       sleeper _is_ more prioritary than any aslept domain since we
+       have stalled each sleeper's stage. Must be called with local hw
+       interrupts off. */
+
+    sleeper->m_link = mutex->sleepq;
+    mutex->sleepq = sleeper;
+    __adeos_switch_to(owner,cpuid);
+    mutex->owner = sleeper;
+    adeos_spin_unlock(&mutex->lock);
+}
+
+static inline void __adeos_signal_mutex (admutex_t *mutex, int cpuid)
+
+{
+    adomain_t *sleeper;
+
+    /* Wake up first most prioritary sleeper. Must be called with
+       local hw interrupts off. */
+
+    sleeper = mutex->sleepq;
+    mutex->sleepq = sleeper->m_link;
+    __adeos_switch_to(sleeper,cpuid);
+}
+
+unsigned long adeos_lock_mutex (admutex_t *mutex)
+
+{
+    unsigned long flags, hwflags;
+    adeos_declare_cpuid;
+    adomain_t *adp;
+
+    if (unlikely(!adp_pipelined))
+	{
+	adeos_hw_local_irq_save(hwflags);
+	flags = !adeos_hw_test_iflag(hwflags);
+	adeos_spin_lock(&mutex->lock);
+	return flags;
+	}
+
+    adp = adp_cpu_current[cpuid];
+    flags = test_and_set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+    /* Two cases to handle here on SMP systems, only one for UP:
+       1) in case of a conflicting access from a prioritary domain
+       running on the same cpu, make this domain sleep on the mutex,
+       and resume the current owner so it can release the lock asap.
+       2) in case of a conflicting access from any domain on a
+       different cpu than the current owner's, simply enter a spinning
+       loop. Note that testing mutex->owncpu is safe since it is only
+       changed by the current owner, and set to -1 when the mutex is
+       unlocked. */
+
+    adeos_hw_local_irq_save(hwflags);
+
+#ifdef CONFIG_SMP
+    while (!adeos_spin_trylock(&mutex->lock))
+	{
+	if (mutex->owncpu == cpuid)
+	    {
+	    __adeos_sleepon_mutex(mutex,adp,cpuid);
+	    adeos_reload_cpuid();
+	    }
+	}
+
+    mutex->owncpu = cpuid;
+#else  /* !CONFIG_SMP */
+    while (mutex->owner != NULL && mutex->owner != adp)
+	__adeos_sleepon_mutex(mutex,adp,cpuid);
+#endif /* CONFIG_SMP */
+
+    mutex->owner = adp;
+
+    adeos_hw_local_irq_restore(hwflags);
+
+    return flags;
+}
+
+void adeos_unlock_mutex (admutex_t *mutex, unsigned long flags)
+
+{
+    unsigned long hwflags;
+    adeos_declare_cpuid;
+    adomain_t *adp;
+
+    if (unlikely(!adp_pipelined))
+	{
+	adeos_spin_unlock(&mutex->lock);
+
+	if (flags)
+	    adeos_hw_cli();
+	else
+	    adeos_hw_sti();
+
+	return;
+	}
+
+#ifdef CONFIG_SMP
+    mutex->owncpu = -1;
+#endif /* CONFIG_SMP */
+
+    adeos_hw_local_irq_save(hwflags);
+
+    if (unlikely(mutex->sleepq != NULL))
+	{
+	__adeos_signal_mutex(mutex,cpuid); /* Wake up one sleeper. */
+#ifdef CONFIG_SMP
+	adeos_reload_cpuid();
+#endif  /* CONFIG_SMP */
+	}
+    else
+	{
+	mutex->owner = NULL;
+	adeos_spin_unlock(&mutex->lock);
+	}
+
+    adeos_hw_local_irq_restore(hwflags);
+
+    adp = adp_cpu_current[cpuid];
+
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    else
+	{
+	adeos_hw_sti();	/* Absolutely needed. */
+	
+	clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+	
+	if (adp->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+	}
+}
+
+void __adeos_takeover (void)
+
+{
+    __adeos_enable_pipeline();
+    printk("ADEOS: Pipelining started.\n");
+}
+
+#ifdef MODULE
+
+static int __init adeos_init_module (void)
+
+{
+    __adeos_takeover();
+    return 0;
+}
+
+static void __exit adeos_exit_module (void)
+
+{
+    __adeos_disable_pipeline();
+    printk("ADEOS: Pipelining stopped.\n");
+}
+
+module_init(adeos_init_module);
+module_exit(adeos_exit_module);
+
+#endif /* MODULE */
+
+EXPORT_SYMBOL(adeos_register_domain);
+EXPORT_SYMBOL(adeos_unregister_domain);
+EXPORT_SYMBOL(adeos_renice_domain);
+EXPORT_SYMBOL(adeos_virtualize_irq);
+EXPORT_SYMBOL(adeos_control_irq);
+EXPORT_SYMBOL(adeos_propagate_irq);
+EXPORT_SYMBOL(adeos_schedule_irq);
+EXPORT_SYMBOL(adeos_alloc_irq);
+EXPORT_SYMBOL(adeos_free_irq);
+EXPORT_SYMBOL(adeos_trigger_irq);
+EXPORT_SYMBOL(adeos_trigger_ipi);
+EXPORT_SYMBOL(adeos_stall_pipeline);
+EXPORT_SYMBOL(adeos_unstall_pipeline);
+EXPORT_SYMBOL(adeos_unstall_pipeline_from);
+EXPORT_SYMBOL(adeos_catch_event_from);
+EXPORT_SYMBOL(adeos_hook_dswitch);
+EXPORT_SYMBOL(adeos_init_attr);
+EXPORT_SYMBOL(adeos_get_sysinfo);
+EXPORT_SYMBOL(adeos_tune_timer);
+EXPORT_SYMBOL(adeos_alloc_ptdkey);
+EXPORT_SYMBOL(adeos_free_ptdkey);
+EXPORT_SYMBOL(adeos_set_ptd);
+EXPORT_SYMBOL(adeos_get_ptd);
+EXPORT_SYMBOL(adeos_set_irq_affinity);
+EXPORT_SYMBOL(adeos_init_mutex);
+EXPORT_SYMBOL(adeos_destroy_mutex);
+EXPORT_SYMBOL(adeos_lock_mutex);
+EXPORT_SYMBOL(adeos_unlock_mutex);
diff -uNrp linux-2.4.23/drivers/adeos/x86.c linux-2.4.23-fusion/drivers/adeos/x86.c
--- linux-2.4.23/drivers/adeos/x86.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/drivers/adeos/x86.c	2004-01-11 15:29:36.000000000 +0100
@@ -0,0 +1,1046 @@
+/*
+ *   linux/drivers/adeos/x86.c
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-dependent ADEOS support for x86.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <linux/irq.h>
+#include <linux/slab.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/hw_irq.h>
+#include <asm/irq.h>
+#include <asm/desc.h>
+#include <asm/io.h>
+#ifdef CONFIG_X86_LOCAL_APIC
+#include <asm/fixmap.h>
+#include <asm/bitops.h>
+#include <asm/mpspec.h>
+#ifdef CONFIG_X86_IO_APIC
+#include <asm/io_apic.h>
+#endif /* CONFIG_X86_IO_APIC */
+#include <asm/apic.h>
+#endif /* CONFIG_X86_LOCAL_APIC */
+
+extern struct desc_struct idt_table[];
+
+extern spinlock_t __adeos_pipelock;
+
+extern unsigned long __adeos_virtual_irq_map;
+
+extern struct list_head __adeos_pipeline;
+
+static void (*__adeos_std_vector_table[256])(void);
+
+static struct hw_interrupt_type __adeos_std_irq_dtype[NR_IRQS];
+
+/* Lifted from arch/i386/kernel/traps.c */ 
+#define __adeos_set_gate(gate_addr,type,dpl,addr)  \
+do { \
+  int __d0, __d1; \
+  __asm__ __volatile__ ("movw %%dx,%%ax\n\t" \
+	"movw %4,%%dx\n\t" \
+	"movl %%eax,%0\n\t" \
+	"movl %%edx,%1" \
+	:"=m" (*((long *) (gate_addr))), \
+	 "=m" (*(1+(long *) (gate_addr))), "=&a" (__d0), "=&d" (__d1) \
+	:"i" ((short) (0x8000+(dpl<<13)+(type<<8))), \
+	 "3" ((char *) (addr)),"2" (__KERNEL_CS << 16)); \
+} while (0)
+
+#define __adeos_get_gate_addr(v) \
+	((void *)((idt_table[v].b & 0xffff0000)|(idt_table[v].a & 0xffff)))
+
+#define __adeos_set_irq_gate(vector,addr) \
+__adeos_set_gate(idt_table+vector,14,0,addr)
+
+#define __adeos_set_trap_gate(vector,addr) \
+__adeos_set_gate(idt_table+vector,15,0,addr)
+
+#define __adeos_set_sys_gate(vector,addr) \
+__adeos_set_gate(idt_table+vector,15,3,addr)
+
+#define BUILD_IRQ_BODY() \
+__asm__( \
+	"\n" __ALIGN_STR"\n" \
+        SYMBOL_NAME_STR(__adeos_irq_common) ":\n\t" \
+	"pushl %es\n\t" \
+	"pushl %ds\n\t" \
+	"pushl %eax\n\t" \
+	"pushl %ebp\n\t" \
+	"pushl %edi\n\t" \
+	"pushl %esi\n\t" \
+	"pushl %edx\n\t" \
+	"pushl %ecx\n\t" \
+	"pushl %ebx\n\t" \
+	"movl $" STR(__KERNEL_DS) ",%edx\n\t" \
+	"movl %edx,%ds\n\t" \
+	"movl %edx,%es\n\t" \
+	"call " SYMBOL_NAME_STR(__adeos_handle_irq) "\n\t" \
+	"testl %eax,%eax\n\t" \
+	"jnz  1f\n\t" \
+        "popl %ebx\n\t" \
+	"popl %ecx\n\t" \
+	"popl %edx\n\t" \
+	"popl %esi\n\t" \
+	"popl %edi\n\t" \
+	"popl %ebp\n\t" \
+	"popl %eax\n\t" \
+	"popl %ds\n\t" \
+	"popl %es\n\t" \
+	"addl $4,%esp\n\t" \
+        "iret\n\t" \
+        "1: cld\n\t" \
+        __adeos_bump_count_noarg /* Depends on CONFIG_PREEMPT. */ \
+        "jmp " SYMBOL_NAME_STR(ret_from_intr) "\n");
+
+
+#define BUILD_IRQ_PROTO(irq)  __adeos_irq_##irq(void)
+#define BUILD_1_IRQ(irq) \
+asmlinkage void BUILD_IRQ_PROTO(irq); \
+__asm__( \
+	"\n" __ALIGN_STR"\n" \
+        SYMBOL_NAME_STR(__adeos_irq_) #irq ":\n\t" \
+	"pushl $"#irq"-256 \n\t" \
+	"jmp " SYMBOL_NAME_STR(__adeos_irq_common) "\n");
+
+#define BUILD_IRQ_N(x,y) \
+BUILD_1_IRQ(x##y)
+
+#define BUILD_16_IRQS(x) \
+BUILD_IRQ_N(x,0)  BUILD_IRQ_N(x,1)  BUILD_IRQ_N(x,2)  BUILD_IRQ_N(x,3) \
+BUILD_IRQ_N(x,4)  BUILD_IRQ_N(x,5)  BUILD_IRQ_N(x,6)  BUILD_IRQ_N(x,7) \
+BUILD_IRQ_N(x,8)  BUILD_IRQ_N(x,9)  BUILD_IRQ_N(x,a)  BUILD_IRQ_N(x,b) \
+BUILD_IRQ_N(x,c)  BUILD_IRQ_N(x,d)  BUILD_IRQ_N(x,e)  BUILD_IRQ_N(x,f)
+
+BUILD_IRQ_BODY();
+
+BUILD_16_IRQS(0x0)
+#if defined(CONFIG_X86_LOCAL_APIC) || defined(CONFIG_X86_IO_APIC)
+BUILD_16_IRQS(0x1)  BUILD_16_IRQS(0x2)  BUILD_16_IRQS(0x3) BUILD_16_IRQS(0x4)
+BUILD_16_IRQS(0x5)  BUILD_16_IRQS(0x6)  BUILD_16_IRQS(0x7) BUILD_16_IRQS(0x8)
+BUILD_16_IRQS(0x9)  BUILD_16_IRQS(0xa)  BUILD_16_IRQS(0xb) BUILD_16_IRQS(0xc)
+BUILD_16_IRQS(0xd)
+#endif /* CONFIG_X86_LOCAL_APIC || CONFIG_X86_IO_APIC */
+
+#define LIST_1_IRQ(irq) &__adeos_irq_ ## irq
+
+#define LIST_IRQ_N(x,y) \
+LIST_1_IRQ(x##y)
+
+#define LIST_16_IRQS(x) \
+LIST_IRQ_N(x,0),  LIST_IRQ_N(x,1),  LIST_IRQ_N(x,2), LIST_IRQ_N(x,3), \
+LIST_IRQ_N(x,4),  LIST_IRQ_N(x,5),  LIST_IRQ_N(x,6), LIST_IRQ_N(x,7), \
+LIST_IRQ_N(x,8),  LIST_IRQ_N(x,9),  LIST_IRQ_N(x,a), LIST_IRQ_N(x,b), \
+LIST_IRQ_N(x,c),  LIST_IRQ_N(x,d),  LIST_IRQ_N(x,e), LIST_IRQ_N(x,f)
+
+static void (*__adeos_irq_trampolines[])(void) = {
+    LIST_16_IRQS(0x0),
+#if defined(CONFIG_X86_LOCAL_APIC) || defined(CONFIG_X86_IO_APIC)
+    LIST_16_IRQS(0x1), LIST_16_IRQS(0x2), LIST_16_IRQS(0x3), LIST_16_IRQS(0x4),
+    LIST_16_IRQS(0x5), LIST_16_IRQS(0x6), LIST_16_IRQS(0x7), LIST_16_IRQS(0x8),
+    LIST_16_IRQS(0x9), LIST_16_IRQS(0xa), LIST_16_IRQS(0xb), LIST_16_IRQS(0xc),
+    LIST_16_IRQS(0xd)
+#endif /* CONFIG_X86_LOCAL_APIC || CONFIG_X86_IO_APIC */
+};
+
+#define BUILD_TRAP_PROTO(trapnr)  __adeos_trap_##trapnr(void)
+
+#define BUILD_TRAP_ERRCODE(trapnr) \
+asmlinkage void BUILD_TRAP_PROTO(trapnr); \
+__asm__ ( \
+	"\n" __ALIGN_STR"\n\t" \
+        SYMBOL_NAME_STR(__adeos_trap_) #trapnr ":\n\t" \
+        "cld\n\t" \
+        "pushl %es\n\t" \
+        "pushl %ds\n\t" \
+        "pushl %eax\n\t" \
+        "pushl %ebp\n\t" \
+        "pushl %edi\n\t" \
+        "pushl %esi\n\t" \
+        "pushl %edx\n\t" \
+        "pushl %ecx\n\t" \
+        "pushl %ebx\n\t" \
+        "movl $" STR(__KERNEL_DS) ",%edx\n\t" \
+	"mov %dx,%ds\n\t" \
+	"mov %dx,%es\n\t" \
+        "movl ("__stringify(__adeos_event_monitors + 4 * trapnr)"),%eax\n\t" \
+	"testl %eax,%eax\n\t" \
+	"jz 1f\n\t" \
+	"movl %esp,%eax\n\t" \
+        "pushl %eax\n\t" \
+	"pushl $"#trapnr"\n\t" \
+        "call " __stringify(__adeos_handle_event) "\n\t" \
+	"addl $8,%esp\n\t" \
+	"testl %eax,%eax\n\t" \
+"1:      popl %ebx\n\t" \
+        "popl %ecx\n\t" \
+	"popl %edx\n\t" \
+        "popl %esi\n\t" \
+        "popl %edi\n\t" \
+	"popl %ebp\n\t" \
+	"jz 2f\n\t" \
+	"popl %eax\n\t" \
+	"popl %ds\n\t" \
+	"popl %es\n\t" \
+	"addl $4,%esp\n\t" \
+	"iret\n" \
+"2:      movl ("SYMBOL_NAME_STR(__adeos_std_vector_table + 4 * trapnr)"),%eax\n\t" \
+	"movl 8(%esp),%es\n\t" \
+	"movl %eax,8(%esp)\n\t" \
+	"popl %eax\n\t" \
+	"popl %ds\n\t" \
+	"ret\n");
+
+#define BUILD_TRAP_NOERRCODE(trapnr) \
+asmlinkage void BUILD_TRAP_PROTO(trapnr); \
+__asm__ ( \
+	"\n" __ALIGN_STR"\n\t" \
+        SYMBOL_NAME_STR(__adeos_trap_) #trapnr ":\n\t" \
+        "cld\n\t" \
+        "pushl $0\n\t" \
+        "pushl %es\n\t" \
+        "pushl %ds\n\t" \
+        "pushl %eax\n\t" \
+        "pushl %ebp\n\t" \
+        "pushl %edi\n\t" \
+        "pushl %esi\n\t" \
+        "pushl %edx\n\t" \
+        "pushl %ecx\n\t" \
+        "pushl %ebx\n\t" \
+        "movl $" STR(__KERNEL_DS) ",%edx\n\t" \
+	"mov %dx,%ds\n\t" \
+	"mov %dx,%es\n\t" \
+        "movl (" __stringify(__adeos_event_monitors + 4 * trapnr)"),%eax\n\t" \
+	"testl %eax,%eax\n\t" \
+	"jz 1f\n\t" \
+	"movl %esp,%eax\n\t" \
+        "pushl %eax\n\t" \
+	"pushl $"#trapnr"\n\t" \
+        "call " __stringify(__adeos_handle_event) "\n\t" \
+	"addl $8,%esp\n\t" \
+	"testl %eax,%eax\n\t" \
+"1:      popl %ebx\n\t" \
+        "popl %ecx\n\t" \
+	"popl %edx\n\t" \
+        "popl %esi\n\t" \
+        "popl %edi\n\t" \
+	"popl %ebp\n\t" \
+	"jz 2f\n\t" \
+	"popl %eax\n\t" \
+	"popl %ds\n\t" \
+	"popl %es\n\t" \
+	"addl $4,%esp\n\t" \
+	"iret\n" \
+"2:      movl ("SYMBOL_NAME_STR(__adeos_std_vector_table + 4 * trapnr)"),%eax\n\t" \
+	"movl %eax,12(%esp)\n\t" \
+	"popl %eax\n\t" \
+	"popl %ds\n\t" \
+	"popl %es\n\t" \
+	"ret\n");
+
+BUILD_TRAP_NOERRCODE(0x0)   BUILD_TRAP_NOERRCODE(0x1)   BUILD_TRAP_NOERRCODE(0x2)
+BUILD_TRAP_NOERRCODE(0x3)   BUILD_TRAP_NOERRCODE(0x4)   BUILD_TRAP_NOERRCODE(0x5)
+BUILD_TRAP_NOERRCODE(0x6)   BUILD_TRAP_NOERRCODE(0x7)   BUILD_TRAP_ERRCODE(0x8)
+BUILD_TRAP_NOERRCODE(0x9)   BUILD_TRAP_ERRCODE(0xa)     BUILD_TRAP_ERRCODE(0xb)
+BUILD_TRAP_ERRCODE(0xc)     BUILD_TRAP_ERRCODE(0xd)     BUILD_TRAP_ERRCODE(0xe)
+BUILD_TRAP_NOERRCODE(0xf)   BUILD_TRAP_NOERRCODE(0x10)  BUILD_TRAP_ERRCODE(0x11)
+BUILD_TRAP_NOERRCODE(0x12)  BUILD_TRAP_NOERRCODE(0x13)  BUILD_TRAP_ERRCODE(0x14)
+BUILD_TRAP_ERRCODE(0x15)    BUILD_TRAP_ERRCODE(0x16)    BUILD_TRAP_ERRCODE(0x17)
+BUILD_TRAP_ERRCODE(0x18)    BUILD_TRAP_ERRCODE(0x19)    BUILD_TRAP_ERRCODE(0x1a)
+BUILD_TRAP_ERRCODE(0x1b)    BUILD_TRAP_ERRCODE(0x1c)    BUILD_TRAP_ERRCODE(0x1d)
+BUILD_TRAP_ERRCODE(0x1e)    BUILD_TRAP_ERRCODE(0x1f)
+
+#define LIST_TRAP(trapnr) &__adeos_trap_ ## trapnr
+
+static void (*__adeos_trap_trampolines[])(void) = {
+    LIST_TRAP(0x0), LIST_TRAP(0x1), LIST_TRAP(0x2), LIST_TRAP(0x3),
+    LIST_TRAP(0x4), LIST_TRAP(0x5), LIST_TRAP(0x6), LIST_TRAP(0x7),
+    LIST_TRAP(0x8), LIST_TRAP(0x9), LIST_TRAP(0xa), LIST_TRAP(0xb),
+    LIST_TRAP(0xc), LIST_TRAP(0xd), LIST_TRAP(0xe), LIST_TRAP(0xf),
+    LIST_TRAP(0x10), LIST_TRAP(0x11), LIST_TRAP(0x12), LIST_TRAP(0x13),
+    LIST_TRAP(0x14), LIST_TRAP(0x15), LIST_TRAP(0x16), LIST_TRAP(0x17),
+    LIST_TRAP(0x18), LIST_TRAP(0x19), LIST_TRAP(0x1a), LIST_TRAP(0x1b),
+    LIST_TRAP(0x1c), LIST_TRAP(0x1d), LIST_TRAP(0x1e), LIST_TRAP(0x1f)
+};
+
+static int __adeos_ack_common_irq (unsigned irq)
+
+{
+    irq_desc_t *desc = irq_desc + irq;
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+    desc->handler->ack(irq);
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+
+    return 1;
+}
+
+static unsigned __adeos_override_irq_startup (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+    unsigned s;
+
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+    __adeos_unlock_irq(adp_cpu_current[cpuid],irq);
+    s = __adeos_std_irq_dtype[irq].startup(irq);
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+
+    return s;
+}
+
+static void __adeos_override_irq_shutdown (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+    __adeos_std_irq_dtype[irq].shutdown(irq);
+    __adeos_clear_irq(adp_cpu_current[cpuid],irq);
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static void __adeos_override_irq_enable (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+    __adeos_unlock_irq(adp_cpu_current[cpuid],irq);
+    __adeos_std_irq_dtype[irq].enable(irq);
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static void __adeos_override_irq_disable (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+    __adeos_std_irq_dtype[irq].disable(irq);
+    __adeos_lock_irq(adp_cpu_current[cpuid],cpuid,irq);
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static void __adeos_override_irq_end (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+
+#ifdef CONFIG_X86_IO_APIC
+    if (!IO_APIC_IRQ(irq) || !(irq_desc[irq].status & (IRQ_DISABLED|IRQ_INPROGRESS)))
+#else /* !CONFIG_X86_IO_APIC */
+    if (!(irq_desc[irq].status & (IRQ_DISABLED|IRQ_INPROGRESS)))
+#endif /* CONFIG_X86_IO_APIC */
+	__adeos_unlock_irq(adp_cpu_current[cpuid],irq);
+
+    __adeos_std_irq_dtype[irq].end(irq);
+
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static void __adeos_override_irq_affinity (unsigned irq, unsigned long mask)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+    __adeos_std_irq_dtype[irq].set_affinity(irq,mask);
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+/* __adeos_enable_pipeline() -- Take over the interrupt control from
+   the root domain (i.e. Linux). After this routine has returned, all
+   interrupts go through the pipeline. */
+
+void __adeos_enable_pipeline (void)
+
+{
+    unsigned vector, irq;
+    unsigned long flags;
+
+    /* Collect the original vector table. */
+
+    for (vector = 0; vector < 256; vector++)
+	__adeos_std_vector_table[vector] = __adeos_get_gate_addr(vector);
+
+#ifdef CONFIG_SMP
+
+    /* This vector must be set up prior to call
+       adeos_critical_enter(). */
+
+    __adeos_set_irq_gate(ADEOS_CRITICAL_VECTOR,
+			 __adeos_irq_trampolines[ADEOS_CRITICAL_IPI]);
+
+#endif /* CONFIG_SMP */
+
+    flags = adeos_critical_enter(NULL);
+
+    /* First, grab the ISA and IO-APIC interrupts. */
+
+    for (irq = 0; irq < NR_IRQS && irq + FIRST_EXTERNAL_VECTOR < FIRST_SYSTEM_VECTOR; irq++)
+	{
+#ifdef CONFIG_X86_IO_APIC
+	if (IO_APIC_IRQ(irq))
+	    {
+	    vector = IO_APIC_VECTOR(irq);
+
+	    if (vector == 0)
+		continue;
+	    }
+	else
+#endif /* CONFIG_X86_IO_APIC */
+	    {
+	    vector = irq + FIRST_EXTERNAL_VECTOR;
+
+	    if (vector == SYSCALL_VECTOR)
+		continue;
+	    }
+
+	/* Fails for ADEOS_CRITICAL_IPI but that's ok. */
+
+	adeos_virtualize_irq(irq,
+			     (void (*)(unsigned))__adeos_std_vector_table[vector],
+			     &__adeos_ack_common_irq,
+			     IPIPE_CALLASM_MASK|IPIPE_HANDLE_MASK|IPIPE_PASS_MASK);
+
+	__adeos_set_irq_gate(vector,__adeos_irq_trampolines[irq]);
+	}
+
+    /* Interpose on the IRQ control routines so we can make them
+       atomic using hw masking and prevent the interrupt log from
+       being untimely flushed. Since we don't want to be too smart
+       about what's going on into irq.c and we want to change only
+       some of the controller members, let's be dumb and interpose the
+       rough way. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	__adeos_std_irq_dtype[irq] = *irq_desc[irq].handler;
+
+    /* The original controller structs are often shared, so we first
+       save them all before changing any of them. Notice that we don't
+       redirect the ack handler since the relevant XT-PIC/IO-APIC
+       management code is already Adeos-aware. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	{
+	irq_desc[irq].handler->startup = &__adeos_override_irq_startup;
+	irq_desc[irq].handler->shutdown = &__adeos_override_irq_shutdown;
+	irq_desc[irq].handler->enable = &__adeos_override_irq_enable;
+	irq_desc[irq].handler->disable = &__adeos_override_irq_disable;
+	irq_desc[irq].handler->end = &__adeos_override_irq_end;
+
+	if (irq_desc[irq].handler->set_affinity != NULL)
+	    irq_desc[irq].handler->set_affinity = &__adeos_override_irq_affinity;
+	}
+
+#ifdef CONFIG_X86_LOCAL_APIC
+
+    /* Map the APIC system vectors including the unused ones so that
+       client domains can virtualize the corresponding IRQs. */
+
+    for (vector = FIRST_SYSTEM_VECTOR; vector < CALL_FUNCTION_VECTOR; vector++)
+      {
+      adeos_virtualize_irq(vector - FIRST_EXTERNAL_VECTOR,
+			   (void (*)(unsigned))__adeos_std_vector_table[vector],
+			   &__adeos_ack_system_irq,
+			   IPIPE_CALLASM_MASK|IPIPE_HANDLE_MASK|IPIPE_PASS_MASK);
+      
+      __adeos_set_irq_gate(vector,
+			   __adeos_irq_trampolines[vector - FIRST_EXTERNAL_VECTOR]);
+      }
+
+    __adeos_tick_irq = using_apic_timer ? LOCAL_TIMER_VECTOR - FIRST_EXTERNAL_VECTOR : 0;
+
+#else  /* !CONFIG_X86_LOCAL_APIC */
+
+    __adeos_tick_irq = 0;
+
+#endif /* CONFIG_X86_LOCAL_APIC */
+
+#ifdef CONFIG_SMP
+
+    /* All interrupts must be pipelined, but the spurious one since we
+       don't even want to acknowledge it. */
+
+    for (vector = CALL_FUNCTION_VECTOR; vector < SPURIOUS_APIC_VECTOR; vector++)
+      {
+      adeos_virtualize_irq(vector - FIRST_EXTERNAL_VECTOR,
+			   (void (*)(unsigned))__adeos_std_vector_table[vector],
+			   &__adeos_ack_system_irq,
+			   IPIPE_CALLASM_MASK|IPIPE_HANDLE_MASK|IPIPE_PASS_MASK);
+      
+      __adeos_set_irq_gate(vector,
+			   __adeos_irq_trampolines[vector - FIRST_EXTERNAL_VECTOR]);
+      }
+
+    __adeos_set_irq_gate(ADEOS_SERVICE_VECTOR,
+			 __adeos_irq_trampolines[ADEOS_SERVICE_IPI]);
+
+#endif /* CONFIG_SMP */
+
+    /* Redirect traps and exceptions (except NMI). */
+
+    __adeos_set_trap_gate(0,__adeos_trap_trampolines[0]);
+    __adeos_set_trap_gate(1,__adeos_trap_trampolines[1]);
+    __adeos_set_sys_gate(3,__adeos_trap_trampolines[3]);
+    __adeos_set_sys_gate(4,__adeos_trap_trampolines[4]);
+    __adeos_set_sys_gate(5,__adeos_trap_trampolines[5]);
+    __adeos_set_trap_gate(6,__adeos_trap_trampolines[6]);
+    __adeos_set_trap_gate(7,__adeos_trap_trampolines[7]);
+    __adeos_set_trap_gate(8,__adeos_trap_trampolines[8]);
+    __adeos_set_trap_gate(9,__adeos_trap_trampolines[9]);
+    __adeos_set_trap_gate(10,__adeos_trap_trampolines[10]);
+    __adeos_set_trap_gate(11,__adeos_trap_trampolines[11]);
+    __adeos_set_trap_gate(12,__adeos_trap_trampolines[12]);
+    __adeos_set_trap_gate(13,__adeos_trap_trampolines[13]);
+    __adeos_set_irq_gate(14,__adeos_trap_trampolines[14]);
+    __adeos_set_trap_gate(15,__adeos_trap_trampolines[15]);
+    __adeos_set_trap_gate(16,__adeos_trap_trampolines[16]);
+    __adeos_set_trap_gate(17,__adeos_trap_trampolines[17]);
+    __adeos_set_trap_gate(18,__adeos_trap_trampolines[18]);
+    __adeos_set_trap_gate(19,__adeos_trap_trampolines[19]);
+
+#if defined(CONFIG_ADEOS_MODULE) || defined(CONFIG_X86_IO_APIC)
+    adp_pipelined = 1;
+#endif /* CONFIG_ADEOS_MODULE || CONFIG_X86_IO_APIC */
+
+    adeos_critical_exit(flags);
+}
+
+/* __adeos_disable_pipeline() -- Disengage the pipeline. */
+
+void __adeos_disable_pipeline (void)
+
+{
+    unsigned vector, irq;
+    unsigned long flags;
+
+    flags = adeos_critical_enter(NULL);
+
+    /* Restore interrupt controllers. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	*irq_desc[irq].handler = __adeos_std_irq_dtype[irq];
+
+    /* Restore original IDT settings. */
+
+    for (irq = 0; irq < NR_IRQS && irq + FIRST_EXTERNAL_VECTOR < FIRST_SYSTEM_VECTOR; irq++)
+	{
+#ifdef CONFIG_X86_IO_APIC
+	if (IO_APIC_IRQ(irq))
+	    {
+	    vector = IO_APIC_VECTOR(irq);
+
+	    if (vector == 0)
+		continue;
+	    }
+	else
+#endif /* CONFIG_X86_IO_APIC */
+	    {
+	    vector = irq + FIRST_EXTERNAL_VECTOR;
+
+	    if (vector == SYSCALL_VECTOR)
+		continue;
+	    }
+
+	__adeos_set_irq_gate(vector,__adeos_std_vector_table[vector]);
+	}
+
+#ifdef CONFIG_X86_LOCAL_APIC
+
+    for (vector = FIRST_SYSTEM_VECTOR; vector < CALL_FUNCTION_VECTOR; vector++)
+      __adeos_set_irq_gate(vector,__adeos_std_vector_table[vector]);
+
+#endif /* CONFIG_X86_LOCAL_APIC */
+
+#ifdef CONFIG_SMP
+
+    for (vector = CALL_FUNCTION_VECTOR; vector < SPURIOUS_APIC_VECTOR; vector++)
+	__adeos_set_irq_gate(vector,__adeos_std_vector_table[vector]);
+
+    __adeos_set_irq_gate(ADEOS_SERVICE_VECTOR,__adeos_std_vector_table[ADEOS_SERVICE_VECTOR]);
+    __adeos_set_irq_gate(ADEOS_CRITICAL_VECTOR,__adeos_std_vector_table[ADEOS_CRITICAL_VECTOR]);
+
+#endif /* CONFIG_SMP */
+
+    __adeos_set_trap_gate(0,__adeos_std_vector_table[0]);
+    __adeos_set_trap_gate(1,__adeos_std_vector_table[1]);
+    __adeos_set_sys_gate(3,__adeos_std_vector_table[3]);
+    __adeos_set_sys_gate(4,__adeos_std_vector_table[4]);
+    __adeos_set_sys_gate(5,__adeos_std_vector_table[5]);
+    __adeos_set_trap_gate(6,__adeos_std_vector_table[6]);
+    __adeos_set_trap_gate(7,__adeos_std_vector_table[7]);
+    __adeos_set_trap_gate(8,__adeos_std_vector_table[8]);
+    __adeos_set_trap_gate(9,__adeos_std_vector_table[9]);
+    __adeos_set_trap_gate(10,__adeos_std_vector_table[10]);
+    __adeos_set_trap_gate(11,__adeos_std_vector_table[11]);
+    __adeos_set_trap_gate(12,__adeos_std_vector_table[12]);
+    __adeos_set_trap_gate(13,__adeos_std_vector_table[13]);
+    __adeos_set_irq_gate(14,__adeos_std_vector_table[14]);
+    __adeos_set_trap_gate(15,__adeos_std_vector_table[15]);
+    __adeos_set_trap_gate(16,__adeos_std_vector_table[16]);
+    __adeos_set_trap_gate(17,__adeos_std_vector_table[17]);
+    __adeos_set_trap_gate(18,__adeos_std_vector_table[18]);
+    __adeos_set_trap_gate(19,__adeos_std_vector_table[19]);
+
+#if defined(CONFIG_ADEOS_MODULE) || defined(CONFIG_X86_IO_APIC)
+    adp_pipelined = 0;
+#endif /* CONFIG_ADEOS_MODULE || CONFIG_X86_IO_APIC */
+
+    adeos_critical_exit(flags);
+}
+
+/* adeos_virtualize_irq() -- Attach a handler (and optionally a hw
+   acknowledge routine) to an interrupt for the current domain. */
+
+int adeos_virtualize_irq (unsigned irq,
+			  void (*handler)(unsigned irq),
+			  int (*acknowledge)(unsigned irq),
+			  unsigned modemask)
+{
+    unsigned long flags;
+
+    if (irq >= IPIPE_NR_IRQS)
+	return -EINVAL;
+
+    if (adp_current->irqs[irq].control & IPIPE_SYSTEM_MASK)
+	return -EPERM;
+	
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (handler != NULL)
+	{
+	if ((modemask & IPIPE_EXCLUSIVE_MASK) != 0)
+	    {
+	    if (adp_current->irqs[irq].handler != NULL)
+		return -EBUSY;
+	    }
+
+	if ((modemask & IPIPE_STICKY_MASK) != 0)
+	    modemask |= IPIPE_HANDLE_MASK;
+	}
+    else
+	modemask &= ~(IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK);
+
+    if (acknowledge == NULL)
+	acknowledge = adp_root->irqs[irq].acknowledge;
+
+    adp_current->irqs[irq].handler = handler;
+    adp_current->irqs[irq].acknowledge = acknowledge;
+    adp_current->irqs[irq].control = modemask;
+
+    if (!adeos_virtual_irq_p(irq) && handler != NULL &&
+	(modemask & IPIPE_ENABLE_MASK) != 0 && irq < NR_IRQS)
+	irq_desc[irq].handler->enable(irq);
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return 0;
+}
+
+/* adeos_control_irq() -- Change an interrupt mode. This affects the
+   way a given interrupt is handled by ADEOS for the current
+   domain. setmask is a bitmask telling whether:
+   - the interrupt should be passed to the domain (IPIPE_HANDLE_MASK),
+     and/or
+   - the interrupt should be passed down to the lower priority domain(s)
+     in the pipeline (IPIPE_PASS_MASK).
+   This leads to four possibilities:
+   - PASS only => Ignore the interrupt
+   - HANDLE only => Terminate the interrupt (process but don't pass down)
+   - PASS + HANDLE => Accept the interrupt (process and pass down)
+   - <none> => Discard the interrupt
+   - DYNAMIC is currently an alias of HANDLE since it marks an interrupt
+   which is processed by the current domain but not implicitely passed
+   down to the pipeline, letting the domain's handler choose on a case-
+   by-case basis whether the interrupt propagation should be forced
+   using adeos_propagate_irq().
+   clrmask clears the corresponding bits from the control field before
+   setmask is applied.
+*/
+
+int adeos_control_irq (unsigned irq,
+		       unsigned clrmask,
+		       unsigned setmask)
+{
+    unsigned long flags;
+    irq_desc_t *desc;
+
+    if (irq >= IPIPE_NR_IRQS)
+	return -EINVAL;
+
+    if (adp_current->irqs[irq].control & IPIPE_SYSTEM_MASK)
+	return -EPERM;
+	
+    desc = irq_desc + irq;
+
+    if (adp_current->irqs[irq].handler == NULL)
+	setmask &= ~(IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK);
+
+    if ((setmask & IPIPE_STICKY_MASK) != 0)
+	setmask |= IPIPE_HANDLE_MASK;
+
+    if ((clrmask & (IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK)) != 0)	/* If one goes, both go. */
+	clrmask |= (IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK);
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    adp_current->irqs[irq].control &= ~clrmask;
+    adp_current->irqs[irq].control |= setmask;
+
+    if ((setmask & IPIPE_ENABLE_MASK) != 0)
+	desc->handler->enable(irq);
+    else if ((clrmask & IPIPE_ENABLE_MASK) != 0)
+	desc->handler->disable(irq);
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return 0;
+}
+
+static inline void __adeos_walk_pipeline (struct list_head *pos, int cpuid)
+
+{
+    while (pos != &__adeos_pipeline)
+	{
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	if (test_bit(IPIPE_STALL_FLAG,&_adp->cpudata[cpuid].status))
+	    break; /* Stalled stage -- do not go further. */
+
+	if (_adp->cpudata[cpuid].irq_pending_hi != 0)
+	    {
+	    /* Since the critical IPI might be serviced by the
+	       following actions, the current domain might not be
+	       linked to the pipeline anymore after its handler
+	       returns on SMP boxes, even if the domain remains valid
+	       (see adeos_unregister_domain()), so don't make any
+	       hazardous assumptions here. */
+
+	    if (_adp == adp_cpu_current[cpuid])
+		__adeos_sync_stage();
+	    else
+		{
+		__adeos_switch_to(_adp,cpuid);
+
+		adeos_reload_cpuid(); /* Processor might have changed. */
+
+		if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+		    adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+		    __adeos_sync_stage();
+		}
+
+	    break;
+	    }
+	else if (_adp == adp_cpu_current[cpuid])
+	    break;
+
+	pos = _adp->p_link.next;
+	}
+}
+
+/* __adeos_handle_irq() -- ADEOS' generic IRQ handler. An optimistic
+   interrupt protection log is maintained here for each
+   domain. Interrupts are off. */
+
+int __adeos_handle_irq (struct pt_regs regs)
+
+{
+    unsigned irq = regs.orig_eax & 0xff;
+    int acked = regs.orig_eax >= 0;
+    struct list_head *head, *pos;
+    adeos_declare_cpuid;
+
+    if (unlikely(test_bit(IPIPE_STICKY_FLAG,&adp_cpu_current[cpuid]->irqs[irq].control)))
+	head = &adp_cpu_current[cpuid]->p_link;
+    else
+	head = __adeos_pipeline.next;
+
+    /* Ack the interrupt. */
+
+    pos = head;
+
+    while (pos != &__adeos_pipeline)
+	{
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	/* For each domain handling the incoming IRQ, mark it as
+           pending in its log. */
+
+	if (test_bit(IPIPE_HANDLE_FLAG,&_adp->irqs[irq].control))
+	    {
+	    /* Domains that handle this IRQ are polled for
+	       acknowledging it by decreasing priority order. The
+	       interrupt must be made pending _first_ in the domain's
+	       status flags before the PIC is unlocked. */
+
+	    _adp->cpudata[cpuid].irq_hits[irq]++;
+	    __adeos_set_irq_bit(_adp,cpuid,irq);
+
+	    if (!acked)
+		acked = _adp->irqs[irq].acknowledge(irq);
+	    }
+
+	/* If the domain does not want the IRQ to be passed down the
+	   interrupt pipe, exit the loop now. */
+
+	if (!test_bit(IPIPE_PASS_FLAG,&_adp->irqs[irq].control))
+	    break;
+
+	pos = _adp->p_link.next;
+	}
+
+    if (likely(irq == __adeos_tick_irq))
+	{
+	__adeos_tick_regs.eflags = regs.eflags;
+	__adeos_tick_regs.eip = regs.eip;
+	__adeos_tick_regs.xcs = regs.xcs;
+	}
+
+    /* Now walk the pipeline, yielding control to the highest priority
+       domain that has pending interrupt(s) or immediately to the
+       current domain if the interrupt has been marked as
+       'sticky'. This search does not go beyond the current domain in
+       the pipeline. To understand this code properly, one must keep
+       in mind that domains having a higher priority than the current
+       one are sleeping on the adeos_suspend_domain() service. In
+       addition, domains having a lower priority have been preempted
+       by an interrupt dispatched to a more prioritary domain. Once
+       the first and most prioritary stage has been selected here, the
+       subsequent stages will be activated in turn when each visited
+       domain calls adeos_suspend_domain() to wake up its neighbour
+       down the pipeline. */
+
+    __adeos_walk_pipeline(head,cpuid);
+
+    adeos_reload_cpuid();
+    
+    return (adp_cpu_current[cpuid] == adp_root &&
+	    !test_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status));
+}
+
+void __adeos_init_domain (adomain_t *adp, adattr_t *attr)
+
+{
+    int estacksz = attr->estacksz > 0 ? attr->estacksz : 8192, _cpuid;
+    adeos_declare_cpuid;
+
+    for (_cpuid = 0; _cpuid < smp_num_cpus; _cpuid++)
+	{
+	int **psp = &adp->esp[_cpuid];
+
+	adp->estackbase[_cpuid] = (int *)kmalloc(estacksz,GFP_KERNEL);
+    
+	if (adp->estackbase[_cpuid] == NULL)
+	    panic("ADEOS: no memory for domain stack on CPU #%d",_cpuid);
+
+	adp->esp[_cpuid] = adp->estackbase[_cpuid];
+	**psp = 0;
+	*psp = (int *)(((unsigned long)*psp + estacksz - 0x10) & ~0xf);
+	*--(*psp) = (_cpuid == cpuid);
+	*--(*psp) = 0;
+	*--(*psp) = (int)attr->entry;
+	}
+}
+
+void __adeos_cleanup_domain (adomain_t *adp)
+
+{
+    int _cpuid;
+
+    for (_cpuid = 0; _cpuid < smp_num_cpus; _cpuid++)
+	{
+	adp->cpudata[_cpuid].irq_pending_hi = 0;
+	clear_bit(IPIPE_XPEND_FLAG,&adp->cpudata[_cpuid].status);
+	kfree(adp->estackbase[_cpuid]);
+	}
+}
+
+int adeos_get_sysinfo (adsysinfo_t *info)
+
+{
+#ifdef CONFIG_SMP
+    info->ncpus = smp_num_cpus;
+#else /* !CONFIG_SMP */
+    info->ncpus = 1;
+#endif /* CONFIG_SMP */
+
+    if (cpu_has_tsc)
+	info->cpufreq = 1000LL * cpu_khz;
+    else
+	info->cpufreq = CLOCK_TICK_RATE;
+
+    info->archdep.tmirq = __adeos_tick_irq;
+
+    return 0;
+}
+
+int adeos_tune_timer (unsigned long ns, int flags)
+
+{
+    unsigned ghz, latch;
+    unsigned long x;
+
+    if (flags & ADEOS_RESET_TIMER)
+	latch = LATCH;
+    else
+	{
+	if (ns < 122071 || ns > (1000 / HZ) * 1000000) /* HZ max, 8khz min */
+	    return -EINVAL;
+
+	ghz = 1000000000 / ns;
+	latch = (CLOCK_TICK_RATE + ghz/2) / ghz;
+	}
+
+    x = adeos_critical_enter(NULL); /* Sync with all CPUs */
+
+    /* Shamelessly lifted from init_IRQ() in i8259.c */
+    outb_p(0x34,0x43);		/* binary, mode 2, LSB/MSB, ch 0 */
+    outb_p(latch & 0xff,0x40);	/* LSB */
+    outb(latch >> 8,0x40);	/* MSB */
+
+    adeos_critical_exit(x);
+
+    return 0;
+}
+
+/* adeos_trigger_irq() -- Push the interrupt to the pipeline entry
+   just like if it has been actually received from a hw source. This
+   both works for real and virtual interrupts. This also means that
+   the current domain might be immediately preempted by a more
+   prioritary domain who happens to handle this interrupt. */
+
+int adeos_trigger_irq (unsigned irq)
+
+{
+    struct pt_regs regs;
+
+    if (irq >= IPIPE_NR_IRQS ||
+	(adeos_virtual_irq_p(irq) && !test_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map)))
+	return -EINVAL;
+
+    regs.orig_eax = irq; /* Won't be acked */
+    regs.xcs = __KERNEL_CS;
+    adeos_hw_local_irq_flags(regs.eflags);
+
+    __adeos_handle_irq(regs);
+
+    return 1;
+}
+
+/* adeos_trigger_ipi() -- Send the ADEOS service IPI to other
+   processors. */
+
+int adeos_trigger_ipi (int _cpuid)
+
+{
+#ifdef CONFIG_SMP
+    adeos_declare_cpuid;
+
+    if (unlikely(_cpuid == cpuid)) /* Self-posting the service interrupt? */
+	adeos_trigger_irq(ADEOS_SERVICE_IPI);
+    else
+	{
+	unsigned long flags;
+
+	adeos_hw_local_irq_save(flags);
+
+	if (cpuid == ADEOS_OTHER_CPUS)
+	    {
+	    if (smp_num_cpus > 1)
+		/* Send the service IPI to all processors but the current one. */
+		__adeos_send_IPI_allbutself(ADEOS_SERVICE_VECTOR);
+	    }
+	else if (_cpuid >= 0 && _cpuid < smp_num_cpus)
+	    __adeos_send_IPI_other(_cpuid,ADEOS_SERVICE_VECTOR);
+
+	adeos_hw_local_irq_restore(flags);
+	}
+
+    return cpuid != ADEOS_OTHER_CPUS ? 1 : smp_num_cpus - 1;
+#else  /* !CONFIG_SMP */
+    return 0;
+#endif /* CONFIG_SMP */
+}
diff -uNrp linux-2.4.23/drivers/block/ll_rw_blk.c linux-2.4.23-fusion/drivers/block/ll_rw_blk.c
--- linux-2.4.23/drivers/block/ll_rw_blk.c	2003-11-28 23:18:30.000000000 +0100
+++ linux-2.4.23-fusion/drivers/block/ll_rw_blk.c	2004-01-11 15:29:36.000000000 +0100
@@ -1309,6 +1309,7 @@ void submit_bh(int rw, struct buffer_hea
 			kstat.pgpgin += count;
 			break;
 	}
+	conditional_schedule();
 }
 
 /**
diff -uNrp linux-2.4.23/drivers/char/mem.c linux-2.4.23-fusion/drivers/char/mem.c
--- linux-2.4.23/drivers/char/mem.c	2003-11-28 23:18:35.000000000 +0100
+++ linux-2.4.23-fusion/drivers/char/mem.c	2004-01-11 15:29:36.000000000 +0100
@@ -401,7 +401,7 @@ static inline size_t read_zero_pagealign
 		if (count > size)
 			count = size;
 
-		zap_page_range(mm, addr, count);
+		zap_page_range(mm, addr, count, 0);
         	zeromap_page_range(addr, count, PAGE_COPY);
 
 		size -= count;
diff -uNrp linux-2.4.23/drivers/char/random.c linux-2.4.23-fusion/drivers/char/random.c
--- linux-2.4.23/drivers/char/random.c	2003-08-25 13:44:41.000000000 +0200
+++ linux-2.4.23-fusion/drivers/char/random.c	2004-01-11 15:29:36.000000000 +0100
@@ -1369,6 +1369,11 @@ static ssize_t extract_entropy(struct en
 		buf += i;
 		ret += i;
 		add_timer_randomness(&extract_timer_state, nbytes);
+#if LOWLATENCY_NEEDED
+		/* This can happen in softirq's, but that's what we want */
+		if (conditional_schedule_needed())
+			break;
+#endif
 	}
 
 	/* Wipe data just returned from memory */
diff -uNrp linux-2.4.23/drivers/i2c/i2c-algo-bit.c linux-2.4.23-fusion/drivers/i2c/i2c-algo-bit.c
--- linux-2.4.23/drivers/i2c/i2c-algo-bit.c	2003-08-25 13:44:41.000000000 +0200
+++ linux-2.4.23-fusion/drivers/i2c/i2c-algo-bit.c	2004-01-11 15:29:36.000000000 +0100
@@ -370,6 +370,7 @@ static int sendbytes(struct i2c_adapter 
 			return (retval<0)? retval : -EFAULT;
 			        /* got a better one ?? */
 		}
+		conditional_schedule();
 #if 0
 		/* from asm/delay.h */
 		__delay(adap->mdelay * (loops_per_sec / 1000) );
diff -uNrp linux-2.4.23/drivers/i2c/i2c-core.c linux-2.4.23-fusion/drivers/i2c/i2c-core.c
--- linux-2.4.23/drivers/i2c/i2c-core.c	2003-08-25 13:44:41.000000000 +0200
+++ linux-2.4.23-fusion/drivers/i2c/i2c-core.c	2004-01-11 15:29:36.000000000 +0100
@@ -763,6 +763,8 @@ int i2c_transfer(struct i2c_adapter * ad
 {
 	int ret;
 
+	conditional_schedule();
+
 	if (adap->algo->master_xfer) {
  	 	DEB2(printk("i2c-core.o: master_xfer: %s with %d msgs.\n",
 		            adap->name,num));
@@ -785,6 +787,8 @@ int i2c_master_send(struct i2c_client *c
 	struct i2c_adapter *adap=client->adapter;
 	struct i2c_msg msg;
 
+	conditional_schedule();
+
 	if (client->adapter->algo->master_xfer) {
 		msg.addr   = client->addr;
 		msg.flags = client->flags & I2C_M_TEN;
@@ -814,6 +818,9 @@ int i2c_master_recv(struct i2c_client *c
 	struct i2c_adapter *adap=client->adapter;
 	struct i2c_msg msg;
 	int ret;
+
+	conditional_schedule();
+
 	if (client->adapter->algo->master_xfer) {
 		msg.addr   = client->addr;
 		msg.flags = client->flags & I2C_M_TEN;
diff -uNrp linux-2.4.23/drivers/ieee1394/csr.c linux-2.4.23-fusion/drivers/ieee1394/csr.c
--- linux-2.4.23/drivers/ieee1394/csr.c	2003-11-28 23:18:37.000000000 +0100
+++ linux-2.4.23-fusion/drivers/ieee1394/csr.c	2004-01-11 15:29:36.000000000 +0100
@@ -18,6 +18,7 @@
  */
 
 #include <linux/string.h>
+#include <linux/sched.h>
 #include <linux/module.h> /* needed for MODULE_PARM */
 #include <linux/param.h>
 #include <linux/spinlock.h>
diff -uNrp linux-2.4.23/drivers/sound/sound_core.c linux-2.4.23-fusion/drivers/sound/sound_core.c
--- linux-2.4.23/drivers/sound/sound_core.c	2001-09-30 21:26:08.000000000 +0200
+++ linux-2.4.23-fusion/drivers/sound/sound_core.c	2004-01-11 15:29:36.000000000 +0100
@@ -37,6 +37,7 @@
 #include <linux/config.h>
 #include <linux/module.h>
 #include <linux/init.h>
+#include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/types.h>
 #include <linux/kernel.h>
diff -uNrp linux-2.4.23/drivers/video/fbcon-cfb16.c linux-2.4.23-fusion/drivers/video/fbcon-cfb16.c
--- linux-2.4.23/drivers/video/fbcon-cfb16.c	2001-10-15 22:47:13.000000000 +0200
+++ linux-2.4.23-fusion/drivers/video/fbcon-cfb16.c	2004-01-11 15:29:37.000000000 +0100
@@ -189,6 +189,7 @@ void fbcon_cfb16_putcs(struct vc_data *c
     case 4:
     case 8:
 	while (count--) {
+	    conditional_schedule();
 	    c = scr_readw(s++) & p->charmask;
 	    cdat = p->fontdata + c * fontheight(p);
 	    for (rows = fontheight(p), dest = dest0; rows--; dest += bytes) {
@@ -206,6 +207,7 @@ void fbcon_cfb16_putcs(struct vc_data *c
     case 12:
     case 16:
 	while (count--) {
+	    conditional_schedule();
 	    c = scr_readw(s++) & p->charmask;
 	    cdat = p->fontdata + (c * fontheight(p) << 1);
 	    for (rows = fontheight(p), dest = dest0; rows--; dest += bytes) {
diff -uNrp linux-2.4.23/fs/adfs/map.c linux-2.4.23-fusion/fs/adfs/map.c
--- linux-2.4.23/fs/adfs/map.c	2001-10-25 22:53:53.000000000 +0200
+++ linux-2.4.23-fusion/fs/adfs/map.c	2004-01-11 15:29:37.000000000 +0100
@@ -12,6 +12,7 @@
 #include <linux/fs.h>
 #include <linux/adfs_fs.h>
 #include <linux/spinlock.h>
+#include <linux/sched.h>
 
 #include "adfs.h"
 
diff -uNrp linux-2.4.23/fs/buffer.c linux-2.4.23-fusion/fs/buffer.c
--- linux-2.4.23/fs/buffer.c	2003-11-28 23:18:48.000000000 +0100
+++ linux-2.4.23-fusion/fs/buffer.c	2004-01-11 15:29:37.000000000 +0100
@@ -231,8 +231,10 @@ static int write_some_buffers(kdev_t dev
 
 		if (dev != NODEV && bh->b_dev != dev)
 			continue;
-		if (test_and_set_bit(BH_Lock, &bh->b_state))
+		if (test_and_set_bit(BH_Lock, &bh->b_state)) {
+			__refile_buffer(bh);
 			continue;
+		}
 		if (atomic_set_buffer_clean(bh)) {
 			__refile_buffer(bh);
 			get_bh(bh);
@@ -242,6 +244,7 @@ static int write_some_buffers(kdev_t dev
 
 			spin_unlock(&lru_list_lock);
 			write_locked_buffers(array, count);
+			conditional_schedule();
 			return -EAGAIN;
 		}
 		unlock_buffer(bh);
@@ -275,12 +278,19 @@ static int wait_for_buffers(kdev_t dev, 
 	struct buffer_head * next;
 	int nr;
 
-	next = lru_list[index];
 	nr = nr_buffers_type[index];
+repeat:
+	next = lru_list[index];
 	while (next && --nr >= 0) {
 		struct buffer_head *bh = next;
 		next = bh->b_next_free;
 
+		if (conditional_schedule_needed()) {
+			spin_unlock(&lru_list_lock);
+			unconditional_schedule();
+			spin_lock(&lru_list_lock);
+			goto repeat;
+		}
 		if (!buffer_locked(bh)) {
 			if (refile)
 				__refile_buffer(bh);
@@ -288,7 +298,6 @@ static int wait_for_buffers(kdev_t dev, 
 		}
 		if (dev != NODEV && bh->b_dev != dev)
 			continue;
-
 		get_bh(bh);
 		spin_unlock(&lru_list_lock);
 		wait_on_buffer (bh);
@@ -321,6 +330,15 @@ int sync_buffers(kdev_t dev, int wait)
 {
 	int err = 0;
 
+#if LOWLATENCY_NEEDED
+	/*
+	 * syncing devA when there are lots of buffers dirty against
+	 * devB is expensive.
+	 */
+	if (enable_lowlatency)
+		dev = NODEV;
+#endif
+
 	/* One pass for no-wait, three for wait:
 	 * 0) write out all dirty, unlocked buffers;
 	 * 1) wait for all dirty locked buffers;
@@ -687,6 +705,7 @@ void invalidate_bdev(struct block_device
 	int i, nlist, slept;
 	struct buffer_head * bh, * bh_next;
 	kdev_t dev = to_kdev_t(bdev->bd_dev);	/* will become bdev */
+	int lolat_retry = 0;
 
  retry:
 	slept = 0;
@@ -704,6 +723,17 @@ void invalidate_bdev(struct block_device
 			/* Not hashed? */
 			if (!bh->b_pprev)
 				continue;
+
+			if (lolat_retry < 10 && conditional_schedule_needed()) {
+				get_bh(bh);
+				spin_unlock(&lru_list_lock);
+				unconditional_schedule();
+				spin_lock(&lru_list_lock);
+				put_bh(bh);
+				slept = 1;
+				lolat_retry++;
+			}
+
 			if (buffer_locked(bh)) {
 				get_bh(bh);
 				spin_unlock(&lru_list_lock);
@@ -855,12 +885,18 @@ int fsync_buffers_list(struct list_head 
 	struct buffer_head *bh;
 	struct list_head tmp;
 	int err = 0, err2;
-	
+	DEFINE_RESCHED_COUNT;
+
 	INIT_LIST_HEAD(&tmp);
-	
+repeat:
 	spin_lock(&lru_list_lock);
 
 	while (!list_empty(list)) {
+		if (conditional_schedule_needed()) {
+			spin_unlock(&lru_list_lock);
+			unconditional_schedule();
+			goto repeat;
+		}
 		bh = BH_ENTRY(list->next);
 		list_del(&bh->b_inode_buffers);
 		if (!buffer_dirty(bh) && !buffer_locked(bh))
@@ -885,8 +921,18 @@ int fsync_buffers_list(struct list_head 
 				spin_lock(&lru_list_lock);
 			}
 		}
+		if (TEST_RESCHED_COUNT(32)) {
+			RESET_RESCHED_COUNT();
+			if (conditional_schedule_needed()) {
+				spin_unlock(&lru_list_lock);
+				unconditional_schedule();
+				spin_lock(&lru_list_lock);
+			}
+		}
 	}
 
+	RESET_RESCHED_COUNT();
+
 	while (!list_empty(&tmp)) {
 		bh = BH_ENTRY(tmp.prev);
 		remove_inode_queue(bh);
@@ -896,6 +942,7 @@ int fsync_buffers_list(struct list_head 
 		if (!buffer_uptodate(bh))
 			err = -EIO;
 		brelse(bh);
+		conditional_schedule();
 		spin_lock(&lru_list_lock);
 	}
 	
@@ -923,11 +970,20 @@ static int osync_buffers_list(struct lis
 	struct buffer_head *bh;
 	struct list_head *p;
 	int err = 0;
+	DEFINE_RESCHED_COUNT;
 
+repeat:
+	conditional_schedule();
 	spin_lock(&lru_list_lock);
 	
- repeat:
 	list_for_each_prev(p, list) {
+		if (TEST_RESCHED_COUNT(32)) {
+			RESET_RESCHED_COUNT();
+			if (conditional_schedule_needed()) {
+				spin_unlock(&lru_list_lock);
+				goto repeat;
+			}
+		}
 		bh = BH_ENTRY(p);
 		if (buffer_locked(bh)) {
 			get_bh(bh);
@@ -936,7 +992,6 @@ static int osync_buffers_list(struct lis
 			if (!buffer_uptodate(bh))
 				err = -EIO;
 			brelse(bh);
-			spin_lock(&lru_list_lock);
 			goto repeat;
 		}
 	}
@@ -953,12 +1008,24 @@ static int osync_buffers_list(struct lis
 void invalidate_inode_buffers(struct inode *inode)
 {
 	struct list_head * entry;
-	
+
+repeat:
+	conditional_schedule();
 	spin_lock(&lru_list_lock);
-	while ((entry = inode->i_dirty_buffers.next) != &inode->i_dirty_buffers)
+	while ((entry = inode->i_dirty_buffers.next) != &inode->i_dirty_buffers) {
+		if (conditional_schedule_needed()) {
+			spin_unlock(&lru_list_lock);
+			goto repeat;
+		}
 		remove_inode_queue(BH_ENTRY(entry));
-	while ((entry = inode->i_dirty_data_buffers.next) != &inode->i_dirty_data_buffers)
+	}
+	while ((entry = inode->i_dirty_data_buffers.next) != &inode->i_dirty_data_buffers) {
+		if (conditional_schedule_needed()) {
+			spin_unlock(&lru_list_lock);
+			goto repeat;
+		}
 		remove_inode_queue(BH_ENTRY(entry));
+	}
 	spin_unlock(&lru_list_lock);
 }
 
@@ -981,6 +1048,7 @@ struct buffer_head * getblk(kdev_t dev, 
 		bh = get_hash_table(dev, block, size);
 		if (bh) {
 			touch_buffer(bh);
+			conditional_schedule();
 			return bh;
 		}
 
diff -uNrp linux-2.4.23/fs/dcache.c linux-2.4.23-fusion/fs/dcache.c
--- linux-2.4.23/fs/dcache.c	2003-06-13 16:51:37.000000000 +0200
+++ linux-2.4.23-fusion/fs/dcache.c	2004-01-11 15:29:37.000000000 +0100
@@ -320,11 +320,23 @@ static inline void prune_one_dentry(stru
  
 void prune_dcache(int count)
 {
+	DEFINE_RESCHED_COUNT;
+
+redo:
 	spin_lock(&dcache_lock);
 	for (;;) {
 		struct dentry *dentry;
 		struct list_head *tmp;
 
+		if (TEST_RESCHED_COUNT(100)) {
+			RESET_RESCHED_COUNT();
+			if (conditional_schedule_needed()) {
+				spin_unlock(&dcache_lock);
+				unconditional_schedule();
+				goto redo;
+			}
+		}
+
 		tmp = dentry_unused.prev;
 
 		if (tmp == &dentry_unused)
@@ -479,6 +491,7 @@ static int select_parent(struct dentry *
 	struct dentry *this_parent = parent;
 	struct list_head *next;
 	int found = 0;
+	DEFINE_RESCHED_COUNT;
 
 	spin_lock(&dcache_lock);
 repeat:
@@ -493,6 +506,13 @@ resume:
 			list_add(&dentry->d_lru, dentry_unused.prev);
 			found++;
 		}
+
+		if (TEST_RESCHED_COUNT(500) && found > 10) {
+			if (conditional_schedule_needed())	/* Typically sys_rmdir() */
+				goto out;
+			RESET_RESCHED_COUNT();
+		}
+
 		/*
 		 * Descend a level if the d_subdirs list is non-empty.
 		 */
@@ -517,6 +537,7 @@ this_parent->d_parent->d_name.name, this
 #endif
 		goto resume;
 	}
+out:
 	spin_unlock(&dcache_lock);
 	return found;
 }
@@ -532,8 +553,10 @@ void shrink_dcache_parent(struct dentry 
 {
 	int found;
 
-	while ((found = select_parent(parent)) != 0)
+	while ((found = select_parent(parent)) != 0) {
 		prune_dcache(found);
+		conditional_schedule();		/* Typically sys_rmdir() */
+	}
 }
 
 /*
diff -uNrp linux-2.4.23/fs/exec.c linux-2.4.23-fusion/fs/exec.c
--- linux-2.4.23/fs/exec.c	2003-11-28 23:18:48.000000000 +0100
+++ linux-2.4.23-fusion/fs/exec.c	2004-01-11 15:29:37.000000000 +0100
@@ -245,7 +245,7 @@ int copy_strings(int argc,char ** argv, 
 					memset(kaddr+offset+len, 0,
 						PAGE_SIZE-offset-len);
 			}
-			err = copy_from_user(kaddr+offset, str, bytes_to_copy);
+			err = ll_copy_from_user(kaddr+offset, str, bytes_to_copy);
 			if (err) {
 				ret = -EFAULT;
 				goto out;
@@ -446,8 +446,8 @@ static int exec_mmap(void)
 		active_mm = current->active_mm;
 		current->mm = mm;
 		current->active_mm = mm;
-		task_unlock(current);
 		activate_mm(active_mm, mm);
+		task_unlock(current);
 		mm_release();
 		if (old_mm) {
 			if (active_mm != old_mm) BUG();
diff -uNrp linux-2.4.23/fs/ext2/dir.c linux-2.4.23-fusion/fs/ext2/dir.c
--- linux-2.4.23/fs/ext2/dir.c	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/fs/ext2/dir.c	2004-01-11 15:29:37.000000000 +0100
@@ -153,6 +153,7 @@ static struct page * ext2_get_page(struc
 	struct address_space *mapping = dir->i_mapping;
 	struct page *page = read_cache_page(mapping, n,
 				(filler_t*)mapping->a_ops->readpage, NULL);
+	conditional_schedule();		/* Scanning large directories */
 	if (!IS_ERR(page)) {
 		wait_on_page(page);
 		kmap(page);
diff -uNrp linux-2.4.23/fs/ext2/inode.c linux-2.4.23-fusion/fs/ext2/inode.c
--- linux-2.4.23/fs/ext2/inode.c	2003-06-13 16:51:37.000000000 +0200
+++ linux-2.4.23-fusion/fs/ext2/inode.c	2004-01-11 15:29:37.000000000 +0100
@@ -715,8 +715,13 @@ static inline void ext2_free_data(struct
 {
 	unsigned long block_to_free = 0, count = 0;
 	unsigned long nr;
+	DEFINE_RESCHED_COUNT;
 
 	for ( ; p < q ; p++) {
+		if (TEST_RESCHED_COUNT(32)) {
+			RESET_RESCHED_COUNT();
+			conditional_schedule();
+		}
 		nr = le32_to_cpu(*p);
 		if (nr) {
 			*p = 0;
@@ -759,6 +764,7 @@ static void ext2_free_branches(struct in
 	if (depth--) {
 		int addr_per_block = EXT2_ADDR_PER_BLOCK(inode->i_sb);
 		for ( ; p < q ; p++) {
+			conditional_schedule();		/* Deleting large files */
 			nr = le32_to_cpu(*p);
 			if (!nr)
 				continue;
diff -uNrp linux-2.4.23/fs/ext3/balloc.c linux-2.4.23-fusion/fs/ext3/balloc.c
--- linux-2.4.23/fs/ext3/balloc.c	2003-06-13 16:51:37.000000000 +0200
+++ linux-2.4.23-fusion/fs/ext3/balloc.c	2004-01-11 15:29:37.000000000 +0100
@@ -363,6 +363,9 @@ do_more:
 			}
 		}
 #endif
+		/* superblock lock is held, so this is safe */
+		conditional_schedule();
+
 		BUFFER_TRACE(bitmap_bh, "clear bit");
 		if (!ext3_clear_bit (bit + i, bitmap_bh->b_data)) {
 			ext3_error(sb, __FUNCTION__,
diff -uNrp linux-2.4.23/fs/ext3/inode.c linux-2.4.23-fusion/fs/ext3/inode.c
--- linux-2.4.23/fs/ext3/inode.c	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/fs/ext3/inode.c	2004-01-11 15:29:37.000000000 +0100
@@ -917,6 +917,8 @@ struct buffer_head *ext3_bread(handle_t 
 
 	prev_blocks = inode->i_blocks;
 
+	conditional_schedule();		/* Reading large directories */
+
 	bh = ext3_getblk (handle, inode, block, create, err);
 	if (!bh)
 		return bh;
@@ -1620,6 +1622,7 @@ ext3_clear_blocks(handle_t *handle, stru
 	 */
 	for (p = first; p < last; p++) {
 		u32 nr = le32_to_cpu(*p);
+		conditional_schedule();
 		if (nr) {
 			struct buffer_head *bh;
 
@@ -1674,6 +1677,7 @@ static void ext3_free_data(handle_t *han
 	}
 
 	for (p = first; p < last; p++) {
+		conditional_schedule();
 		nr = le32_to_cpu(*p);
 		if (nr) {
 			/* accumulate blocks to free if they're contiguous */
diff -uNrp linux-2.4.23/fs/ext3/namei.c linux-2.4.23-fusion/fs/ext3/namei.c
--- linux-2.4.23/fs/ext3/namei.c	2003-06-13 16:51:37.000000000 +0200
+++ linux-2.4.23-fusion/fs/ext3/namei.c	2004-01-11 15:29:37.000000000 +0100
@@ -157,6 +157,7 @@ restart:
 		if ((bh = bh_use[ra_ptr++]) == NULL)
 			goto next;
 		wait_on_buffer(bh);
+		conditional_schedule();
 		if (!buffer_uptodate(bh)) {
 			/* read error, skip block & hope for the best */
 			brelse(bh);
diff -uNrp linux-2.4.23/fs/fat/cache.c linux-2.4.23-fusion/fs/fat/cache.c
--- linux-2.4.23/fs/fat/cache.c	2001-10-12 22:48:42.000000000 +0200
+++ linux-2.4.23-fusion/fs/fat/cache.c	2004-01-11 15:29:37.000000000 +0100
@@ -14,6 +14,7 @@
 #include <linux/string.h>
 #include <linux/stat.h>
 #include <linux/fat_cvf.h>
+#include <linux/sched.h>
 
 #if 0
 #  define PRINTK(x) printk x
diff -uNrp linux-2.4.23/fs/inode.c linux-2.4.23-fusion/fs/inode.c
--- linux-2.4.23/fs/inode.c	2003-11-28 23:18:48.000000000 +0100
+++ linux-2.4.23-fusion/fs/inode.c	2004-01-11 15:29:37.000000000 +0100
@@ -305,6 +305,8 @@ static inline void __sync_one(struct ino
 
 	filemap_fdatawait(inode->i_mapping);
 
+	conditional_schedule();
+
 	spin_lock(&inode_lock);
 	inode->i_state &= ~I_LOCK;
 	if (!(inode->i_state & I_FREEING)) {
@@ -615,6 +617,7 @@ static void dispose_list(struct list_hea
 	while (!list_empty(head)) {
 		struct inode *inode;
 
+		conditional_schedule();
 		inode = list_entry(head->next, struct inode, i_list);
 		list_del(&inode->i_list);
 
@@ -651,9 +654,22 @@ static int invalidate_list(struct list_h
 		if (tmp == head)
 			break;
 		inode = list_entry(tmp, struct inode, i_list);
+
+		if (conditional_schedule_needed()) {
+			atomic_inc(&inode->i_count);
+			spin_unlock(&inode_lock);
+			unconditional_schedule();
+			spin_lock(&inode_lock);
+			atomic_dec(&inode->i_count);
+		}
+
 		if (inode->i_sb != sb)
 			continue;
+		atomic_inc(&inode->i_count);
+		spin_unlock(&inode_lock);
 		invalidate_inode_buffers(inode);
+		spin_lock(&inode_lock);
+		atomic_dec(&inode->i_count);
 		if (!atomic_read(&inode->i_count)) {
 			list_del_init(&inode->i_hash);
 			list_del(&inode->i_list);
@@ -759,15 +775,28 @@ void prune_icache(int goal)
 	struct list_head *entry, *freeable = &list;
 	int count;
 	struct inode * inode;
+	int nr_to_scan = inodes_stat.nr_unused;
 
+resume:
 	spin_lock(&inode_lock);
-
 	count = 0;
 	entry = inode_unused.prev;
-	while (entry != &inode_unused)
-	{
+	while (entry != &inode_unused && nr_to_scan--) {
 		struct list_head *tmp = entry;
 
+		if (conditional_schedule_needed()) {
+			/*
+			 * Need to drop the lock.  Reposition
+			 * the list head so we start here next time.
+			 * This can corrupt the LRU nature of the
+			 * unused list, but this isn't very important.
+			 */
+			list_del(&inode_unused);
+			list_add(&inode_unused, entry);
+			spin_unlock(&inode_lock);
+			unconditional_schedule();
+			goto resume;
+		}
 		entry = entry->prev;
 		inode = INODE(tmp);
 		if (inode->i_state & (I_FREEING|I_CLEAR|I_LOCK))
@@ -892,6 +921,8 @@ static struct inode * get_new_inode(stru
 	if (inode) {
 		struct inode * old;
 
+		conditional_schedule();			/* sync_old_buffers */
+
 		spin_lock(&inode_lock);
 		/* We released the lock, so.. */
 		old = find_inode(sb, ino, head, find_actor, opaque);
diff -uNrp linux-2.4.23/fs/jbd/checkpoint.c linux-2.4.23-fusion/fs/jbd/checkpoint.c
--- linux-2.4.23/fs/jbd/checkpoint.c	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/fs/jbd/checkpoint.c	2004-01-11 15:29:37.000000000 +0100
@@ -431,7 +431,11 @@ int __journal_clean_checkpoint_list(jour
 {
 	transaction_t *transaction, *last_transaction, *next_transaction;
 	int ret = 0;
+	int ll_retries = 4;		/* lowlatency addition */
 
+restart:
+	if (ll_retries-- == 0)
+		goto out;
 	transaction = journal->j_checkpoint_transactions;
 	if (transaction == 0)
 		goto out;
@@ -451,6 +455,12 @@ int __journal_clean_checkpoint_list(jour
 				jh = next_jh;
 				next_jh = jh->b_cpnext;
 				ret += __try_to_free_cp_buf(jh);
+				if (conditional_schedule_needed()) {
+					spin_unlock(&journal_datalist_lock);
+					unconditional_schedule();
+					spin_lock(&journal_datalist_lock);
+					goto restart;
+				}
 			} while (jh != last_jh);
 		}
 	} while (transaction != last_transaction);
diff -uNrp linux-2.4.23/fs/jbd/commit.c linux-2.4.23-fusion/fs/jbd/commit.c
--- linux-2.4.23/fs/jbd/commit.c	2003-06-13 16:51:37.000000000 +0200
+++ linux-2.4.23-fusion/fs/jbd/commit.c	2004-01-11 15:29:37.000000000 +0100
@@ -212,6 +212,16 @@ write_out_data_locked:
 				__journal_remove_journal_head(bh);
 				refile_buffer(bh);
 				__brelse(bh);
+				if (conditional_schedule_needed()) {
+					if (commit_transaction->t_sync_datalist)
+						commit_transaction->t_sync_datalist =
+							next_jh;
+					if (bufs)
+						break;
+					spin_unlock(&journal_datalist_lock);
+					unconditional_schedule();
+					goto write_out_data;
+				}
 			}
 		}
 		if (bufs == ARRAY_SIZE(wbuf)) {
@@ -235,8 +245,7 @@ write_out_data_locked:
 		journal_brelse_array(wbuf, bufs);
 		lock_journal(journal);
 		spin_lock(&journal_datalist_lock);
-		if (bufs)
-			goto write_out_data_locked;
+		goto write_out_data_locked;
 	}
 
 	/*
@@ -272,6 +281,15 @@ sync_datalist_empty:
 	 */
 	while ((jh = commit_transaction->t_async_datalist)) {
 		struct buffer_head *bh = jh2bh(jh);
+
+		if (conditional_schedule_needed()) {
+			spin_unlock(&journal_datalist_lock);
+			unlock_journal(journal);
+			unconditional_schedule();
+			lock_journal(journal);
+			spin_lock(&journal_datalist_lock);
+			continue;	/* List may have changed */
+		}
 		if (__buffer_state(bh, Freed)) {
 			BUFFER_TRACE(bh, "Cleaning freed buffer");
 			clear_bit(BH_Freed, &bh->b_state);
@@ -491,6 +509,8 @@ start_journal_io:
  wait_for_iobuf:
 	while (commit_transaction->t_iobuf_list != NULL) {
 		struct buffer_head *bh;
+
+		conditional_schedule();
 		jh = commit_transaction->t_iobuf_list->b_tprev;
 		bh = jh2bh(jh);
 		if (buffer_locked(bh)) {
@@ -649,6 +669,8 @@ skip_commit: /* The journal should be un
 		transaction_t *cp_transaction;
 		struct buffer_head *bh;
 
+		conditional_schedule();		/* journal is locked */
+
 		jh = commit_transaction->t_forget;
 		J_ASSERT_JH(jh,	jh->b_transaction == commit_transaction ||
 			jh->b_transaction == journal->j_running_transaction);
diff -uNrp linux-2.4.23/fs/nfsd/nfssvc.c linux-2.4.23-fusion/fs/nfsd/nfssvc.c
--- linux-2.4.23/fs/nfsd/nfssvc.c	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/fs/nfsd/nfssvc.c	2004-01-11 15:29:37.000000000 +0100
@@ -250,6 +250,7 @@ nfsd(struct svc_rqst *rqstp)
 	svc_exit_thread(rqstp);
 
 	/* Release module */
+	unlock_kernel();
 	MOD_DEC_USE_COUNT;
 }
 
diff -uNrp linux-2.4.23/fs/nls/nls_base.c linux-2.4.23-fusion/fs/nls/nls_base.c
--- linux-2.4.23/fs/nls/nls_base.c	2002-08-03 02:39:45.000000000 +0200
+++ linux-2.4.23-fusion/fs/nls/nls_base.c	2004-01-11 15:29:37.000000000 +0100
@@ -18,6 +18,7 @@
 #ifdef CONFIG_KMOD
 #include <linux/kmod.h>
 #endif
+#include <linux/sched.h>
 #include <linux/spinlock.h>
 
 static struct nls_table *tables;
diff -uNrp linux-2.4.23/fs/proc/array.c linux-2.4.23-fusion/fs/proc/array.c
--- linux-2.4.23/fs/proc/array.c	2003-11-28 23:18:49.000000000 +0100
+++ linux-2.4.23-fusion/fs/proc/array.c	2004-01-11 15:29:37.000000000 +0100
@@ -416,9 +416,11 @@ static inline void statm_pte_range(pmd_t
 	if (end > PMD_SIZE)
 		end = PMD_SIZE;
 	do {
-		pte_t page = *pte;
+		pte_t page;
 		struct page *ptpage;
 
+		conditional_schedule();		/* For `top' and `ps' */
+		page = *pte;
 		address += PAGE_SIZE;
 		pte++;
 		if (pte_none(page))
diff -uNrp linux-2.4.23/fs/proc/generic.c linux-2.4.23-fusion/fs/proc/generic.c
--- linux-2.4.23/fs/proc/generic.c	2003-11-28 23:18:49.000000000 +0100
+++ linux-2.4.23-fusion/fs/proc/generic.c	2004-01-11 15:29:37.000000000 +0100
@@ -98,6 +98,8 @@ proc_file_read(struct file * file, char 
 				retval = n;
 			break;
 		}
+
+		conditional_schedule();		/* Some /proc files are large */
 		
 		/* This is a hack to allow mangling of file pos independent
  		 * of actual bytes read.  Simply place the data at page,
diff -uNrp linux-2.4.23/fs/reiserfs/buffer2.c linux-2.4.23-fusion/fs/reiserfs/buffer2.c
--- linux-2.4.23/fs/reiserfs/buffer2.c	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/fs/reiserfs/buffer2.c	2004-01-11 15:29:37.000000000 +0100
@@ -54,6 +54,7 @@ struct buffer_head  * reiserfs_bread (st
     PROC_EXP( unsigned int ctx_switches = kstat.context_swtch );
 
     result = bread (super -> s_dev, n_block, n_size);
+    conditional_schedule();
     PROC_INFO_INC( super, breads );
     PROC_EXP( if( kstat.context_swtch != ctx_switches ) 
 	      PROC_INFO_INC( super, bread_miss ) );
diff -uNrp linux-2.4.23/fs/reiserfs/journal.c linux-2.4.23-fusion/fs/reiserfs/journal.c
--- linux-2.4.23/fs/reiserfs/journal.c	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/fs/reiserfs/journal.c	2004-01-11 15:29:37.000000000 +0100
@@ -574,6 +574,7 @@ inline void insert_journal_hash(struct r
 /* lock the current transaction */
 inline static void lock_journal(struct super_block *p_s_sb) {
   PROC_INFO_INC( p_s_sb, journal.lock_journal );
+  conditional_schedule();
   while(atomic_read(&(SB_JOURNAL(p_s_sb)->j_wlock)) > 0) {
     PROC_INFO_INC( p_s_sb, journal.lock_journal_wait );
     sleep_on(&(SB_JOURNAL(p_s_sb)->j_wait)) ;
@@ -704,6 +705,7 @@ reiserfs_panic(s, "journal-539: flush_co
 	mark_buffer_dirty(tbh) ;
       }
       ll_rw_block(WRITE, 1, &tbh) ;
+      conditional_schedule();
       count++ ;
       put_bh(tbh) ; /* once for our get_hash */
     } 
@@ -833,6 +835,7 @@ static int _update_journal_header_block(
     set_bit(BH_Dirty, &(SB_JOURNAL(p_s_sb)->j_header_bh->b_state)) ;
     ll_rw_block(WRITE, 1, &(SB_JOURNAL(p_s_sb)->j_header_bh)) ;
     wait_on_buffer((SB_JOURNAL(p_s_sb)->j_header_bh)) ; 
+    conditional_schedule();
     if (!buffer_uptodate(SB_JOURNAL(p_s_sb)->j_header_bh)) {
       reiserfs_warning( p_s_sb, "reiserfs: journal-837: IO error during journal replay\n" );
       return -EIO ;
@@ -2357,6 +2360,7 @@ static int journal_join(struct reiserfs_
 }
 
 int journal_begin(struct reiserfs_transaction_handle *th, struct super_block  * p_s_sb, unsigned long nblocks) {
+  conditional_schedule();
   return do_journal_begin_r(th, p_s_sb, nblocks, 0) ;
 }
 
@@ -2497,6 +2501,7 @@ int journal_mark_dirty_nolog(struct reis
 }
 
 int journal_end(struct reiserfs_transaction_handle *th, struct super_block *p_s_sb, unsigned long nblocks) {
+  conditional_schedule();
   return do_journal_end(th, p_s_sb, nblocks, 0) ;
 }
 
@@ -2968,6 +2973,7 @@ void reiserfs_prepare_for_journal(struct
       RFALSE( buffer_locked(bh) && cur_tb != NULL,
 	      "waiting while do_balance was running\n") ;
       wait_on_buffer(bh) ;
+      conditional_schedule();
     }
     PROC_INFO_INC( p_s_sb, journal.prepare_retry );
     retry_count++ ;
@@ -3142,6 +3148,7 @@ reiserfs_warning(p_s_sb, "journal-2020: 
     /* copy all the real blocks into log area.  dirty log blocks */
     if (test_bit(BH_JDirty, &cn->bh->b_state)) {
       struct buffer_head *tmp_bh ;
+      conditional_schedule();
       tmp_bh =  journal_getblk(p_s_sb, SB_ONDISK_JOURNAL_1st_BLOCK(p_s_sb) + 
 		       ((cur_write_start + jindex) % SB_ONDISK_JOURNAL_SIZE(p_s_sb))) ;
       mark_buffer_uptodate(tmp_bh, 1) ;
diff -uNrp linux-2.4.23/fs/reiserfs/stree.c linux-2.4.23-fusion/fs/reiserfs/stree.c
--- linux-2.4.23/fs/reiserfs/stree.c	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/fs/reiserfs/stree.c	2004-01-11 15:29:37.000000000 +0100
@@ -652,9 +652,8 @@ int search_by_key (struct super_block * 
                                        stop at leaf level - set to
                                        DISK_LEAF_NODE_LEVEL */
     ) {
-    int  n_block_number = SB_ROOT_BLOCK (p_s_sb),
-      expected_level = SB_TREE_HEIGHT (p_s_sb),
-      n_block_size    = p_s_sb->s_blocksize;
+    int n_block_number, expected_level;
+    int n_block_size    = p_s_sb->s_blocksize;
     struct buffer_head  *       p_s_bh;
     struct path_element *       p_s_last_element;
     int				n_node_level, n_retval;
@@ -666,7 +665,8 @@ int search_by_key (struct super_block * 
 #endif
     
     PROC_INFO_INC( p_s_sb, search_by_key );
-    
+    conditional_schedule();
+
     /* As we add each node to a path we increase its count.  This means that
        we must be careful to release all nodes in a path before we either
        discard the path struct or re-use the path struct, as we do here. */
@@ -678,6 +678,8 @@ int search_by_key (struct super_block * 
     /* With each iteration of this loop we search through the items in the
        current node, and calculate the next current node(next path element)
        for the next iteration of this loop.. */
+    n_block_number = SB_ROOT_BLOCK (p_s_sb);
+    expected_level = SB_TREE_HEIGHT (p_s_sb);
     while ( 1 ) {
 
 #ifdef CONFIG_REISERFS_CHECK
@@ -1104,6 +1106,8 @@ static char  prepare_for_delete_or_cut(
 	    for (n_counter = *p_n_removed;
 		 n_counter < n_unfm_number; n_counter++, p_n_unfm_pointer-- ) {
 
+		conditional_schedule();
+
 		if (item_moved (&s_ih, p_s_path)) {
 		    need_research = 1 ;
 		    break;
diff -uNrp linux-2.4.23/include/asm-i386/adeos.h linux-2.4.23-fusion/include/asm-i386/adeos.h
--- linux-2.4.23/include/asm-i386/adeos.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-i386/adeos.h	2004-01-11 21:44:07.000000000 +0100
@@ -0,0 +1,425 @@
+/*
+ *   include/asm-i386/adeos.h
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef __I386_ADEOS_H
+#define __I386_ADEOS_H
+
+struct task_struct;
+
+#include <asm/irq.h>
+#include <asm/siginfo.h>
+#include <asm/ptrace.h>
+#include <asm/bitops.h>
+#include <linux/list.h>
+#include <linux/threads.h>
+#include <linux/spinlock.h>
+
+#define ADEOS_ARCH_STRING "x86"
+
+#if defined(CONFIG_ADEOS_MODULE) || defined(CONFIG_X86_IO_APIC)
+extern int adp_pipelined;
+#else  /* !CONFIG_ADEOS_MODULE && !CONFIG_X86_IO_APIC */
+#define adp_pipelined 1		/* Testing this should be optimized out. */
+#endif /* CONFIG_ADEOS_MODULE || CONFIG_X86_IO_APIC */
+
+#ifdef CONFIG_SMP
+
+#include <asm/fixmap.h>
+#include <asm/apicdef.h>
+
+extern int smp_num_cpus;
+
+#define ADEOS_NR_CPUS          NR_CPUS
+#define ADEOS_CRITICAL_VECTOR  0xf1 /* Used by adeos_critical_enter/exit() */
+#define ADEOS_CRITICAL_IPI     (ADEOS_CRITICAL_VECTOR - FIRST_EXTERNAL_VECTOR)
+#define ADEOS_SERVICE_VECTOR   0xf9
+#define ADEOS_SERVICE_IPI      (ADEOS_SERVICE_VECTOR - FIRST_EXTERNAL_VECTOR)
+
+static __inline int adeos_smp_apic_id(void) {
+    return GET_APIC_ID(*(unsigned long *)(APIC_BASE+APIC_ID));
+}
+
+/* Cannot include asm/smpboot.h safely, so we define here the little
+   we need to know about CPU/apicid mappings. */
+
+#ifdef CONFIG_MULTIQUAD
+extern volatile int logical_apicid_2_cpu[];
+#define adeos_processor_id()   (logical_apicid_2_cpu[adeos_smp_apic_id()])
+#else /* !CONFIG_MULTIQUAD */		/* use physical IDs to bootstrap */
+extern volatile int physical_apicid_2_cpu[];
+#define adeos_processor_id()   (physical_apicid_2_cpu[adeos_smp_apic_id()])
+#endif /* CONFIG_MULTIQUAD */
+
+#define adeos_declare_cpuid    int cpuid = adeos_processor_id()
+#define adeos_reload_cpuid()   do { cpuid = adeos_processor_id(); } while(0)
+
+#else  /* !CONFIG_SMP */
+
+#define ADEOS_NR_CPUS          1
+#define adeos_processor_id()   0
+/* Array references using this index should be optimized out. */
+#define adeos_declare_cpuid  const int cpuid = 0
+#define adeos_reload_cpuid()  /* nop */
+
+#endif /* CONFIG_SMP */
+
+ /* IDT fault vectors */
+#define ADEOS_NR_FAULTS         32
+/* Pseudo-vectors used for kernel events */
+#define ADEOS_FIRST_KEVENT      ADEOS_NR_FAULTS
+#define ADEOS_SYSCALL_PROLOGUE  (ADEOS_FIRST_KEVENT)
+#define ADEOS_SYSCALL_EPILOGUE  (ADEOS_FIRST_KEVENT + 1)
+#define ADEOS_SCHEDULE_HEAD     (ADEOS_FIRST_KEVENT + 2)
+#define ADEOS_SCHEDULE_TAIL     (ADEOS_FIRST_KEVENT + 3)
+#define ADEOS_ENTER_PROCESS     (ADEOS_FIRST_KEVENT + 4)
+#define ADEOS_EXIT_PROCESS      (ADEOS_FIRST_KEVENT + 5)
+#define ADEOS_SIGNAL_PROCESS    (ADEOS_FIRST_KEVENT + 6)
+#define ADEOS_RENICE_PROCESS    (ADEOS_FIRST_KEVENT + 7)
+#define ADEOS_USER_EVENT        (ADEOS_FIRST_KEVENT + 8)
+#define ADEOS_LAST_KEVENT       (ADEOS_USER_EVENT)
+
+#define ADEOS_NR_EVENTS         (ADEOS_LAST_KEVENT + 1)
+
+typedef struct adevinfo {
+
+    unsigned domid;
+    unsigned event;
+    void *evdata;
+
+    volatile int propagate;	/* Private */
+
+} adevinfo_t;
+
+typedef struct adsysinfo {
+
+    int ncpus;			/* Number of CPUs on board */
+
+    unsigned long long cpufreq;	/* CPU frequency (in Hz) */
+
+    /* Arch-dependent block */
+
+    struct {
+	unsigned tmirq;		/* Timer tick IRQ */
+    } archdep;
+
+} adsysinfo_t;
+
+#ifdef CONFIG_X86_LOCAL_APIC
+/* We must cover the whole IRQ space (224 external vectors for x86) to
+   map the local timer interrupt (#207). */
+#define IPIPE_NR_XIRQS   224
+#else /* !CONFIG_X86_LOCAL_APIC */
+#define IPIPE_NR_XIRQS   NR_IRQS
+#endif /* CONFIG_X86_LOCAL_APIC */
+/* Number of virtual IRQs */
+#define IPIPE_NR_VIRQS   32
+/* First virtual IRQ # */
+#define IPIPE_VIRQ_BASE   IPIPE_NR_XIRQS
+/* Total number of IRQs (external + virtual) */
+#define IPIPE_NR_IRQS     (IPIPE_NR_XIRQS + IPIPE_NR_VIRQS)
+/* Number of indirect words needed to map the whole IRQ space. */
+#define IPIPE_IRQ_IWORDS  ((IPIPE_NR_IRQS + 31) / 32)
+
+typedef struct adomain {
+
+    /* -- Section: offset-based references are made on these fields
+       from inline assembly code. Please don't move or reorder. */
+    void (*dswitch)(void);	/* Domain switch hook */
+    int *esp[ADEOS_NR_CPUS];	/* Domain stack pointers */
+    /* -- End of section. */
+
+    int *estackbase[ADEOS_NR_CPUS];
+
+    unsigned domid;
+
+    const char *name;
+
+    int priority;
+
+    struct adcpudata {
+	volatile unsigned long status;
+	volatile unsigned long irq_pending_hi;
+	volatile unsigned long irq_pending_lo[IPIPE_IRQ_IWORDS];
+	volatile unsigned irq_hits[IPIPE_NR_IRQS];
+	adevinfo_t event_info;
+    } cpudata[ADEOS_NR_CPUS];
+
+    struct {
+	int (*acknowledge)(unsigned irq);
+	void (*handler)(unsigned irq);
+	unsigned long control;
+    } irqs[IPIPE_NR_IRQS];
+
+    struct {
+	void (*handler)(adevinfo_t *evinfo);
+    } events[ADEOS_NR_EVENTS];
+
+    int ptd_keymax;
+    int ptd_keycount;
+    unsigned long ptd_keymap;
+    void (*ptd_setfun)(int, void *);
+    void *(*ptd_getfun)(int);
+
+    struct adomain *m_link;	/* Link in mutex sleep queue */
+
+    struct list_head p_link;	/* Link in pipeline */
+
+} adomain_t;
+
+typedef struct admutex {
+
+    spinlock_t lock;
+
+    adomain_t *sleepq, /* Pending domains queue */
+	      *owner;	/* Domain owning the mutex */
+#ifdef CONFIG_SMP
+    volatile int owncpu;
+#define ADEOS_MUTEX_UNLOCKED { SPIN_LOCK_UNLOCKED, NULL, NULL, -1 }
+#else  /* !CONFIG_SMP */
+#define ADEOS_MUTEX_UNLOCKED { SPIN_LOCK_UNLOCKED, NULL, NULL }
+#endif /* CONFIG_SMP */
+
+} admutex_t;
+
+#define __clear_bit(nr,addr) clear_bit(nr,addr)
+#define __test_bit(nr,addr)  test_bit(nr,addr)
+
+/* The following macros must be used hw interrupts off. */
+
+#define __adeos_set_irq_bit(adp,cpuid,irq) \
+do { \
+    if (!test_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control)) { \
+        __set_bit(irq & 0x1f,&(adp)->cpudata[cpuid].irq_pending_lo[irq >> 5]); \
+        __set_bit(irq >> 5,&(adp)->cpudata[cpuid].irq_pending_hi); \
+       } \
+} while(0)
+
+#define __adeos_clear_pend(adp,cpuid,irq) \
+do { \
+    __clear_bit(irq & 0x1f,&(adp)->cpudata[cpuid].irq_pending_lo[irq >> 5]); \
+    if (&(adp)->cpudata[cpuid].irq_pending_lo[irq >> 5] == 0) \
+        __clear_bit(irq >> 5,&(adp)->cpudata[cpuid].irq_pending_hi); \
+} while(0)
+
+#define __adeos_lock_irq(adp,cpuid,irq) \
+do { \
+    if (!test_and_set_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control)) \
+	__adeos_clear_pend(adp,cpuid,irq); \
+} while(0)
+
+#define __adeos_unlock_irq(adp,irq) \
+do { \
+    int __cpuid; \
+    if (test_and_clear_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control)) \
+      for (__cpuid = 0; __cpuid < smp_num_cpus; __cpuid++) \
+         if ((adp)->cpudata[__cpuid].irq_hits[irq] > 0) { \
+           set_bit(irq & 0x1f,&(adp)->cpudata[__cpuid].irq_pending_lo[irq >> 5]); \
+           set_bit(irq >> 5,&(adp)->cpudata[__cpuid].irq_pending_hi); \
+         } \
+} while(0)
+
+#define __adeos_clear_irq(adp,irq) \
+do { \
+    int __cpuid; \
+    clear_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control); \
+    for (__cpuid = 0; __cpuid < smp_num_cpus; __cpuid++) { \
+       (adp)->cpudata[__cpuid].irq_hits[irq] = 0; \
+       __adeos_clear_pend(adp,__cpuid,irq); \
+    } \
+} while(0)
+
+#define adeos_virtual_irq_p(irq) ((irq) >= IPIPE_VIRQ_BASE && \
+				  (irq) < IPIPE_NR_IRQS)
+
+#define adeos_hw_save_flags_and_sti(x)	__asm__ __volatile__("pushfl ; popl %0 ; sti":"=g" (x): /* no input */ :"memory")
+#define adeos_hw_cli() 			__asm__ __volatile__("cli": : :"memory")
+#define adeos_hw_sti()			__asm__ __volatile__("sti": : :"memory")
+#define adeos_hw_local_irq_save(x)    __asm__ __volatile__("pushfl ; popl %0 ; cli":"=g" (x): /* no input */ :"memory")
+#define adeos_hw_local_irq_restore(x) __asm__ __volatile__("pushl %0 ; popfl": /* no output */ :"g" (x):"memory", "cc")
+#define adeos_hw_local_irq_flags(x)   __asm__ __volatile__("pushfl ; popl %0":"=g" (x): /* no input */)
+#define adeos_hw_test_iflag(x)        ((x) & (1<<9))
+#define adeos_hw_irqs_disabled()	\
+({					\
+	unsigned long flags;		\
+	adeos_hw_local_irq_flags(flags);	\
+	!adeos_hw_test_iflag(flags);	\
+})
+
+#ifdef CONFIG_PREEMPT
+#define adeos_spin_lock(x)    _raw_spin_lock(x)
+#define adeos_spin_unlock(x)  _raw_spin_unlock(x)
+#define adeos_spin_trylock(x) _raw_spin_trylock(x)
+/* Ok, the following is stupid, but I want to keep all direct use of
+   preempt_count's offset here. */
+#define __adeos_bump_count   "movl %%esp, %%ebx\n\t" \
+	                     "andl $-8192, %%ebx\n\t" \
+			     "incl 4(%%ebx)\n\t"
+#define __adeos_bump_count_noarg  "movl %esp, %ebx\n\t" \
+	                          "andl $-8192, %ebx\n\t" \
+			          "incl 4(%ebx)\n\t"
+#else /* !CONFIG_PREEMPT */
+#define adeos_spin_lock(x)    spin_lock(x)
+#define adeos_spin_unlock(x)  spin_unlock(x)
+#define adeos_spin_trylock(x) spin_trylock(x)
+#define __adeos_bump_count
+#define __adeos_bump_count_noarg
+#endif /* CONFIG_PREEMPT */
+
+#define adeos_spin_lock_irqsave(x,flags)      do { adeos_hw_local_irq_save(flags); adeos_spin_lock(x); } while (0)
+#define adeos_spin_unlock_irqrestore(x,flags) do { adeos_spin_unlock(x); adeos_hw_local_irq_restore(flags); } while (0)
+#define adeos_spin_lock_disable(x)            do { adeos_hw_cli(); adeos_spin_lock(x); } while (0)
+#define adeos_spin_unlock_enable(x)           do { adeos_spin_unlock(x); adeos_hw_sti(); } while (0)
+
+/* Private interface -- Internal use only */
+
+struct adattr;
+
+void __adeos_init(void);
+
+void __adeos_init_domain(adomain_t *adp,
+			 struct adattr *attr);
+
+void __adeos_cleanup_domain(adomain_t *adp);
+
+void __adeos_send_IPI_allbutself(int vector);
+
+void __adeos_send_IPI_other(int cpu,
+			    int vector);
+
+#ifdef CONFIG_SMP
+
+/*
+ * __adeos_switch_to() -- Switch domain contexts. The current domain
+ * is switched out while the domain pointed by "adp" is switched
+ * in. The current cpu identifier which is always known from callers
+ * is also passed to save a few cycles.
+ * This code works out the following tasks:
+ * - build a resume frame for the suspended domain,
+ * - save it's stack pointer,
+ * - load the incoming domain's stack pointer,
+ * - update the global domain descriptor pointer,
+ * - then finally activate the resume frame of the incoming domain.
+ *
+ * SMP version also provides for safe CPU migration (i.e. the domain
+ * may be switched back in on behalf of a different CPU than the one
+ * which switched it out).
+ */
+
+static inline void __adeos_switch_to (adomain_t *adp, int cpuid) {
+
+    __asm__ __volatile__( \
+	"pushl %%ecx\n\t" \
+	"pushl %%ebx\n\t" \
+	"pushl %%ebp\n\t" \
+	"pushl %%edi\n\t" \
+	"pushl %%esi\n\t" \
+	"pushfl\n\t" \
+	"cli\n\t" \
+	"movl %%eax, %%ecx\n\t" \
+	"leal " __stringify(adp_cpu_current) "(,%%eax,4),%%eax\n\t" \
+	"xchg (%%eax), %%edx\n\t" \
+	"pushl %%edx\n\t" \
+	"pushl $1f\n\t" \
+	"sall $2,%%ecx\n\t" \
+	"addl $4,%%ecx\n\t" \
+	"movl %%esp, (%%ecx,%%edx)\n\t" \
+	"movl (%%eax), %%eax\n\t" \
+	"movl (%%ecx,%%eax), %%esp\n\t" \
+	"ret\n\t" \
+	  /* Call domain switch hook (if any) */
+"1:	 popl %%eax\n\t" \
+	"movl (%%eax), %%eax\n\t" \
+	"popfl\n\t" \
+	"testl %%eax,%%eax\n\t" \
+	"je 2f\n\t" \
+	"call *%%eax\n\t" \
+	  /* Domain resume point */
+"2:	 popl %%esi\n\t" \
+	"popl %%edi\n\t" \
+	"popl %%ebp\n\t" \
+	"popl %%ebx\n\t" \
+	"popl %%ecx\n\t" \
+	: /* no output */ \
+	: "a" (cpuid), "d" (adp));
+}
+
+unsigned long __adeos_set_irq_affinity(unsigned irq,
+				       unsigned long cpumask);
+
+#else /* !CONFIG_SMP */
+
+static inline void __adeos_switch_to (adomain_t *adp, int cpuid) {
+
+    __asm__ __volatile__( \
+	"pushl %%edx\n\t" \
+	"pushl %%ecx\n\t" \
+	"pushl %%ebx\n\t" \
+	"pushl %%ebp\n\t" \
+	"pushl %%edi\n\t" \
+	"pushl %%esi\n\t" \
+	"pushfl\n\t" \
+	"cli\n\t" \
+	"movl " __stringify(adp_cpu_current)", %%edx\n\t" \
+	"pushl %%edx\n\t" \
+	"pushl $1f\n\t" \
+	"movl %%esp, 4(%%edx)\n\t" \
+	"movl 4(%%eax), %%esp\n\t" \
+	"movl %%eax, " __stringify(adp_cpu_current)"\n\t" \
+	"ret\n\t" \
+	  /* Call domain switch hook (if any) */
+"1:      popl %%eax\n\t" \
+	"movl (%%eax),%%eax\n\t" \
+	"popfl\n\t" \
+	"testl %%eax,%%eax\n\t" \
+	"je 2f\n\t" \
+	"call *%%eax\n\t" \
+	  /* Domain resume point */
+"2:	 popl %%esi\n\t" \
+	"popl %%edi\n\t" \
+	"popl %%ebp\n\t" \
+	"popl %%ebx\n\t" \
+	"popl %%ecx\n\t" \
+	"popl %%edx\n\t" \
+	: /* no output */ \
+        : "a" (adp));
+}
+
+#endif /* CONFIG_SMP */
+
+void __adeos_check_machine(void);
+
+void __adeos_enable_pipeline(void);
+
+void __adeos_disable_pipeline(void);
+
+void __adeos_init_stage(adomain_t *adp);
+
+void __adeos_sync_stage(void);
+
+int __adeos_ack_system_irq(unsigned irq);
+
+int __adeos_handle_irq(struct pt_regs regs);
+
+extern struct pt_regs __adeos_tick_regs;
+
+extern int __adeos_tick_irq;
+
+#endif /* !__I386_ADEOS_H */
diff -uNrp linux-2.4.23/include/asm-i386/desc.h linux-2.4.23-fusion/include/asm-i386/desc.h
--- linux-2.4.23/include/asm-i386/desc.h	2001-07-26 22:40:32.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/desc.h	2004-01-11 15:29:37.000000000 +0100
@@ -71,9 +71,12 @@ extern void set_tss_desc(unsigned int n,
 
 static inline void clear_LDT(void)
 {
-	int cpu = smp_processor_id();
+	int cpu;
+	preempt_disable();
+	cpu = smp_processor_id();
 	set_ldt_desc(cpu, &default_ldt[0], 5);
 	__load_LDT(cpu);
+	preempt_enable();
 }
 
 /*
diff -uNrp linux-2.4.23/include/asm-i386/hardirq.h linux-2.4.23-fusion/include/asm-i386/hardirq.h
--- linux-2.4.23/include/asm-i386/hardirq.h	2003-11-28 23:18:49.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-i386/hardirq.h	2004-01-11 21:44:07.000000000 +0100
@@ -19,12 +19,16 @@ typedef struct {
 
 /*
  * Are we in an interrupt context? Either doing bottom half
- * or hardware interrupt processing?
+ * or hardware interrupt processing?  Note the preempt check,
+ * this is both a bugfix and an optimization.  If we are
+ * preemptible, we cannot be in an interrupt.
  */
-#define in_interrupt() ({ int __cpu = smp_processor_id(); \
-	(local_irq_count(__cpu) + local_bh_count(__cpu) != 0); })
+#define in_interrupt() (preempt_is_disabled() && \
+	({unsigned long __cpu = smp_processor_id(); \
+	(local_irq_count(__cpu) + local_bh_count(__cpu) != 0); }))
 
-#define in_irq() (local_irq_count(smp_processor_id()) != 0)
+#define in_irq() (preempt_is_disabled() && \
+        (local_irq_count(smp_processor_id()) != 0))
 
 #ifndef CONFIG_SMP
 
@@ -36,6 +40,8 @@ typedef struct {
 
 #define synchronize_irq()	barrier()
 
+#define release_irqlock(cpu)	do { } while (0)
+
 #else
 
 #include <asm/atomic.h>
diff -uNrp linux-2.4.23/include/asm-i386/highmem.h linux-2.4.23-fusion/include/asm-i386/highmem.h
--- linux-2.4.23/include/asm-i386/highmem.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/highmem.h	2004-01-11 21:44:09.000000000 +0100
@@ -91,6 +91,7 @@ static inline void *kmap_atomic(struct p
 	enum fixed_addresses idx;
 	unsigned long vaddr;
 
+	preempt_disable();
 	if (page < highmem_start_page)
 		return page_address(page);
 
@@ -112,8 +113,10 @@ static inline void kunmap_atomic(void *k
 	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 	enum fixed_addresses idx = type + KM_TYPE_NR*smp_processor_id();
 
-	if (vaddr < FIXADDR_START) // FIXME
+	if (vaddr < FIXADDR_START) { // FIXME
+		preempt_enable();
 		return;
+	}
 
 	if (vaddr != __fix_to_virt(FIX_KMAP_BEGIN+idx))
 		out_of_line_bug();
@@ -125,6 +128,8 @@ static inline void kunmap_atomic(void *k
 	pte_clear(kmap_pte-idx);
 	__flush_tlb_one(vaddr);
 #endif
+
+	preempt_enable();
 }
 
 #endif /* __KERNEL__ */
diff -uNrp linux-2.4.23/include/asm-i386/hw_irq.h linux-2.4.23-fusion/include/asm-i386/hw_irq.h
--- linux-2.4.23/include/asm-i386/hw_irq.h	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/hw_irq.h	2004-01-11 21:44:07.000000000 +0100
@@ -56,7 +56,12 @@
  * levels. (0x80 is the syscall vector)
  */
 #define FIRST_DEVICE_VECTOR	0x31
+#ifdef CONFIG_ADEOS_CORE
+/* Reserve 16 more vectors for domains. */
+#define FIRST_SYSTEM_VECTOR	0xdf
+#else /* !CONFIG_ADEOS_CORE */
 #define FIRST_SYSTEM_VECTOR	0xef
+#endif /* CONFIG_ADEOS_CORE */
 
 extern int irq_vector[NR_IRQS];
 #define IO_APIC_VECTOR(irq)	irq_vector[irq]
@@ -95,6 +100,18 @@ extern char _stext, _etext;
 #define __STR(x) #x
 #define STR(x) __STR(x)
 
+#define GET_CURRENT \
+	"movl %esp, %ebx\n\t" \
+	"andl $-8192, %ebx\n\t"
+
+#ifdef CONFIG_PREEMPT
+#define BUMP_LOCK_COUNT \
+	GET_CURRENT \
+	"incl 4(%ebx)\n\t"
+#else
+#define BUMP_LOCK_COUNT
+#endif
+
 #define SAVE_ALL \
 	"cld\n\t" \
 	"pushl %es\n\t" \
@@ -108,15 +125,12 @@ extern char _stext, _etext;
 	"pushl %ebx\n\t" \
 	"movl $" STR(__KERNEL_DS) ",%edx\n\t" \
 	"movl %edx,%ds\n\t" \
-	"movl %edx,%es\n\t"
+	"movl %edx,%es\n\t" \
+	BUMP_LOCK_COUNT
 
 #define IRQ_NAME2(nr) nr##_interrupt(void)
 #define IRQ_NAME(nr) IRQ_NAME2(IRQ##nr)
 
-#define GET_CURRENT \
-	"movl %esp, %ebx\n\t" \
-	"andl $-8192, %ebx\n\t"
-
 /*
  *	SMP has a few special interrupts for IPI messages
  */
diff -uNrp linux-2.4.23/include/asm-i386/i387.h linux-2.4.23-fusion/include/asm-i386/i387.h
--- linux-2.4.23/include/asm-i386/i387.h	2002-08-03 02:39:45.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/i387.h	2004-01-11 21:48:22.000000000 +0100
@@ -12,6 +12,7 @@
 #define __ASM_I386_I387_H
 
 #include <linux/sched.h>
+#include <linux/spinlock.h>
 #include <asm/processor.h>
 #include <asm/sigcontext.h>
 #include <asm/user.h>
@@ -24,7 +25,7 @@ extern void save_init_fpu( struct task_s
 extern void restore_fpu( struct task_struct *tsk );
 
 extern void kernel_fpu_begin(void);
-#define kernel_fpu_end() stts()
+#define kernel_fpu_end() do { stts(); preempt_enable(); } while(0)
 
 
 #define unlazy_fpu( tsk ) do { \
diff -uNrp linux-2.4.23/include/asm-i386/pgalloc.h linux-2.4.23-fusion/include/asm-i386/pgalloc.h
--- linux-2.4.23/include/asm-i386/pgalloc.h	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/pgalloc.h	2004-01-11 21:44:07.000000000 +0100
@@ -75,20 +75,26 @@ static inline pgd_t *get_pgd_fast(void)
 {
 	unsigned long *ret;
 
+	preempt_disable();
 	if ((ret = pgd_quicklist) != NULL) {
 		pgd_quicklist = (unsigned long *)(*ret);
 		ret[0] = 0;
 		pgtable_cache_size--;
-	} else
+		preempt_enable();
+	} else {
+		preempt_enable();
 		ret = (unsigned long *)get_pgd_slow();
+	}
 	return (pgd_t *)ret;
 }
 
 static inline void free_pgd_fast(pgd_t *pgd)
 {
+	preempt_disable();
 	*(unsigned long *)pgd = (unsigned long) pgd_quicklist;
 	pgd_quicklist = (unsigned long *) pgd;
 	pgtable_cache_size++;
+	preempt_enable();
 }
 
 static inline void free_pgd_slow(pgd_t *pgd)
@@ -119,19 +125,23 @@ static inline pte_t *pte_alloc_one_fast(
 {
 	unsigned long *ret;
 
+	preempt_disable();
 	if ((ret = (unsigned long *)pte_quicklist) != NULL) {
 		pte_quicklist = (unsigned long *)(*ret);
 		ret[0] = ret[1];
 		pgtable_cache_size--;
 	}
+	preempt_enable();
 	return (pte_t *)ret;
 }
 
 static inline void pte_free_fast(pte_t *pte)
 {
+	preempt_disable();
 	*(unsigned long *)pte = (unsigned long) pte_quicklist;
 	pte_quicklist = (unsigned long *) pte;
 	pgtable_cache_size++;
+	preempt_enable();
 }
 
 static __inline__ void pte_free_slow(pte_t *pte)
@@ -139,6 +149,42 @@ static __inline__ void pte_free_slow(pte
 	free_page((unsigned long)pte);
 }
 
+#ifdef CONFIG_ADEOS_CORE
+
+/* Non-root Adeos domains cannot might not cope with on-demand
+   mappings for I/O or vmalloc'ed memory, so fall back to a brute
+   force instantaneous kernel global mapping of these areas. */
+
+static inline void set_pgdir(unsigned long address, pgd_t entry) {
+
+	struct task_struct *p;
+	pgd_t *pgd;
+
+	read_lock(&tasklist_lock);
+	for_each_task(p) {
+		if (!p->mm)
+			continue;
+		*pgd_offset(p->mm,address) = entry;
+	}
+	read_unlock(&tasklist_lock);
+#ifdef CONFIG_SMP
+	{
+	int cpu;
+	/* To pgd_alloc/pgd_free, one holds the page table lock and so
+	   does our callee, so we can modify pgd caches of other CPUs
+	   as well. -jj +rpm*/
+	for (cpu = 0; cpu < NR_CPUS; cpu++)
+		for (pgd = (pgd_t *)cpu_data[cpu].pgd_quick; pgd; pgd = (pgd_t *)*(unsigned long *)pgd)
+			pgd[address >> PGDIR_SHIFT] = entry;
+	}
+#else /* !CONFIG_SMP */
+	for (pgd = (pgd_t *)pgd_quicklist; pgd; pgd = (pgd_t *)*(unsigned long *)pgd)
+	       pgd[address >> PGDIR_SHIFT] = entry;
+#endif /* CONFIG_SMP */
+}
+
+#endif /* CONFIG_ADEOS_CORE */
+
 #define pte_free(pte)		pte_free_fast(pte)
 #define pgd_free(pgd)		free_pgd_slow(pgd)
 #define pgd_alloc(mm)		get_pgd_fast()
diff -uNrp linux-2.4.23/include/asm-i386/smp.h linux-2.4.23-fusion/include/asm-i386/smp.h
--- linux-2.4.23/include/asm-i386/smp.h	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-i386/smp.h	2004-01-11 21:44:07.000000000 +0100
@@ -81,7 +81,11 @@ extern void smp_store_cpu_info(int id);	
  * so this is correct in the x86 case.
  */
 
+#ifdef CONFIG_ADEOS_CORE
+#define smp_processor_id() adeos_processor_id()
+#else /* !CONFIG_ADEOS_CORE */
 #define smp_processor_id() (current->processor)
+#endif /* CONFIG_ADEOS_CORE */
 
 static __inline int hard_smp_processor_id(void)
 {
diff -uNrp linux-2.4.23/include/asm-i386/smplock.h linux-2.4.23-fusion/include/asm-i386/smplock.h
--- linux-2.4.23/include/asm-i386/smplock.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/smplock.h	2004-01-11 21:44:07.000000000 +0100
@@ -14,7 +14,15 @@
 extern spinlock_cacheline_t kernel_flag_cacheline;  
 #define kernel_flag kernel_flag_cacheline.lock      
 
+#ifdef CONFIG_SMP
 #define kernel_locked()		spin_is_locked(&kernel_flag)
+#else
+#ifdef CONFIG_PREEMPT
+#define kernel_locked()		preempt_get_count()
+#else
+#define kernel_locked()		1
+#endif
+#endif
 
 /*
  * Release global kernel lock and global interrupt lock
@@ -46,6 +54,11 @@ do { \
  */
 static __inline__ void lock_kernel(void)
 {
+#ifdef CONFIG_PREEMPT
+	if (current->lock_depth == -1)
+		spin_lock(&kernel_flag);
+	++current->lock_depth;
+#else
 #if 1
 	if (!++current->lock_depth)
 		spin_lock(&kernel_flag);
@@ -58,6 +71,7 @@ static __inline__ void lock_kernel(void)
 		:"=m" (__dummy_lock(&kernel_flag)),
 		 "=m" (current->lock_depth));
 #endif
+#endif
 }
 
 static __inline__ void unlock_kernel(void)
diff -uNrp linux-2.4.23/include/asm-i386/softirq.h linux-2.4.23-fusion/include/asm-i386/softirq.h
--- linux-2.4.23/include/asm-i386/softirq.h	2002-08-03 02:39:45.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/softirq.h	2004-01-11 21:44:07.000000000 +0100
@@ -5,14 +5,15 @@
 #include <asm/hardirq.h>
 
 #define __cpu_bh_enable(cpu) \
-		do { barrier(); local_bh_count(cpu)--; } while (0)
+		do { barrier(); local_bh_count(cpu)--; preempt_enable(); } while (0)
 #define cpu_bh_disable(cpu) \
-		do { local_bh_count(cpu)++; barrier(); } while (0)
+		do { preempt_disable(); local_bh_count(cpu)++; barrier(); } while (0)
 
 #define local_bh_disable()	cpu_bh_disable(smp_processor_id())
 #define __local_bh_enable()	__cpu_bh_enable(smp_processor_id())
 
-#define in_softirq() (local_bh_count(smp_processor_id()) != 0)
+#define in_softirq() ( preempt_is_disabled() & \
+			(local_bh_count(smp_processor_id()) != 0))
 
 /*
  * NOTE: this assembly code assumes:
@@ -22,7 +23,7 @@
  * If you change the offsets in irq_stat then you have to
  * update this code as well.
  */
-#define local_bh_enable()						\
+#define _local_bh_enable()						\
 do {									\
 	unsigned int *ptr = &local_bh_count(smp_processor_id());	\
 									\
@@ -45,4 +46,6 @@ do {									\
 		/* no registers clobbered */ );				\
 } while (0)
 
+#define local_bh_enable() do { _local_bh_enable(); preempt_enable(); } while (0)
+
 #endif	/* __ASM_SOFTIRQ_H */
diff -uNrp linux-2.4.23/include/asm-i386/spinlock.h linux-2.4.23-fusion/include/asm-i386/spinlock.h
--- linux-2.4.23/include/asm-i386/spinlock.h	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-i386/spinlock.h	2004-01-11 21:44:06.000000000 +0100
@@ -77,7 +77,7 @@ typedef struct {
 		:"=m" (lock->lock) : : "memory"
 
 
-static inline void spin_unlock(spinlock_t *lock)
+static inline void _raw_spin_unlock(spinlock_t *lock)
 {
 #if SPINLOCK_DEBUG
 	if (lock->magic != SPINLOCK_MAGIC)
@@ -97,7 +97,7 @@ static inline void spin_unlock(spinlock_
 		:"=q" (oldval), "=m" (lock->lock) \
 		:"0" (oldval) : "memory"
 
-static inline void spin_unlock(spinlock_t *lock)
+static inline void _raw_spin_unlock(spinlock_t *lock)
 {
 	char oldval = 1;
 #if SPINLOCK_DEBUG
@@ -113,7 +113,7 @@ static inline void spin_unlock(spinlock_
 
 #endif
 
-static inline int spin_trylock(spinlock_t *lock)
+static inline int _raw_spin_trylock(spinlock_t *lock)
 {
 	char oldval;
 	__asm__ __volatile__(
@@ -123,7 +123,7 @@ static inline int spin_trylock(spinlock_
 	return oldval > 0;
 }
 
-static inline void spin_lock(spinlock_t *lock)
+static inline void _raw_spin_lock(spinlock_t *lock)
 {
 #if SPINLOCK_DEBUG
 	__label__ here;
@@ -179,7 +179,7 @@ typedef struct {
  */
 /* the spinlock helpers are in arch/i386/kernel/semaphore.c */
 
-static inline void read_lock(rwlock_t *rw)
+static inline void _raw_read_lock(rwlock_t *rw)
 {
 #if SPINLOCK_DEBUG
 	if (rw->magic != RWLOCK_MAGIC)
@@ -188,7 +188,7 @@ static inline void read_lock(rwlock_t *r
 	__build_read_lock(rw, "__read_lock_failed");
 }
 
-static inline void write_lock(rwlock_t *rw)
+static inline void _raw_write_lock(rwlock_t *rw)
 {
 #if SPINLOCK_DEBUG
 	if (rw->magic != RWLOCK_MAGIC)
@@ -197,10 +197,10 @@ static inline void write_lock(rwlock_t *
 	__build_write_lock(rw, "__write_lock_failed");
 }
 
-#define read_unlock(rw)		asm volatile("lock ; incl %0" :"=m" ((rw)->lock) : : "memory")
-#define write_unlock(rw)	asm volatile("lock ; addl $" RW_LOCK_BIAS_STR ",%0":"=m" ((rw)->lock) : : "memory")
+#define _raw_read_unlock(rw)		asm volatile("lock ; incl %0" :"=m" ((rw)->lock) : : "memory")
+#define _raw_write_unlock(rw)	asm volatile("lock ; addl $" RW_LOCK_BIAS_STR ",%0":"=m" ((rw)->lock) : : "memory")
 
-static inline int write_trylock(rwlock_t *lock)
+static inline int _raw_write_trylock(rwlock_t *lock)
 {
 	atomic_t *count = (atomic_t *)lock;
 	if (atomic_sub_and_test(RW_LOCK_BIAS, count))
diff -uNrp linux-2.4.23/include/asm-i386/system.h linux-2.4.23-fusion/include/asm-i386/system.h
--- linux-2.4.23/include/asm-i386/system.h	2003-08-25 13:44:43.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-i386/system.h	2004-01-11 21:44:06.000000000 +0100
@@ -12,6 +12,34 @@
 struct task_struct;	/* one of the stranger aspects of C forward declarations.. */
 extern void FASTCALL(__switch_to(struct task_struct *prev, struct task_struct *next));
 
+#ifdef CONFIG_ADEOS_CORE
+#define prepare_to_switch(prev,next) \
+do { \
+    struct { struct task_struct *prev, *next; } arg = { (prev), (next) }; \
+    __adeos_schedule_head(&arg); \
+    adeos_hw_cli(); \
+} while(0)
+#define switch_to(prev,next,last) do {					\
+	asm volatile("pushl %%esi\n\t"					\
+		     "pushl %%edi\n\t"					\
+		     "pushl %%ebp\n\t"					\
+		     "movl %%esp,%0\n\t"	/* save ESP */		\
+		     "movl %3,%%esp\n\t"	/* restore ESP */	\
+		     "movl $1f,%1\n\t"		/* save EIP */		\
+		     "pushl %4\n\t"		/* restore EIP */	\
+		     "jmp __switch_to\n"				\
+		     "1:\t"						\
+		     "popl %%ebp\n\t"					\
+		     "popl %%edi\n\t"					\
+		     "popl %%esi\n\t"					\
+		     "sti\n\t"						\
+		     :"=m" (prev->thread.esp),"=m" (prev->thread.eip),	\
+		      "=b" (last)					\
+		     :"m" (next->thread.esp),"m" (next->thread.eip),	\
+		      "a" (prev), "d" (next),				\
+		      "b" (prev));					\
+} while (0)
+#else /* !CONFIG_ADEOS_CORE */
 #define prepare_to_switch()	do { } while(0)
 #define switch_to(prev,next,last) do {					\
 	asm volatile("pushl %%esi\n\t"					\
@@ -32,6 +60,7 @@ extern void FASTCALL(__switch_to(struct 
 		      "a" (prev), "d" (next),				\
 		      "b" (prev));					\
 } while (0)
+#endif /* CONFIG_ADEOS_CORE */
 
 #define _set_base(addr,base) do { unsigned long __pr; \
 __asm__ __volatile__ ("movw %%dx,%1\n\t" \
@@ -316,6 +345,40 @@ static inline unsigned long __cmpxchg(vo
 #define set_wmb(var, value) do { var = value; wmb(); } while (0)
 
 /* interrupt control.. */
+
+#ifdef CONFIG_ADEOS_CORE
+
+extern inline void __adeos_stall_root(void);
+
+extern inline void __adeos_unstall_root(void);
+
+extern inline unsigned long __adeos_test_root(void);
+
+extern inline unsigned long __adeos_test_and_stall_root(void);
+
+void FASTCALL(__adeos_restore_root(unsigned long flags));
+
+#define __save_flags(x)		((x) = __adeos_test_root())
+#define __restore_flags(x) 	__adeos_restore_root(x)
+#define __cli() 		__adeos_stall_root()
+#define __sti()			__adeos_unstall_root()
+/* used in the idle loop; sti takes one instruction cycle to complete */
+#define safe_halt()		__asm__ __volatile__("call "__stringify(__adeos_unstall_root)"; hlt": : :"memory")
+
+#define __save_and_cli(x)	do { __save_flags(x); __cli(); } while(0);
+#define __save_and_sti(x)	do { __save_flags(x); __sti(); } while(0);
+
+/* For spinlocks etc */
+#define local_irq_save(x)	((x) = __adeos_test_and_stall_root())
+#define local_irq_set(x)	__save_and_sti(x)
+#define local_irq_restore(x)	__adeos_restore_root(x)
+#define local_irq_disable()	__adeos_stall_root()
+#define local_irq_enable()	__adeos_unstall_root()
+
+#define irqs_disabled()		__adeos_test_root()
+
+#else /* !CONFIG_ADEOS_CORE */
+
 #define __save_flags(x)		__asm__ __volatile__("pushfl ; popl %0":"=g" (x): /* no input */)
 #define __restore_flags(x) 	__asm__ __volatile__("pushl %0 ; popfl": /* no output */ :"g" (x):"memory", "cc")
 #define __cli() 		__asm__ __volatile__("cli": : :"memory")
@@ -326,6 +389,13 @@ static inline unsigned long __cmpxchg(vo
 #define __save_and_cli(x)	do { __save_flags(x); __cli(); } while(0);
 #define __save_and_sti(x)	do { __save_flags(x); __sti(); } while(0);
 
+#define irqs_disabled()			\
+({					\
+	unsigned long flags;		\
+	__save_flags(flags);		\
+	!(flags & (1<<9));		\
+})
+
 /* For spinlocks etc */
 #if 0
 #define local_irq_save(x)	__asm__ __volatile__("pushfl ; popl %0 ; cli":"=g" (x): /* no input */ :"memory")
@@ -339,6 +409,8 @@ static inline unsigned long __cmpxchg(vo
 #define local_irq_disable()	__cli()
 #define local_irq_enable()	__sti()
 
+#endif /* CONFIG_ADEOS_CORE */
+
 #ifdef CONFIG_SMP
 
 extern void __global_cli(void);
diff -uNrp linux-2.4.23/include/asm-mips/smplock.h linux-2.4.23-fusion/include/asm-mips/smplock.h
--- linux-2.4.23/include/asm-mips/smplock.h	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-mips/smplock.h	2004-01-11 15:29:37.000000000 +0100
@@ -8,12 +8,21 @@
 #ifndef __ASM_SMPLOCK_H
 #define __ASM_SMPLOCK_H
 
+#include <linux/config.h>
 #include <linux/interrupt.h>
 #include <linux/spinlock.h>
 
 extern spinlock_t kernel_flag;
 
+#ifdef CONFIG_SMP
 #define kernel_locked()		spin_is_locked(&kernel_flag)
+#else
+#ifdef CONFIG_PREEMPT
+#define kernel_locked()         preempt_get_count()
+#else
+#define kernel_locked()         1
+#endif
+#endif
 
 /*
  * Release global kernel lock and global interrupt lock
@@ -45,8 +54,14 @@ do { \
  */
 extern __inline__ void lock_kernel(void)
 {
+#ifdef CONFIG_PREEMPT
+	if (current->lock_depth == -1)
+		spin_lock(&kernel_flag);
+	++current->lock_depth;
+#else
 	if (!++current->lock_depth)
 		spin_lock(&kernel_flag);
+#endif
 }
 
 extern __inline__ void unlock_kernel(void)
diff -uNrp linux-2.4.23/include/asm-mips/softirq.h linux-2.4.23-fusion/include/asm-mips/softirq.h
--- linux-2.4.23/include/asm-mips/softirq.h	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-mips/softirq.h	2004-01-11 15:29:37.000000000 +0100
@@ -15,6 +15,7 @@
 
 static inline void cpu_bh_disable(int cpu)
 {
+	preempt_disable();
 	local_bh_count(cpu)++;
 	barrier();
 }
@@ -23,6 +24,7 @@ static inline void __cpu_bh_enable(int c
 {
 	barrier();
 	local_bh_count(cpu)--;
+	preempt_enable();
 }
 
 
@@ -36,6 +38,7 @@ do {								\
 	cpu = smp_processor_id();				\
 	if (!--local_bh_count(cpu) && softirq_pending(cpu))	\
 		do_softirq();					\
+	preempt_enable();                                       \
 } while (0)
 
 #define in_softirq() (local_bh_count(smp_processor_id()) != 0)
diff -uNrp linux-2.4.23/include/asm-mips/system.h linux-2.4.23-fusion/include/asm-mips/system.h
--- linux-2.4.23/include/asm-mips/system.h	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-mips/system.h	2004-01-11 15:29:37.000000000 +0100
@@ -337,4 +337,18 @@ extern void __die_if_kernel(const char *
 #define die_if_kernel(msg, regs)					\
 	__die_if_kernel(msg, regs, __FILE__ ":", __FUNCTION__, __LINE__)
 
+extern __inline__ int intr_on(void)
+{
+	unsigned long flags;
+	save_flags(flags);
+	return flags & 1;
+}
+
+extern __inline__ int intr_off(void)
+{
+	return ! intr_on();
+}
+
+#define irqs_disabled()	intr_off()
+
 #endif /* _ASM_SYSTEM_H */
diff -uNrp linux-2.4.23/include/asm-ppc/dma.h linux-2.4.23-fusion/include/asm-ppc/dma.h
--- linux-2.4.23/include/asm-ppc/dma.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-ppc/dma.h	2004-01-11 15:29:37.000000000 +0100
@@ -11,6 +11,7 @@
 #include <linux/config.h>
 #include <asm/io.h>
 #include <linux/spinlock.h>
+#include <linux/sched.h>
 #include <asm/system.h>
 
 /*
diff -uNrp linux-2.4.23/include/asm-ppc/hardirq.h linux-2.4.23-fusion/include/asm-ppc/hardirq.h
--- linux-2.4.23/include/asm-ppc/hardirq.h	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-ppc/hardirq.h	2004-01-11 15:29:37.000000000 +0100
@@ -31,10 +31,12 @@ typedef struct {
  * Are we in an interrupt context? Either doing bottom half
  * or hardware interrupt processing?
  */
-#define in_interrupt() ({ int __cpu = smp_processor_id(); \
-	(local_irq_count(__cpu) + local_bh_count(__cpu) != 0); })
+#define in_interrupt() (preempt_is_disabled() && \
+	({ unsigned long __cpu = smp_processor_id(); \
+	(local_irq_count(__cpu) + local_bh_count(__cpu) != 0); }))
 
-#define in_irq() (local_irq_count(smp_processor_id()) != 0)
+#define in_irq() (preempt_is_disabled() && \
+	(local_irq_count(smp_processor_id()) != 0))
 
 #ifndef CONFIG_SMP
 
@@ -45,6 +47,7 @@ typedef struct {
 #define hardirq_exit(cpu)	(local_irq_count(cpu)--)
 
 #define synchronize_irq()	do { } while (0)
+#define release_irqlock(cpu)	do { } while (0)
 
 #else /* CONFIG_SMP */
 
diff -uNrp linux-2.4.23/include/asm-ppc/highmem.h linux-2.4.23-fusion/include/asm-ppc/highmem.h
--- linux-2.4.23/include/asm-ppc/highmem.h	2003-11-28 23:18:50.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-ppc/highmem.h	2004-01-11 15:29:37.000000000 +0100
@@ -84,6 +84,7 @@ static inline void *kmap_atomic(struct p
 	unsigned int idx;
 	unsigned long vaddr;
 
+	preempt_disable();
 	if (page < highmem_start_page)
 		return page_address(page);
 
@@ -105,8 +106,10 @@ static inline void kunmap_atomic(void *k
 	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 	unsigned int idx = type + KM_TYPE_NR*smp_processor_id();
 
-	if (vaddr < KMAP_FIX_BEGIN) // FIXME
+	if (vaddr < KMAP_FIX_BEGIN) { // FIXME
+		preempt_enable();
 		return;
+	}
 
 	if (vaddr != KMAP_FIX_BEGIN + idx * PAGE_SIZE)
 		BUG();
@@ -118,6 +121,7 @@ static inline void kunmap_atomic(void *k
 	pte_clear(kmap_pte+idx);
 	flush_tlb_page(0, vaddr);
 #endif
+	preempt_enable();
 }
 
 #endif /* __KERNEL__ */
diff -uNrp linux-2.4.23/include/asm-ppc/hw_irq.h linux-2.4.23-fusion/include/asm-ppc/hw_irq.h
--- linux-2.4.23/include/asm-ppc/hw_irq.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-ppc/hw_irq.h	2004-01-11 15:29:37.000000000 +0100
@@ -20,6 +20,12 @@ extern unsigned long __sti_end, __cli_en
 #define __save_and_cli(flags) ({__save_flags(flags);__cli();})
 #define __save_and_sti(flags) ({__save_flags(flags);__sti();})
 
+#define mfmsr()		({unsigned int rval; \
+			asm volatile("mfmsr %0" : "=r" (rval)); rval;})
+#define mtmsr(v)	asm volatile("mtmsr %0" : : "r" (v))
+
+#define irqs_disabled()	((mfmsr() & MSR_EE) == 0)
+
 extern void do_lost_interrupts(unsigned long);
 
 #define mask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->disable) irq_desc[irq].handler->disable(irq);})
diff -uNrp linux-2.4.23/include/asm-ppc/mmu_context.h linux-2.4.23-fusion/include/asm-ppc/mmu_context.h
--- linux-2.4.23/include/asm-ppc/mmu_context.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-ppc/mmu_context.h	2004-01-11 15:29:37.000000000 +0100
@@ -155,6 +155,10 @@ static inline void destroy_context(struc
 static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 			     struct task_struct *tsk, int cpu)
 {
+#ifdef CONFIG_PREEMPT
+	if (preempt_get_count() == 0)
+		BUG();
+#endif
 	tsk->thread.pgdir = next->pgd;
 	get_mmu_context(next);
 	set_context(next->context, next->pgd);
diff -uNrp linux-2.4.23/include/asm-ppc/pgalloc.h linux-2.4.23-fusion/include/asm-ppc/pgalloc.h
--- linux-2.4.23/include/asm-ppc/pgalloc.h	2003-11-28 23:18:50.000000000 +0100
+++ linux-2.4.23-fusion/include/asm-ppc/pgalloc.h	2004-01-11 15:29:37.000000000 +0100
@@ -72,20 +72,25 @@ extern __inline__ pgd_t *get_pgd_fast(vo
 {
         unsigned long *ret;
 
+	preempt_disable();
         if ((ret = pgd_quicklist) != NULL) {
                 pgd_quicklist = (unsigned long *)(*ret);
                 ret[0] = 0;
                 pgtable_cache_size--;
+		preempt_enable();
         } else
+		preempt_enable();
                 ret = (unsigned long *)get_pgd_slow();
         return (pgd_t *)ret;
 }
 
 extern __inline__ void free_pgd_fast(pgd_t *pgd)
 {
+	preempt_disable();
         *(unsigned long **)pgd = pgd_quicklist;
         pgd_quicklist = (unsigned long *) pgd;
         pgtable_cache_size++;
+	preempt_enable();
 }
 
 extern __inline__ void free_pgd_slow(pgd_t *pgd)
@@ -124,19 +129,23 @@ static inline pte_t *pte_alloc_one_fast(
 {
         unsigned long *ret;
 
+	preempt_disable();
         if ((ret = pte_quicklist) != NULL) {
                 pte_quicklist = (unsigned long *)(*ret);
                 ret[0] = 0;
                 pgtable_cache_size--;
 	}
+	preempt_enable();
         return (pte_t *)ret;
 }
 
 extern __inline__ void pte_free_fast(pte_t *pte)
 {
+	preempt_disable();
         *(unsigned long **)pte = pte_quicklist;
         pte_quicklist = (unsigned long *) pte;
         pgtable_cache_size++;
+	preempt_enable();
 }
 
 extern __inline__ void pte_free_slow(pte_t *pte)
diff -uNrp linux-2.4.23/include/asm-ppc/smplock.h linux-2.4.23-fusion/include/asm-ppc/smplock.h
--- linux-2.4.23/include/asm-ppc/smplock.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-ppc/smplock.h	2004-01-11 15:29:37.000000000 +0100
@@ -12,7 +12,15 @@
 
 extern spinlock_t kernel_flag;
 
+#ifdef CONFIG_SMP
 #define kernel_locked()		spin_is_locked(&kernel_flag)
+#else
+#ifdef CONFIG_PREEMPT
+#define kernel_locked()		preempt_get_count()
+#else
+#define kernel_locked()		1
+#endif
+#endif
 
 /*
  * Release global kernel lock and global interrupt lock
@@ -44,8 +52,14 @@ do { \
  */
 static __inline__ void lock_kernel(void)
 {
+#ifdef CONFIG_PREEMPT
+	if (current->lock_depth == -1)
+		spin_lock(&kernel_flag);
+	++current->lock_depth;
+#else
 	if (!++current->lock_depth)
 		spin_lock(&kernel_flag);
+#endif
 }
 
 static __inline__ void unlock_kernel(void)
diff -uNrp linux-2.4.23/include/asm-ppc/softirq.h linux-2.4.23-fusion/include/asm-ppc/softirq.h
--- linux-2.4.23/include/asm-ppc/softirq.h	2003-06-13 16:51:38.000000000 +0200
+++ linux-2.4.23-fusion/include/asm-ppc/softirq.h	2004-01-11 15:29:37.000000000 +0100
@@ -7,6 +7,7 @@
 
 #define local_bh_disable()			\
 do {						\
+	preempt_disable();			\
 	local_bh_count(smp_processor_id())++;	\
 	barrier();				\
 } while (0)
@@ -15,9 +16,10 @@ do {						\
 do {						\
 	barrier();				\
 	local_bh_count(smp_processor_id())--;	\
+	preempt_enable();			\
 } while (0)
 
-#define local_bh_enable()				\
+#define _local_bh_enable()				\
 do {							\
 	if (!--local_bh_count(smp_processor_id())	\
 	    && softirq_pending(smp_processor_id())) {	\
@@ -25,7 +27,14 @@ do {							\
 	}						\
 } while (0)
 
-#define in_softirq() (local_bh_count(smp_processor_id()) != 0)
+#define local_bh_enable()			\
+do {						\
+	_local_bh_enable();			\
+	preempt_enable();			\
+} while (0)
+
+#define in_softirq() (preempt_is_disabled() && \
+			(local_bh_count(smp_processor_id()) != 0))
 
 #endif	/* __ASM_SOFTIRQ_H */
 #endif /* __KERNEL__ */
diff -uNrp linux-2.4.23/include/linux/adeos.h linux-2.4.23-fusion/include/linux/adeos.h
--- linux-2.4.23/include/linux/adeos.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/adeos.h	2004-01-11 21:44:07.000000000 +0100
@@ -0,0 +1,368 @@
+/*
+ *   include/linux/adeos.h
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef __LINUX_ADEOS_H
+#define __LINUX_ADEOS_H
+
+#include <linux/kernel.h>
+#include <asm/adeos.h>
+
+#define ADEOS_VERSION_PREFIX  "2.4r10-fusion/"
+#define ADEOS_VERSION_STRING  (ADEOS_VERSION_PREFIX ADEOS_ARCH_STRING)
+#define ADEOS_RELEASE_NUMBER  0x02040a00
+
+#define ADEOS_ROOT_PRI       100
+#define ADEOS_ROOT_ID        0
+#define ADEOS_ROOT_NPTDKEYS  4	/* Must be <= 32 */
+
+#define ADEOS_OTHER_CPUS   (-1)
+#define ADEOS_RESET_TIMER  0x1
+
+#define IPIPE_STALL_FLAG   0	/* Stalls a pipeline stage */
+#define IPIPE_SYNC_FLAG    1	/* IRQ sync is undergoing */
+#define IPIPE_XPEND_FLAG   2	/* Exception notification is pending */
+#define IPIPE_SLEEP_FLAG   3	/* Domain has suspended itself */
+
+#define IPIPE_HANDLE_FLAG    0
+#define IPIPE_PASS_FLAG      1
+#define IPIPE_CALLASM_FLAG   2
+#define IPIPE_ENABLE_FLAG    3
+#define IPIPE_DYNAMIC_FLAG   IPIPE_HANDLE_FLAG
+#define IPIPE_EXCLUSIVE_FLAG 4
+#define IPIPE_STICKY_FLAG    5
+#define IPIPE_SYSTEM_FLAG    6
+#define IPIPE_LOCK_FLAG      7
+
+#define IPIPE_HANDLE_MASK    (1 << IPIPE_HANDLE_FLAG)
+#define IPIPE_PASS_MASK      (1 << IPIPE_PASS_FLAG)
+#define IPIPE_CALLASM_MASK   (1 << IPIPE_CALLASM_FLAG)
+#define IPIPE_ENABLE_MASK    (1 << IPIPE_ENABLE_FLAG)
+#define IPIPE_DYNAMIC_MASK   IPIPE_HANDLE_MASK
+#define IPIPE_EXCLUSIVE_MASK (1 << IPIPE_EXCLUSIVE_FLAG)
+#define IPIPE_STICKY_MASK    (1 << IPIPE_STICKY_FLAG)
+#define IPIPE_SYSTEM_MASK    (1 << IPIPE_SYSTEM_FLAG)
+#define IPIPE_LOCK_MASK      (1 << IPIPE_LOCK_FLAG)
+
+#define IPIPE_DEFAULT_MASK  (IPIPE_HANDLE_MASK|IPIPE_PASS_MASK)
+
+typedef struct adattr {
+
+    unsigned domid;		/* Domain identifier -- Magic value set by caller */
+    const char *name;		/* Domain name -- Warning: won't be dup'ed! */
+    int priority;		/* Priority in interrupt pipeline */
+    void (*entry)(int);		/* Domain entry point */
+    int estacksz;		/* Stack size for entry context -- 0 means unspec */
+    void (*dswitch)(void);	/* Handler called each time the domain is switched in */
+    int nptdkeys;		/* Max. number of per-thread data keys */
+    void (*ptdset)(int,void *);	/* Routine to set pt values */
+    void *(*ptdget)(int);	/* Routine to get pt values */
+
+} adattr_t;
+
+extern adomain_t *adp_cpu_current[],
+                 *adp_root;
+
+extern int __adeos_event_monitors[];
+
+#define adp_current (adp_cpu_current[adeos_processor_id()])
+
+/* Private interface */
+
+#ifdef CONFIG_PROC_FS
+void __adeos_init_proc(void);
+#endif /* CONFIG_PROC_FS */
+
+void __adeos_takeover(void);
+
+int __adeos_handle_event(unsigned event,
+			 void *evdata);
+
+void __adeos_dump_state(void);
+
+static inline void __adeos_schedule_head(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SCHEDULE_HEAD] > 0))
+	__adeos_handle_event(ADEOS_SCHEDULE_HEAD,evdata);
+}
+
+static inline int __adeos_schedule_tail(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SCHEDULE_TAIL] > 0))
+	return __adeos_handle_event(ADEOS_SCHEDULE_TAIL,evdata);
+
+    return 0;
+}
+
+static inline void __adeos_enter_process(void) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_ENTER_PROCESS] > 0))
+	__adeos_handle_event(ADEOS_ENTER_PROCESS,NULL);
+}
+
+static inline void __adeos_exit_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_EXIT_PROCESS] > 0))
+	__adeos_handle_event(ADEOS_EXIT_PROCESS,evdata);
+}
+
+static inline int __adeos_signal_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SIGNAL_PROCESS] > 0))
+	return __adeos_handle_event(ADEOS_SIGNAL_PROCESS,evdata);
+
+    return 0;
+}
+
+static inline int __adeos_renice_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_RENICE_PROCESS] > 0))
+	return __adeos_handle_event(ADEOS_RENICE_PROCESS,evdata);
+
+    return 0;
+}
+
+extern inline void __adeos_stall_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+	set_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	}
+    else
+	adeos_hw_cli();
+}
+
+extern inline void __adeos_unstall_root (void)
+
+{
+    adeos_hw_sti();	/* Absolutely needed. */
+
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+
+	clear_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+
+	if (adp_root->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+	}
+}
+
+extern inline unsigned long __adeos_test_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+	return test_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	}
+
+    return adeos_hw_irqs_disabled();
+}
+
+extern inline unsigned long __adeos_test_and_stall_root (void)
+
+{
+    unsigned long flags;
+
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+	return test_and_set_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	}
+
+    adeos_hw_local_irq_save(flags);
+
+    return !adeos_hw_test_iflag(flags);
+}
+
+void FASTCALL(__adeos_restore_root(unsigned long flags));
+
+/* Public interface */
+
+int adeos_register_domain(adomain_t *adp,
+			  adattr_t *attr);
+
+int adeos_unregister_domain(adomain_t *adp);
+
+void adeos_renice_domain(int newpri);
+
+void adeos_suspend_domain(void);
+
+int adeos_virtualize_irq(unsigned irq,
+			 void (*handler)(unsigned irq),
+			 int (*acknowledge)(unsigned irq),
+			 unsigned modemask);
+
+int adeos_control_irq(unsigned irq,
+		      unsigned clrmask,
+		      unsigned setmask);
+
+unsigned long adeos_set_irq_affinity(unsigned irq,
+				     unsigned long cpumask);
+unsigned adeos_alloc_irq(void);
+
+int adeos_free_irq(unsigned irq);
+
+int FASTCALL(adeos_trigger_irq(unsigned irq));
+
+int FASTCALL(adeos_propagate_irq(unsigned irq));
+
+int FASTCALL(adeos_schedule_irq(unsigned irq));
+
+int FASTCALL(adeos_trigger_ipi(int cpuid));
+
+static inline void adeos_stall_pipeline_from (adomain_t *adp) {
+
+    adeos_declare_cpuid;
+    set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+}
+
+static inline void adeos_stall_pipeline (void) {
+
+    adeos_stall_pipeline_from(adp_current);
+}
+
+void FASTCALL(adeos_unstall_pipeline_from(adomain_t *adp));
+
+static inline void adeos_unstall_pipeline (void) {
+
+    adeos_declare_cpuid;
+
+    clear_bit(IPIPE_STALL_FLAG,&adp_current->cpudata[cpuid].status);
+
+    if (unlikely(adp_current->cpudata[cpuid].irq_pending_hi != 0))
+	__adeos_sync_stage();
+}
+
+static inline unsigned long adeos_test_pipeline_from (adomain_t *adp) {
+
+    return test_bit(IPIPE_STALL_FLAG,&adp->cpudata[adeos_processor_id()].status);
+}
+
+static inline unsigned long adeos_test_pipeline_remote (adomain_t *adp, int cpuid) {
+
+    return test_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+}
+
+static inline unsigned long adeos_test_pipeline (void) {
+
+    return adeos_test_pipeline_from(adp_current);
+}
+
+static inline unsigned long adeos_test_and_stall_pipeline_from (adomain_t *adp) {
+    
+    return test_and_set_bit(IPIPE_STALL_FLAG,&adp->cpudata[adeos_processor_id()].status);
+}
+
+static inline unsigned long adeos_test_and_stall_pipeline_remote (adomain_t *adp, int cpuid) {
+    
+    return test_and_set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+}
+
+static inline unsigned long adeos_test_and_stall_pipeline (void) {
+    
+    return adeos_test_and_stall_pipeline_from(adp_current);
+}
+
+static inline void adeos_restore_pipeline_from (adomain_t *adp, unsigned long flags) {
+
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[adeos_processor_id()].status);
+    else
+	adeos_unstall_pipeline_from(adp);
+}
+
+static inline void adeos_restore_pipeline (unsigned long flags) {
+
+    adeos_declare_cpuid;
+    adomain_t *adp = adp_cpu_current[cpuid];
+
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    else
+	{
+	clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+	if (unlikely(adp->cpudata[cpuid].irq_pending_hi != 0))
+	    __adeos_sync_stage();
+	}
+}
+
+static inline void adeos_restore_pipeline_nosync (adomain_t *adp, unsigned long flags, int cpuid) {
+
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    else
+	clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+}
+
+static inline void adeos_restore_pipeline_remote (adomain_t *adp, unsigned long flags, int cpuid) {
+
+    adeos_restore_pipeline_nosync(adp,flags,cpuid);
+}
+
+int adeos_catch_event_from(adomain_t *adp,
+			   unsigned event,
+			   void (*handler)(adevinfo_t *));
+
+static inline int adeos_catch_event (unsigned event, void (*handler)(adevinfo_t *)) {
+    return adeos_catch_event_from(adp_current,event,handler);
+}
+
+static inline void adeos_propagate_event(adevinfo_t *evinfo) {
+
+    evinfo->propagate = 1;
+}
+
+void (*adeos_hook_dswitch(void (*handler)(void)))(void);
+
+void adeos_init_attr(adattr_t *attr);
+
+int adeos_get_sysinfo(adsysinfo_t *sysinfo);
+
+int adeos_tune_timer(unsigned long ns,
+		     int flags);
+
+int adeos_alloc_ptdkey(void);
+
+int adeos_free_ptdkey(int key);
+
+int adeos_set_ptd(int key,
+		  void *value);
+
+void *adeos_get_ptd(int key);
+
+unsigned long adeos_critical_enter(void (*syncfn)(void));
+
+void adeos_critical_exit(unsigned long flags);
+
+int adeos_init_mutex(admutex_t *mutex);
+
+int adeos_destroy_mutex(admutex_t *mutex);
+
+unsigned long FASTCALL(adeos_lock_mutex(admutex_t *mutex));
+
+void FASTCALL(adeos_unlock_mutex(admutex_t *mutex,
+				 unsigned long flags));
+
+#endif /* !__LINUX_ADEOS_H */
diff -uNrp linux-2.4.23/include/linux/brlock.h linux-2.4.23-fusion/include/linux/brlock.h
--- linux-2.4.23/include/linux/brlock.h	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/brlock.h	2004-01-11 21:44:37.000000000 +0100
@@ -171,11 +171,11 @@ static inline void br_write_unlock (enum
 }
 
 #else
-# define br_read_lock(idx)	((void)(idx))
-# define br_read_unlock(idx)	((void)(idx))
-# define br_write_lock(idx)	((void)(idx))
-# define br_write_unlock(idx)	((void)(idx))
-#endif
+# define br_read_lock(idx)	({ (void)(idx); preempt_disable(); })
+# define br_read_unlock(idx)	({ (void)(idx); preempt_enable(); })
+# define br_write_lock(idx)	({ (void)(idx); preempt_disable(); })
+# define br_write_unlock(idx)	({ (void)(idx); preempt_enable(); })
+#endif	/* CONFIG_SMP */
 
 /*
  * Now enumerate all of the possible sw/hw IRQ protected
diff -uNrp linux-2.4.23/include/linux/dcache.h linux-2.4.23-fusion/include/linux/dcache.h
--- linux-2.4.23/include/linux/dcache.h	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/dcache.h	2004-01-11 21:44:06.000000000 +0100
@@ -127,31 +127,6 @@ d_iput:		no		no		yes
 
 extern spinlock_t dcache_lock;
 
-/**
- * d_drop - drop a dentry
- * @dentry: dentry to drop
- *
- * d_drop() unhashes the entry from the parent
- * dentry hashes, so that it won't be found through
- * a VFS lookup any more. Note that this is different
- * from deleting the dentry - d_delete will try to
- * mark the dentry negative if possible, giving a
- * successful _negative_ lookup, while d_drop will
- * just make the cache lookup fail.
- *
- * d_drop() is used mainly for stuff that wants
- * to invalidate a dentry for some reason (NFS
- * timeouts or autofs deletes).
- */
-
-static __inline__ void d_drop(struct dentry * dentry)
-{
-	spin_lock(&dcache_lock);
-	list_del(&dentry->d_hash);
-	INIT_LIST_HEAD(&dentry->d_hash);
-	spin_unlock(&dcache_lock);
-}
-
 static __inline__ int dname_external(struct dentry *d)
 {
 	return d->d_name.name != d->d_iname; 
@@ -276,3 +251,34 @@ extern struct vfsmount *lookup_mnt(struc
 #endif /* __KERNEL__ */
 
 #endif	/* __LINUX_DCACHE_H */
+
+#if !defined(__LINUX_DCACHE_H_INLINES) && defined(_TASK_STRUCT_DEFINED)
+#define __LINUX_DCACHE_H_INLINES
+
+#ifdef __KERNEL__
+/**
+ * d_drop - drop a dentry
+ * @dentry: dentry to drop
+ *
+ * d_drop() unhashes the entry from the parent
+ * dentry hashes, so that it won't be found through
+ * a VFS lookup any more. Note that this is different
+ * from deleting the dentry - d_delete will try to
+ * mark the dentry negative if possible, giving a
+ * successful _negative_ lookup, while d_drop will
+ * just make the cache lookup fail.
+ *
+ * d_drop() is used mainly for stuff that wants
+ * to invalidate a dentry for some reason (NFS
+ * timeouts or autofs deletes).
+ */
+
+static __inline__ void d_drop(struct dentry * dentry)
+{
+	spin_lock(&dcache_lock);
+	list_del(&dentry->d_hash);
+	INIT_LIST_HEAD(&dentry->d_hash);
+	spin_unlock(&dcache_lock);
+}
+#endif
+#endif
diff -uNrp linux-2.4.23/include/linux/fs_struct.h linux-2.4.23-fusion/include/linux/fs_struct.h
--- linux-2.4.23/include/linux/fs_struct.h	2001-07-14 00:10:44.000000000 +0200
+++ linux-2.4.23-fusion/include/linux/fs_struct.h	2004-01-11 15:29:37.000000000 +0100
@@ -20,6 +20,15 @@ struct fs_struct {
 extern void exit_fs(struct task_struct *);
 extern void set_fs_altroot(void);
 
+struct fs_struct *copy_fs_struct(struct fs_struct *old);
+void put_fs_struct(struct fs_struct *fs);
+
+#endif
+#endif
+
+#if !defined(_LINUX_FS_STRUCT_H_INLINES) && defined(_TASK_STRUCT_DEFINED)
+#define _LINUX_FS_STRUCT_H_INLINES
+#ifdef __KERNEL__
 /*
  * Replace the fs->{rootmnt,root} with {mnt,dentry}. Put the old values.
  * It can block. Requires the big lock held.
@@ -65,9 +74,5 @@ static inline void set_fs_pwd(struct fs_
 		mntput(old_pwdmnt);
 	}
 }
-
-struct fs_struct *copy_fs_struct(struct fs_struct *old);
-void put_fs_struct(struct fs_struct *fs);
-
 #endif
 #endif
diff -uNrp linux-2.4.23/include/linux/highmem.h linux-2.4.23-fusion/include/linux/highmem.h
--- linux-2.4.23/include/linux/highmem.h	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/include/linux/highmem.h	2004-01-11 21:44:09.000000000 +0100
@@ -33,18 +33,8 @@ static inline char *bh_kmap_irq(struct b
 {
 	unsigned long addr;
 
-	__save_flags(*flags);
+	local_irq_save(*flags);
 
-	/*
-	 * could be low
-	 */
-	if (!PageHighMem(bh->b_page))
-		return bh->b_data;
-
-	/*
-	 * it's a highmem page
-	 */
-	__cli();
 	addr = (unsigned long) kmap_atomic(bh->b_page, KM_BH_IRQ);
 
 	if (addr & ~PAGE_MASK)
@@ -58,7 +48,7 @@ static inline void bh_kunmap_irq(char *b
 	unsigned long ptr = (unsigned long) buffer & PAGE_MASK;
 
 	kunmap_atomic((void *) ptr, KM_BH_IRQ);
-	__restore_flags(*flags);
+	local_irq_restore(*flags);
 }
 
 #else /* CONFIG_HIGHMEM */
diff -uNrp linux-2.4.23/include/linux/low-latency.h linux-2.4.23-fusion/include/linux/low-latency.h
--- linux-2.4.23/include/linux/low-latency.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/low-latency.h	2004-01-11 21:44:07.000000000 +0100
@@ -0,0 +1,109 @@
+/*
+ * include/linux/low-latency.h
+ *
+ * Andrew Morton <akpm@zip.com.au>
+ */
+
+#ifndef LOW_LATENCY_H_INCLUDED
+#define LOW_LATENCY_H_INCLUDED
+
+#if defined(CONFIG_LOLAT)
+#define LOWLATENCY_NEEDED	1
+#else
+#define LOWLATENCY_NEEDED	0
+#endif
+
+#if LOWLATENCY_NEEDED
+
+#include <linux/cache.h>		/* For ____cacheline_aligned */
+
+#ifdef CONFIG_LOLAT_SYSCTL
+extern struct low_latency_enable_struct {
+	int yep;
+} ____cacheline_aligned __enable_lowlatency;
+#define enable_lowlatency __enable_lowlatency.yep
+
+#else
+#define enable_lowlatency 1
+#endif
+
+/*
+ * Set this non-zero to generate low-latency instrumentation
+ */
+#define LOWLATENCY_DEBUG		0
+
+/*
+ * Set this non-zero for robustness testing
+ */
+#define LOWLATENCY_ALWAYS_SCHEDULE	0
+
+#if LOWLATENCY_DEBUG
+
+#if LOWLATENCY_ALWAYS_SCHEDULE
+#define conditional_schedule_needed() ((enable_lowlatency == 2) || (enable_lowlatency && current->need_resched))
+#else
+#define conditional_schedule_needed() (enable_lowlatency && current->need_resched)
+#endif
+
+struct lolat_stats_t {
+	unsigned long count;
+	int visited;
+	const char *file;
+	int line;
+	struct lolat_stats_t *next;
+};
+
+void set_running_and_schedule(struct lolat_stats_t *stats);
+
+#define unconditional_schedule()					\
+	do {								\
+		static struct lolat_stats_t stats = {			\
+			file: __FILE__,					\
+			line: __LINE__,					\
+		};							\
+		set_running_and_schedule(&stats);			\
+	} while (0)
+
+extern void show_lolat_stats(void);
+
+#else	/* LOWLATENCY_DEBUG */
+
+#if LOWLATENCY_ALWAYS_SCHEDULE
+#define conditional_schedule_needed() 1
+#else
+#define conditional_schedule_needed() (current->need_resched)
+#endif
+
+void set_running_and_schedule(void);
+#define unconditional_schedule() set_running_and_schedule()
+
+#endif	/* LOWLATENCY_DEBUG */
+
+#define conditional_schedule()						\
+	do {								\
+		if (conditional_schedule_needed())			\
+			unconditional_schedule();			\
+	} while (0)
+
+#define DEFINE_RESCHED_COUNT	int resched_count = 0
+#define TEST_RESCHED_COUNT(n)	(enable_lowlatency && (++resched_count > (n)))
+#define RESET_RESCHED_COUNT()	resched_count = 0
+extern int ll_copy_to_user(void *to_user, const void *from, unsigned long len);
+extern int ll_copy_from_user(void *to, const void *from_user, unsigned long len);
+
+#else	/* LOWLATENCY_NEEDED */
+
+#define conditional_schedule_needed() 0
+#define conditional_schedule()
+#define unconditional_schedule()
+
+#define DEFINE_RESCHED_COUNT
+#define TEST_RESCHED_COUNT(n)	0
+#define RESET_RESCHED_COUNT()
+#define ll_copy_to_user(to_user, from, len) copy_to_user((to_user), (from), (len))
+#define ll_copy_from_user(to, from_user, len) copy_from_user((to), (from_user), (len))
+
+#endif	/* LOWLATENCY_NEEDED */
+
+#endif /* LOW_LATENCY_H_INCLUDED */
+
diff -uNrp linux-2.4.23/include/linux/mm.h linux-2.4.23-fusion/include/linux/mm.h
--- linux-2.4.23/include/linux/mm.h	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/mm.h	2004-01-11 21:44:07.000000000 +0100
@@ -124,6 +124,8 @@ extern int vm_max_readahead;
  */
 extern pgprot_t protection_map[16];
 
+/* Actions for zap_page_range() */
+#define ZPR_COND_RESCHED	1	/* Do a conditional_schedule() occasionally */
 
 /*
  * These are the virtual MM functions - opening of an area, closing and
@@ -484,7 +486,7 @@ struct file *shmem_file_setup(char * nam
 extern void shmem_lock(struct file * file, int lock);
 extern int shmem_zero_setup(struct vm_area_struct *);
 
-extern void zap_page_range(struct mm_struct *mm, unsigned long address, unsigned long size);
+extern void zap_page_range(struct mm_struct *mm, unsigned long address, unsigned long size, int actions);
 extern int copy_page_range(struct mm_struct *dst, struct mm_struct *src, struct vm_area_struct *vma);
 extern int remap_page_range(unsigned long from, unsigned long to, unsigned long size, pgprot_t prot);
 extern int zeromap_page_range(unsigned long from, unsigned long size, pgprot_t prot);
diff -uNrp linux-2.4.23/include/linux/reiserfs_fs.h linux-2.4.23-fusion/include/linux/reiserfs_fs.h
--- linux-2.4.23/include/linux/reiserfs_fs.h	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/include/linux/reiserfs_fs.h	2004-01-11 21:54:44.000000000 +0100
@@ -1329,8 +1329,8 @@ static inline loff_t max_reiserfs_offset
 #define fs_generation(s) ((s)->u.reiserfs_sb.s_generation_counter)
 #define get_generation(s) atomic_read (&fs_generation(s))
 #define FILESYSTEM_CHANGED_TB(tb)  (get_generation((tb)->tb_sb) != (tb)->fs_gen)
-#define fs_changed(gen,s) (gen != get_generation (s))
-
+#define __fs_changed(gen,s) (gen != get_generation (s))
+#define fs_changed(gen,s) ({conditional_schedule(); __fs_changed(gen,s);})
 
 /***************************************************************************/
 /*                  FIXATE NODES                                           */
diff -uNrp linux-2.4.23/include/linux/sched.h linux-2.4.23-fusion/include/linux/sched.h
--- linux-2.4.23/include/linux/sched.h	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/sched.h	2004-01-11 21:44:07.000000000 +0100
@@ -26,6 +26,7 @@ extern unsigned long event;
 #include <linux/signal.h>
 #include <linux/securebits.h>
 #include <linux/fs_struct.h>
+#include <linux/low-latency.h>
 
 struct exec_domain;
 
@@ -91,6 +92,7 @@ extern int last_pid;
 #define TASK_UNINTERRUPTIBLE	2
 #define TASK_ZOMBIE		4
 #define TASK_STOPPED		8
+#define PREEMPT_ACTIVE		0x4000000
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
@@ -125,6 +127,10 @@ struct completion;
 
 #include <linux/spinlock.h>
 
+#ifdef CONFIG_ADEOS_CORE
+#include <linux/adeos.h>
+#endif /* CONFIG_ADEOS_CORE */
+
 /*
  * This serializes "schedule()" and also protects
  * the run-queue from deletions/modifications (but
@@ -147,6 +153,9 @@ extern void update_one_process(struct ta
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long FASTCALL(schedule_timeout(signed long timeout));
 asmlinkage void schedule(void);
+#ifdef CONFIG_PREEMPT
+asmlinkage void preempt_schedule(void);
+#endif
 
 extern int schedule_task(struct tq_struct *task);
 extern void flush_scheduled_tasks(void);
@@ -285,7 +294,7 @@ struct task_struct {
 	 * offsets of these are hardcoded elsewhere - touch with care
 	 */
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
-	unsigned long flags;	/* per process flags, defined below */
+	int preempt_count;	/* 0 => preemptable, <0 => BUG */
 	int sigpending;
 	mm_segment_t addr_limit;	/* thread address space:
 					 	0-0xBFFFFFFF for user-thead
@@ -327,6 +336,7 @@ struct task_struct {
 	struct mm_struct *active_mm;
 	struct list_head local_pages;
 	unsigned int allocation_order, nr_local_pages;
+	unsigned long flags;
 
 /* task state */
 	struct linux_binfmt *binfmt;
@@ -415,6 +425,10 @@ struct task_struct {
 
 /* journalling filesystem info */
 	void *journal_info;
+
+#ifdef CONFIG_ADEOS_CORE
+        void *ptd[ADEOS_ROOT_NPTDKEYS];
+#endif /* CONFIG_ADEOS_CORE */
 };
 
 /*
@@ -467,6 +481,7 @@ extern struct exec_domain	default_exec_d
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
  */
+#ifdef CONFIG_ADEOS_CORE
 #define INIT_TASK(tsk)	\
 {									\
     state:		0,						\
@@ -508,7 +523,52 @@ extern struct exec_domain	default_exec_d
     blocked:		{{0}},						\
     alloc_lock:		SPIN_LOCK_UNLOCKED,				\
     journal_info:	NULL,						\
+    ptd:                { [ 0 ... ADEOS_ROOT_NPTDKEYS - 1] = 0 },       \
 }
+#else /* !CONFIG_ADEOS_CORE */
+#define INIT_TASK(tsk)	\
+{									\
+    state:		0,						\
+    flags:		0,						\
+    sigpending:		0,						\
+    addr_limit:		KERNEL_DS,					\
+    exec_domain:	&default_exec_domain,				\
+    lock_depth:		-1,						\
+    counter:		DEF_COUNTER,					\
+    nice:		DEF_NICE,					\
+    policy:		SCHED_OTHER,					\
+    mm:			NULL,						\
+    active_mm:		&init_mm,					\
+    cpus_runnable:	~0UL,						\
+    cpus_allowed:	~0UL,						\
+    run_list:		LIST_HEAD_INIT(tsk.run_list),			\
+    next_task:		&tsk,						\
+    prev_task:		&tsk,						\
+    p_opptr:		&tsk,						\
+    p_pptr:		&tsk,						\
+    thread_group:	LIST_HEAD_INIT(tsk.thread_group),		\
+    wait_chldexit:	__WAIT_QUEUE_HEAD_INITIALIZER(tsk.wait_chldexit),\
+    real_timer:		{						\
+	function:		it_real_fn				\
+    },									\
+    cap_effective:	CAP_INIT_EFF_SET,				\
+    cap_inheritable:	CAP_INIT_INH_SET,				\
+    cap_permitted:	CAP_FULL_SET,					\
+    keep_capabilities:	0,						\
+    rlim:		INIT_RLIMITS,					\
+    user:		INIT_USER,					\
+    comm:		"swapper",					\
+    thread:		INIT_THREAD,					\
+    fs:			&init_fs,					\
+    files:		&init_files,					\
+    sigmask_lock:	SPIN_LOCK_UNLOCKED,				\
+    sig:		&init_signals,					\
+    pending:		{ NULL, &tsk.pending.head, {{0}}},		\
+    blocked:		{{0}},						\
+    alloc_lock:		SPIN_LOCK_UNLOCKED,				\
+    journal_info:	NULL,						\
+}
+#endif /* CONFIG_ADEOS_CORE */
 
 
 #ifndef INIT_TASK_SIZE
@@ -956,5 +1016,10 @@ static inline void cond_resched(void)
 		__cond_resched();
 }
 
+#define _TASK_STRUCT_DEFINED
+#include <linux/dcache.h>
+#include <linux/tqueue.h>
+#include <linux/fs_struct.h>
+
 #endif /* __KERNEL__ */
 #endif
diff -uNrp linux-2.4.23/include/linux/smp_lock.h linux-2.4.23-fusion/include/linux/smp_lock.h
--- linux-2.4.23/include/linux/smp_lock.h	2001-11-22 20:46:27.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/smp_lock.h	2004-01-11 21:44:07.000000000 +0100
@@ -3,7 +3,7 @@
 
 #include <linux/config.h>
 
-#ifndef CONFIG_SMP
+#if !defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT)
 
 #define lock_kernel()				do { } while(0)
 #define unlock_kernel()				do { } while(0)
diff -uNrp linux-2.4.23/include/linux/spinlock.h linux-2.4.23-fusion/include/linux/spinlock.h
--- linux-2.4.23/include/linux/spinlock.h	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/spinlock.h	2004-01-11 21:44:06.000000000 +0100
@@ -2,6 +2,7 @@
 #define __LINUX_SPINLOCK_H
 
 #include <linux/config.h>
+#include <linux/compiler.h>
 
 /*
  * These are the generic versions of the spinlocks and read-write
@@ -62,8 +63,10 @@
 
 #if (DEBUG_SPINLOCKS < 1)
 
+#ifndef CONFIG_PREEMPT
 #define atomic_dec_and_lock(atomic,lock) atomic_dec_and_test(atomic)
 #define ATOMIC_DEC_AND_LOCK
+#endif
 
 /*
  * Your basic spinlocks, allowing only a single CPU anywhere
@@ -80,11 +83,11 @@
 #endif
 
 #define spin_lock_init(lock)	do { } while(0)
-#define spin_lock(lock)		(void)(lock) /* Not "unused variable". */
+#define _raw_spin_lock(lock)	(void)(lock) /* Not "unused variable". */
 #define spin_is_locked(lock)	(0)
-#define spin_trylock(lock)	({1; })
+#define _raw_spin_trylock(lock)	({1; })
 #define spin_unlock_wait(lock)	do { } while(0)
-#define spin_unlock(lock)	do { } while(0)
+#define _raw_spin_unlock(lock)	do { } while(0)
 
 #elif (DEBUG_SPINLOCKS < 2)
 
@@ -144,13 +147,113 @@ typedef struct {
 #endif
 
 #define rwlock_init(lock)	do { } while(0)
-#define read_lock(lock)		(void)(lock) /* Not "unused variable". */
-#define read_unlock(lock)	do { } while(0)
-#define write_lock(lock)	(void)(lock) /* Not "unused variable". */
-#define write_unlock(lock)	do { } while(0)
+#define _raw_read_lock(lock)	(void)(lock) /* Not "unused variable". */
+#define _raw_read_unlock(lock)	do { } while(0)
+#define _raw_write_lock(lock)	(void)(lock) /* Not "unused variable". */
+#define _raw_write_unlock(lock)	do { } while(0)
 
 #endif /* !SMP */
 
+#ifdef CONFIG_PREEMPT
+
+#ifdef CONFIG_ADEOS_CORE
+
+#define preempt_get_count()	(adp_current == adp_root ? current->preempt_count : 0)
+#define preempt_is_disabled()	(adp_current == adp_root ? preempt_get_count() != 0 : 1)
+
+#define preempt_disable() \
+do { \
+	if (adp_current == adp_root) { \
+	    ++current->preempt_count; \
+	    barrier(); \
+        } \
+} while (0)
+
+#define preempt_enable_no_resched() \
+do { \
+	if (adp_current == adp_root) { \
+	    --current->preempt_count; \
+	    barrier(); \
+        } \
+} while (0)
+
+#define preempt_enable() \
+do { \
+	if (adp_current == adp_root) { \
+	    --current->preempt_count; \
+	    barrier(); \
+	    if (unlikely(current->preempt_count < current->need_resched)) \
+		preempt_schedule(); \
+        } \
+} while (0)
+
+#else /* !CONFIG_ADEOS_CORE */
+
+#define preempt_get_count()	(current->preempt_count)
+#define preempt_is_disabled()	(preempt_get_count() != 0)
+
+#define preempt_disable() \
+do { \
+	++current->preempt_count; \
+	barrier(); \
+} while (0)
+
+#define preempt_enable_no_resched() \
+do { \
+	--current->preempt_count; \
+	barrier(); \
+} while (0)
+
+#define preempt_enable() \
+do { \
+	--current->preempt_count; \
+	barrier(); \
+	if (unlikely(current->preempt_count < current->need_resched)) \
+		preempt_schedule(); \
+} while (0)
+
+#endif /* CONFIG_ADEOS_CORE */
+
+#define spin_lock(lock)	\
+do { \
+	preempt_disable(); \
+	_raw_spin_lock(lock); \
+} while(0)
+
+#define spin_trylock(lock)	({preempt_disable(); _raw_spin_trylock(lock) ? \
+				1 : ({preempt_enable(); 0;});})
+#define spin_unlock(lock) \
+do { \
+	_raw_spin_unlock(lock); \
+	preempt_enable(); \
+} while (0)
+
+#define read_lock(lock)		({preempt_disable(); _raw_read_lock(lock);})
+#define read_unlock(lock)	({_raw_read_unlock(lock); preempt_enable();})
+#define write_lock(lock)	({preempt_disable(); _raw_write_lock(lock);})
+#define write_unlock(lock)	({_raw_write_unlock(lock); preempt_enable();})
+#define write_trylock(lock)	({preempt_disable();_raw_write_trylock(lock) ? \
+				1 : ({preempt_enable(); 0;});})
+
+#else
+
+#define preempt_get_count()	(0)
+#define preempt_is_disabled()	(1)
+#define preempt_disable()	do { } while (0)
+#define preempt_enable_no_resched()	do {} while(0)
+#define preempt_enable()	do { } while (0)
+
+#define spin_lock(lock)		_raw_spin_lock(lock)
+#define spin_trylock(lock)	_raw_spin_trylock(lock)
+#define spin_unlock(lock)	_raw_spin_unlock(lock)
+
+#define read_lock(lock)		_raw_read_lock(lock)
+#define read_unlock(lock)	_raw_read_unlock(lock)
+#define write_lock(lock)	_raw_write_lock(lock)
+#define write_unlock(lock)	_raw_write_unlock(lock)
+#define write_trylock(lock)	_raw_write_trylock(lock)
+#endif
+
 /* "lock on reference count zero" */
 #ifndef ATOMIC_DEC_AND_LOCK
 #include <asm/atomic.h>
diff -uNrp linux-2.4.23/include/linux/sysctl.h linux-2.4.23-fusion/include/linux/sysctl.h
--- linux-2.4.23/include/linux/sysctl.h	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/sysctl.h	2004-01-11 21:44:12.000000000 +0100
@@ -124,6 +124,7 @@ enum
 	KERN_CORE_USES_PID=52,		/* int: use core or core.%pid */
 	KERN_TAINTED=53,	/* int: various kernel tainted flags */
 	KERN_CADPID=54,		/* int: PID of the process to notify on CAD */
+	KERN_LOWLATENCY=55,     /* int: enable low latency scheduling */
  	KERN_CORE_PATTERN=56,	/* string: pattern for core-files */
 	KERN_PPC_L3CR=57,       /* l3cr register on PPC */
 	KERN_EXCEPTION_TRACE=58, /* boolean: exception trace */
diff -uNrp linux-2.4.23/include/linux/tqueue.h linux-2.4.23-fusion/include/linux/tqueue.h
--- linux-2.4.23/include/linux/tqueue.h	2001-11-22 20:46:19.000000000 +0100
+++ linux-2.4.23-fusion/include/linux/tqueue.h	2004-01-11 21:44:06.000000000 +0100
@@ -94,6 +94,22 @@ extern task_queue tq_timer, tq_immediate
 extern spinlock_t tqueue_lock;
 
 /*
+ * Call all "bottom halfs" on a given list.
+ */
+
+extern void __run_task_queue(task_queue *list);
+
+static inline void run_task_queue(task_queue *list)
+{
+	if (TQ_ACTIVE(*list))
+		__run_task_queue(list);
+}
+
+#endif /* _LINUX_TQUEUE_H */
+
+#if !defined(_LINUX_TQUEUE_H_INLINES) && defined(_TASK_STRUCT_DEFINED)
+#define _LINUX_TQUEUE_H_INLINES
+/*
  * Queue a task on a tq.  Return non-zero if it was successfully
  * added.
  */
@@ -109,17 +125,4 @@ static inline int queue_task(struct tq_s
 	}
 	return ret;
 }
-
-/*
- * Call all "bottom halfs" on a given list.
- */
-
-extern void __run_task_queue(task_queue *list);
-
-static inline void run_task_queue(task_queue *list)
-{
-	if (TQ_ACTIVE(*list))
-		__run_task_queue(list);
-}
-
-#endif /* _LINUX_TQUEUE_H */
+#endif
diff -uNrp linux-2.4.23/init/main.c linux-2.4.23-fusion/init/main.c
--- linux-2.4.23/init/main.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/init/main.c	2004-01-11 15:29:37.000000000 +0100
@@ -365,6 +365,9 @@ asmlinkage void __init start_kernel(void
 	parse_options(command_line);
 	trap_init();
 	init_IRQ();
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_init();
+#endif /* CONFIG_ADEOS_CORE */
 	sched_init();
 	softirq_init();
 	time_init();
@@ -434,6 +437,9 @@ asmlinkage void __init start_kernel(void
 	 *	make syscalls (and thus be locked).
 	 */
 	smp_init();
+#if defined(CONFIG_ADEOS) && defined(CONFIG_X86_IO_APIC)
+ 	__adeos_takeover();
+#endif /* CONFIG_ADEOS && CONFIG_X86_IO_APIC */
 #if defined(CONFIG_SYSVIPC)
 	ipc_init();
 #endif
diff -uNrp linux-2.4.23/kernel/Makefile linux-2.4.23-fusion/kernel/Makefile
--- linux-2.4.23/kernel/Makefile	2001-09-17 06:22:40.000000000 +0200
+++ linux-2.4.23-fusion/kernel/Makefile	2004-01-11 15:29:37.000000000 +0100
@@ -9,7 +9,7 @@
 
 O_TARGET := kernel.o
 
-export-objs = signal.o sys.o kmod.o context.o ksyms.o pm.o exec_domain.o printk.o
+export-objs = adeos.o signal.o sys.o kmod.o context.o ksyms.o pm.o exec_domain.o printk.o
 
 obj-y     = sched.o dma.o fork.o exec_domain.o panic.o printk.o \
 	    module.o exit.o itimer.o info.o time.o softirq.o resource.o \
@@ -19,6 +19,7 @@ obj-y     = sched.o dma.o fork.o exec_do
 obj-$(CONFIG_UID16) += uid16.o
 obj-$(CONFIG_MODULES) += ksyms.o
 obj-$(CONFIG_PM) += pm.o
+obj-$(CONFIG_ADEOS_CORE) += adeos.o
 
 ifneq ($(CONFIG_IA64),y)
 # According to Alan Modra <alan@linuxcare.com.au>, the -fno-omit-frame-pointer is
diff -uNrp linux-2.4.23/kernel/adeos.c linux-2.4.23-fusion/kernel/adeos.c
--- linux-2.4.23/kernel/adeos.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.4.23-fusion/kernel/adeos.c	2004-01-11 15:29:37.000000000 +0100
@@ -0,0 +1,501 @@
+/*
+ *   linux/kernel/adeos.c
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-independent ADEOS core support.
+ */
+
+#include <linux/sched.h>
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#endif /* CONFIG_PROC_FS */
+
+/* The pre-defined domain slot for the root domain. */
+static adomain_t adeos_root_domain;
+
+/* A constant pointer to the root domain. */
+adomain_t *adp_root = &adeos_root_domain;
+
+/* A pointer to the current domain. */
+adomain_t *adp_cpu_current[ADEOS_NR_CPUS] = { [ 0 ... ADEOS_NR_CPUS - 1] = &adeos_root_domain };
+
+/* The spinlock protecting from races while modifying the pipeline. */
+spinlock_t __adeos_pipelock = SPIN_LOCK_UNLOCKED;
+
+/* The pipeline data structure. Enqueues adomain_t objects by priority. */
+struct list_head __adeos_pipeline;
+
+/* An array of global counters tracking domains monitoring events. */
+int __adeos_event_monitors[ADEOS_NR_EVENTS] = { [ 0 ... ADEOS_NR_EVENTS - 1] = 0 };
+
+/* The allocated VIRQ map. */
+unsigned long __adeos_virtual_irq_map = 0;
+
+static void __adeos_set_root_ptd (int key, void *value) {
+
+    current->ptd[key] = value;
+}
+
+static void *__adeos_get_root_ptd (int key) {
+
+    return current->ptd[key];
+}
+
+/* adeos_init() -- Initialization routine of the ADEOS layer. Called
+   by the host kernel early during the boot procedure. */
+
+void __adeos_init (void)
+
+{
+    adomain_t *adp = &adeos_root_domain;
+
+    __adeos_check_machine();	/* Do platform dependent checks first. */
+
+    /*
+      A lightweight registration code for the root domain. Current
+      assumptions are:
+      - We are running on the boot CPU, and secondary CPUs are still
+      lost in space.
+      - adeos_root_domain has been zero'ed.
+    */
+
+    INIT_LIST_HEAD(&__adeos_pipeline);
+
+    adp->name = "Linux";
+    adp->domid = ADEOS_ROOT_ID;
+    adp->priority = ADEOS_ROOT_PRI;
+    adp->ptd_setfun = &__adeos_set_root_ptd;
+    adp->ptd_getfun = &__adeos_get_root_ptd;
+    adp->ptd_keymax = ADEOS_ROOT_NPTDKEYS;
+
+    __adeos_init_stage(adp);
+
+    INIT_LIST_HEAD(&adp->p_link);
+    list_add_tail(&adp->p_link,&__adeos_pipeline);
+
+    printk("ADEOS %s: Root domain %s registered.\n",
+	   ADEOS_VERSION_STRING,
+	   adp->name);
+
+#if defined(CONFIG_ADEOS) && !defined(CONFIG_X86_IO_APIC)
+    __adeos_takeover();
+#endif /* CONFIG_ADEOS && !CONFIG_X86_IO_APIC */
+}
+
+/* adeos_handle_event() -- Adeos' generic event handler. This routine
+calls the per-domain handlers registered for a given
+exception/event. Each domain before the one which raised the event in
+the pipeline will get a chance to process the event. The latter will
+eventually be allowed to process its own event too if a valid handler
+exists for it.  Handler executions are always scheduled by the domain
+which raised the event for the prioritary domains wanting to be
+notified of such event.  Note: evdata might be NULL. */
+
+int __adeos_handle_event (unsigned event, void *evdata)
+
+{
+    struct list_head *pos, *npos;
+    unsigned long flags;
+    adeos_declare_cpuid;
+    adevinfo_t evinfo;
+    int propagate = 1;
+
+    adeos_hw_local_irq_save(flags);
+
+    list_for_each_safe(pos,npos,&__adeos_pipeline) {
+
+    	adomain_t *adp = list_entry(pos,adomain_t,p_link);
+
+	if (adp->events[event].handler != NULL)
+	    {
+	    if (adp == adp_cpu_current[cpuid])
+		{
+		evinfo.domid = adp->domid;
+		evinfo.event = event;
+		evinfo.evdata = evdata;
+		evinfo.propagate = 0;
+		adp->events[event].handler(&evinfo);
+		propagate = evinfo.propagate;
+		break;
+		}
+
+	    adp->cpudata[cpuid].event_info.domid = adp_cpu_current[cpuid]->domid;
+	    adp->cpudata[cpuid].event_info.event = event;
+	    adp->cpudata[cpuid].event_info.evdata = evdata;
+	    adp->cpudata[cpuid].event_info.propagate = 0;
+	    set_bit(IPIPE_XPEND_FLAG,&adp->cpudata[cpuid].status);
+
+	    /* Let the prioritary domain process the event. */
+
+	    __adeos_switch_to(adp,cpuid);
+
+#ifdef CONFIG_SMP
+	    adeos_reload_cpuid();	/* Processor might have changed. */
+#endif /* CONFIG_SMP */
+
+	    if (!adp->cpudata[cpuid].event_info.propagate)
+		{
+		propagate = 0;
+		break;
+		}
+	    }
+
+	if (adp == adp_cpu_current[cpuid])
+	    break;
+    }
+
+    adeos_hw_local_irq_restore(flags);
+
+    if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+	adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+	__adeos_sync_stage();
+
+    return !propagate;
+}
+
+void __adeos_stall_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+	set_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	}
+    else
+	adeos_hw_cli();
+}
+
+void __adeos_unstall_root (void)
+
+{
+    adeos_hw_sti();	/* Absolutely needed. */
+
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+
+	clear_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+
+	if (adp_root->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+	}
+}
+
+unsigned long __adeos_test_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+	return test_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	}
+
+    return adeos_hw_irqs_disabled();
+}
+
+unsigned long __adeos_test_and_stall_root (void)
+
+{
+    unsigned long flags;
+
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+	return test_and_set_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	}
+
+    adeos_hw_local_irq_save(flags);
+
+    return !adeos_hw_test_iflag(flags);
+}
+
+void __adeos_restore_root (unsigned long flags)
+
+{
+    if (likely(adp_pipelined))
+	{
+	adeos_declare_cpuid;
+
+	if (flags)
+	    set_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+	else
+	    {
+	    clear_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status);
+
+	    if (adp_root->cpudata[cpuid].irq_pending_hi != 0)
+		__adeos_sync_stage();
+	    }
+	}
+    else if (flags)
+	adeos_hw_cli();
+    else
+	adeos_hw_sti();
+}
+
+/* adeos_suspend_domain() -- tell the ADEOS layer that the current
+   domain is now dormant. The calling domain is switched out, while
+   the next domain with work in progress or pending in the pipeline is
+   switched in. */
+
+void adeos_suspend_domain (void)
+
+{
+    struct adcpudata *cpudata;
+    adomain_t *adp, *nadp;
+    struct list_head *ln;
+    unsigned long flags;
+    adeos_declare_cpuid;
+
+    adp = nadp = adp_cpu_current[cpuid];
+    cpudata = &adp->cpudata[cpuid];
+
+    adeos_hw_local_irq_save(flags);
+
+    for (;;)
+	{
+	ln = nadp->p_link.next;
+
+	if (ln == &__adeos_pipeline)	/* End of pipeline reached? */
+	    {
+	    /* Caller should loop on its idle task on return. */
+	    adeos_hw_local_irq_restore(flags);
+	    return;
+	    }
+
+	nadp = list_entry(ln,adomain_t,p_link);
+
+	/* Make sure the domain was preempted (i.e. not sleeping) or
+	   has some event to process before switching to it. */
+
+	if (!test_bit(IPIPE_SLEEP_FLAG,&nadp->cpudata[cpuid].status) ||
+	    (!test_bit(IPIPE_STALL_FLAG,&nadp->cpudata[cpuid].status) &&
+	     nadp->cpudata[cpuid].irq_pending_hi != 0) ||
+	    test_bit(IPIPE_XPEND_FLAG,&nadp->cpudata[cpuid].status))
+	    break;
+	}
+
+    /* A suspending domain implicitely unstalls the pipeline. */
+    clear_bit(IPIPE_STALL_FLAG,&cpudata->status);
+
+    /* Mark the outgoing domain as aslept (i.e. not preempted). */
+    set_bit(IPIPE_SLEEP_FLAG,&cpudata->status);
+
+    /* Suspend the calling domain, switching to the next one. */
+    __adeos_switch_to(nadp,cpuid);
+
+    /* Clear the sleep bit for the incoming domain (on the initial processor). */
+    clear_bit(IPIPE_SLEEP_FLAG,&cpudata->status);
+
+#ifdef CONFIG_SMP
+    adeos_reload_cpuid();	/* Processor might have changed. */
+    cpudata = &adp->cpudata[cpuid];
+#endif /* CONFIG_SMP */
+
+    /* Now, we are back into the calling domain. Flush the interrupt
+       log and fire the event interposition handler if needed.  Note
+       that a domain might be switched in because it was preempted or
+       has an event to process, even with a stalled stage. However, we
+       still ensure that interrupts won't be dispatched in the latter
+       case. */
+
+    if (!test_bit(IPIPE_STALL_FLAG,&cpudata->status) &&
+	cpudata->irq_pending_hi != 0)
+	{
+	__adeos_sync_stage();
+#ifdef CONFIG_SMP
+	adeos_reload_cpuid();	/* Processor might have changed. */
+	cpudata = &adp->cpudata[cpuid];
+#endif /* CONFIG_SMP */
+	}
+
+    /* Caution: CPU migration is allowed in SMP-mode on behalf of an
+       event handler provided that the current domain raised
+       it. Otherwise, it's not. */
+
+    if (test_and_clear_bit(IPIPE_XPEND_FLAG,&cpudata->status))
+	adp->events[cpudata->event_info.event].handler(&cpudata->event_info);
+
+    adeos_hw_local_irq_restore(flags);
+
+    /* Return to the point of suspension in the calling domain. */
+}
+
+#ifdef CONFIG_PROC_FS
+
+#include <linux/proc_fs.h>
+
+static struct proc_dir_entry *adeos_proc_entry;
+
+static int __adeos_read_proc (char *page,
+			      char **start,
+			      off_t off,
+			      int count,
+			      int *eof,
+			      void *data)
+{
+    unsigned long flags, ctlbits;
+    struct list_head *pos;
+    unsigned irq, _irq;
+    char *p = page;
+    int len;
+
+    adeos_hw_local_irq_save(flags);
+
+    list_for_each(pos,&__adeos_pipeline) {
+
+    	adomain_t *adp = list_entry(pos,adomain_t,p_link);
+
+	p += sprintf(p,"%8s: priority=%d, id=0x%.8x, ptdkeys=%d/%d\n",
+		     adp->name,
+		     adp->priority,
+		     adp->domid,
+		     adp->ptd_keycount,
+		     adp->ptd_keymax);
+	irq = 0;
+
+	while (irq < IPIPE_NR_IRQS)
+	    {
+	    ctlbits = (adp->irqs[irq].control & (IPIPE_HANDLE_MASK|IPIPE_PASS_MASK|IPIPE_STICKY_MASK));
+
+	    if (adeos_virtual_irq_p(irq) && !test_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map))
+		{
+		irq++;
+		continue;
+		}
+
+	    /* Attempt to group consecutive IRQ numbers having the
+	       same virtualization settings in a single line. */
+
+	    _irq = irq;
+
+	    while (++_irq < IPIPE_NR_IRQS)
+		{
+		if (adeos_virtual_irq_p(_irq) != adeos_virtual_irq_p(irq) ||
+		    (adeos_virtual_irq_p(_irq) && !test_bit(_irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map)) ||
+		    ctlbits != (adp->irqs[_irq].control & (IPIPE_HANDLE_MASK|IPIPE_PASS_MASK|IPIPE_STICKY_MASK)))
+		    break;
+		}
+
+	    if (_irq == irq + 1)
+		p += sprintf(p,"\tirq%u: ",irq);
+	    else
+		p += sprintf(p,"\tirq%u-%u: ",irq,_irq - 1);
+
+	    /* Statuses are as follows:
+	       o "accepted" means handled _and_ passed down the
+	       pipeline.
+	       o "grabbed" means handled, but the interrupt might be
+	       terminated _or_ passed down the pipeline depending on
+	       what the domain handler asks for to Adeos.
+	       o "passed" means unhandled by the domain but passed
+	       down the pipeline.
+	       o "discarded" means unhandled and _not_ passed down the
+	       pipeline. The interrupt merely disappears from the
+	       current domain down to the end of the pipeline. */
+
+	    if (ctlbits & IPIPE_HANDLE_MASK)
+		{
+		if (ctlbits & IPIPE_PASS_MASK)
+		    p += sprintf(p,"accepted");
+		else
+		    p += sprintf(p,"grabbed");
+		}
+	    else if (ctlbits & IPIPE_PASS_MASK)
+		p += sprintf(p,"passed");
+	    else
+		p += sprintf(p,"discarded");
+
+	    if (ctlbits & IPIPE_STICKY_MASK)
+		p += sprintf(p,", sticky");
+
+	    if (adeos_virtual_irq_p(irq))
+		p += sprintf(p,", virtual");
+
+	    p += sprintf(p,"\n");
+
+	    irq = _irq;
+	    }
+    }
+
+    adeos_hw_local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_MODULE
+    p += sprintf(p,"\nInterrupt pipelining: %s\n",adp_pipelined ? "active" : "stopped");
+#else /* !CONFIG_ADEOS_MODULE */
+    p += sprintf(p,"\nInterrupt pipelining: permanent\n");
+#endif /* CONFIG_ADEOS_MODULE */
+
+    len = p - page;
+
+    if (len <= off + count)
+	*eof = 1;
+
+    *start = page + off;
+
+    len -= off;
+
+    if (len > count)
+	len = count;
+
+    if (len < 0)
+	len = 0;
+
+    return len;
+}
+
+void __adeos_init_proc (void) {
+
+    adeos_proc_entry = create_proc_read_entry("adeos",
+					      0444,
+					      NULL,
+					      &__adeos_read_proc,
+					      NULL);
+}
+
+#endif /* CONFIG_PROC_FS */
+
+void __adeos_dump_state (void)
+
+{
+    struct list_head *pos;
+    unsigned long flags;
+    int cpuid;
+
+    printk("ADEOS STATE: current domain=%s on CPU #%d [stackbase=%p]\n",
+	   adp_current->name,
+	   adeos_processor_id(),
+	   (void *)adp_current->estackbase[adeos_processor_id()]);
+
+    adeos_hw_local_irq_save(flags);
+
+    list_for_each(pos,&__adeos_pipeline) {
+
+        adomain_t *adp = list_entry(pos,adomain_t,p_link);
+
+        for (cpuid = 0; cpuid < smp_num_cpus; cpuid++)
+            printk("%8s[cpuid=%d]: priority=%d, status=0x%lx, pending_hi=0x%lx\n",
+                   adp->name,
+                   cpuid,
+                   adp->priority,
+                   adp->cpudata[cpuid].status,
+                   adp->cpudata[cpuid].irq_pending_hi);
+    }
+
+    adeos_hw_local_irq_restore(flags);
+}
diff -uNrp linux-2.4.23/kernel/exit.c linux-2.4.23-fusion/kernel/exit.c
--- linux-2.4.23/kernel/exit.c	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/kernel/exit.c	2004-01-11 15:29:37.000000000 +0100
@@ -196,6 +196,7 @@ static inline void close_files(struct fi
 			}
 			i++;
 			set >>= 1;
+			conditional_schedule();		/* sys_exit, many files open */
 		}
 	}
 }
@@ -282,7 +283,9 @@ struct mm_struct * start_lazy_tlb(void)
 	current->mm = NULL;
 	/* active_mm is still 'mm' */
 	atomic_inc(&mm->mm_count);
+	preempt_disable();
 	enter_lazy_tlb(mm, current, smp_processor_id());
+	preempt_enable();
 	return mm;
 }
 
@@ -313,8 +316,8 @@ static inline void __exit_mm(struct task
 		/* more a memory barrier than a real lock */
 		task_lock(tsk);
 		tsk->mm = NULL;
-		task_unlock(tsk);
 		enter_lazy_tlb(mm, current, smp_processor_id());
+		task_unlock(tsk);
 		mmput(mm);
 	}
 }
@@ -435,10 +438,18 @@ NORET_TYPE void do_exit(long code)
 	tsk->flags |= PF_EXITING;
 	del_timer_sync(&tsk->real_timer);
 
+	if (unlikely(preempt_get_count()))
+		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
+				current->comm, current->pid,
+				preempt_get_count());
+
 fake_volatile:
 #ifdef CONFIG_BSD_PROCESS_ACCT
 	acct_process(code);
 #endif
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_exit_process(tsk);
+#endif /* CONFIG_ADEOS_CORE */
 	__exit_mm(tsk);
 
 	lock_kernel();
diff -uNrp linux-2.4.23/kernel/fork.c linux-2.4.23-fusion/kernel/fork.c
--- linux-2.4.23/kernel/fork.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/fork.c	2004-01-11 15:29:37.000000000 +0100
@@ -233,7 +233,13 @@ static struct mm_struct * mm_init(struct
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	mm->page_table_lock = SPIN_LOCK_UNLOCKED;
+#ifdef CONFIG_ADEOS_CORE
+	adeos_spin_lock(&init_mm.page_table_lock);
+#endif /* CONFIG_ADEOS_CORE */
 	mm->pgd = pgd_alloc(mm);
+#ifdef CONFIG_ADEOS_CORE
+	adeos_spin_unlock(&init_mm.page_table_lock);
+#endif /* CONFIG_ADEOS_CORE */
 	mm->def_flags = 0;
 	if (mm->pgd)
 		return mm;
@@ -265,8 +271,15 @@ struct mm_struct * mm_alloc(void)
 inline void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
-	pgd_free(mm->pgd);
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_sti();
+	adeos_spin_lock(&init_mm.page_table_lock);
+#endif /* CONFIG_ADEOS_CORE */
+  	pgd_free(mm->pgd);
 	check_pgt_cache();
+#ifdef CONFIG_ADEOS_CORE
+	adeos_spin_unlock(&init_mm.page_table_lock);
+#endif /* CONFIG_ADEOS_CORE */
 	destroy_context(mm);
 	free_mm(mm);
 }
@@ -688,6 +701,13 @@ int do_fork(unsigned long clone_flags, u
 	if (p->binfmt && p->binfmt->module)
 		__MOD_INC_USE_COUNT(p->binfmt->module);
 
+#ifdef CONFIG_PREEMPT
+	/*
+	 * Continue with preemption disabled as part of the context
+	 * switch, so start with preempt_count set to 1.
+	 */
+	p->preempt_count = 1;
+#endif
 	p->did_exec = 0;
 	p->swappable = 0;
 	p->state = TASK_UNINTERRUPTIBLE;
@@ -808,6 +828,15 @@ int do_fork(unsigned long clone_flags, u
 	nr_threads++;
 	write_unlock_irq(&tasklist_lock);
 
+#ifdef CONFIG_ADEOS_CORE
+	{
+	int k;
+
+	for (k = 0; k < ADEOS_ROOT_NPTDKEYS; k++)
+	    p->ptd[k] = NULL;
+	}
+#endif /* CONFIG_ADEOS_CORE */
+
 	if (p->ptrace & PT_PTRACED)
 		send_sig(SIGSTOP, p, 1);
 
diff -uNrp linux-2.4.23/kernel/ksyms.c linux-2.4.23-fusion/kernel/ksyms.c
--- linux-2.4.23/kernel/ksyms.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/ksyms.c	2004-01-11 15:29:37.000000000 +0100
@@ -59,6 +59,49 @@
 #include <linux/kmod.h>
 #endif
 
+#ifdef CONFIG_ADEOS_CORE
+EXPORT_SYMBOL(adeos_suspend_domain);
+#if defined(CONFIG_ADEOS_MODULE) || defined(CONFIG_X86_IO_APIC)
+EXPORT_SYMBOL(adp_pipelined);
+#endif /* CONFIG_ADEOS_MODULE || CONFIG_X86_IO_APIC */
+EXPORT_SYMBOL(adp_cpu_current);
+EXPORT_SYMBOL(adp_root);
+EXPORT_SYMBOL(__adeos_init_stage);
+EXPORT_SYMBOL(__adeos_handle_event);
+EXPORT_SYMBOL(__adeos_unstall_root);
+EXPORT_SYMBOL(__adeos_stall_root);
+EXPORT_SYMBOL(__adeos_restore_root);
+EXPORT_SYMBOL(__adeos_test_and_stall_root);
+EXPORT_SYMBOL(__adeos_test_root);
+EXPORT_SYMBOL(__adeos_dump_state);
+extern struct list_head __adeos_pipeline;
+EXPORT_SYMBOL(__adeos_pipeline);
+extern spinlock_t __adeos_pipelock;
+EXPORT_SYMBOL(__adeos_pipelock);
+extern unsigned long __adeos_virtual_irq_map;
+EXPORT_SYMBOL(__adeos_virtual_irq_map);
+EXPORT_SYMBOL(__adeos_event_monitors);
+/* The following are convenience exports which are needed by some
+   Adeos domains loaded as kernel modules. */
+EXPORT_SYMBOL_NOVERS(__mmdrop);
+EXPORT_SYMBOL_NOVERS(do_fork);
+EXPORT_SYMBOL_NOVERS(do_exit);
+ssize_t sys_read(unsigned fd,char *buf,size_t count);
+EXPORT_SYMBOL_NOVERS(sys_read);
+ssize_t sys_write(unsigned fd,const char *buf,size_t count);
+EXPORT_SYMBOL_NOVERS(sys_write);
+ssize_t sys_pread(unsigned fd,char *buf,size_t count,loff_t pos);
+EXPORT_SYMBOL_NOVERS(sys_pread);
+ssize_t sys_pwrite(unsigned fd,const char *buf,size_t count,loff_t pos);
+EXPORT_SYMBOL_NOVERS(sys_pwrite);
+long sys_fsync(unsigned fd);
+EXPORT_SYMBOL_NOVERS(sys_fsync);
+long sys_fdatasync(unsigned fd);
+EXPORT_SYMBOL_NOVERS(sys_fdatasync);
+long sys_open(const char *filename,int flags,int mode);
+EXPORT_SYMBOL_NOVERS(sys_open);
+#endif /* CONFIG_ADEOS_CORE */
+
 extern void set_device_ro(kdev_t dev,int flag);
 
 extern void *sys_call_table;
@@ -458,6 +501,9 @@ EXPORT_SYMBOL(sleep_on_timeout);
 EXPORT_SYMBOL(interruptible_sleep_on);
 EXPORT_SYMBOL(interruptible_sleep_on_timeout);
 EXPORT_SYMBOL(schedule);
+#ifdef CONFIG_PREEMPT
+EXPORT_SYMBOL(preempt_schedule);
+#endif
 EXPORT_SYMBOL(schedule_timeout);
 #if CONFIG_SMP
 EXPORT_SYMBOL(set_cpus_allowed);
@@ -469,6 +515,13 @@ EXPORT_SYMBOL(xtime);
 EXPORT_SYMBOL(do_gettimeofday);
 EXPORT_SYMBOL(do_settimeofday);
 
+#if LOWLATENCY_NEEDED
+EXPORT_SYMBOL(set_running_and_schedule);
+#ifdef CONFIG_LOLAT_SYSCTL
+EXPORT_SYMBOL(__enable_lowlatency);
+#endif
+#endif
+
 #if !defined(__ia64__)
 EXPORT_SYMBOL(loops_per_jiffy);
 #endif
diff -uNrp linux-2.4.23/kernel/module.c linux-2.4.23-fusion/kernel/module.c
--- linux-2.4.23/kernel/module.c	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/kernel/module.c	2004-01-11 15:29:37.000000000 +0100
@@ -1187,6 +1187,11 @@ static void *s_start(struct seq_file *m,
 		return ERR_PTR(-ENOMEM);
 	lock_kernel();
 	for (v = module_list, n = *pos; v; n -= v->nsyms, v = v->next) {
+#if 0
+		/* We can't actually do this, because we'd create a
+		 * race against module unload.  Need a semaphore. */
+		conditional_schedule();
+#endif
 		if (n < v->nsyms) {
 			p->mod = v;
 			p->index = n;
diff -uNrp linux-2.4.23/kernel/panic.c linux-2.4.23-fusion/kernel/panic.c
--- linux-2.4.23/kernel/panic.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/panic.c	2004-01-11 15:29:37.000000000 +0100
@@ -68,6 +68,9 @@ NORET_TYPE void panic(const char * fmt, 
 		printk(KERN_EMERG "In idle task - not syncing\n");
 	else
 		sys_sync();
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_dump_state();
+#endif /* CONFIG_ADEOS_CORE */
 	bust_spinlocks(0);
 
 #ifdef CONFIG_SMP
diff -uNrp linux-2.4.23/kernel/printk.c linux-2.4.23-fusion/kernel/printk.c
--- linux-2.4.23/kernel/printk.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/printk.c	2004-01-11 15:29:37.000000000 +0100
@@ -29,6 +29,17 @@
 
 #include <asm/uaccess.h>
 
+#ifdef CONFIG_ADEOS_CORE
+#undef spin_lock_irq
+#define	spin_lock_irq(lock)                 adeos_spin_lock_disable(lock)
+#undef spin_unlock_irq
+#define	spin_unlock_irq(lock)               adeos_spin_unlock_enable(lock)
+#undef spin_lock_irqsave
+#define	spin_lock_irqsave(lock, flags)      adeos_spin_lock_irqsave(lock,flags)
+#undef spin_unlock_irqrestore
+#define spin_unlock_irqrestore(lock, flags) adeos_spin_unlock_irqrestore(lock,flags)
+#endif /* CONFIG_ADEOS_CORE */
+
 #if !defined(CONFIG_LOG_BUF_SHIFT) || (CONFIG_LOG_BUF_SHIFT == 0)
 #if defined(CONFIG_MULTIQUAD) || defined(CONFIG_IA64)
 #define LOG_BUF_LEN	(65536)
diff -uNrp linux-2.4.23/kernel/sched.c linux-2.4.23-fusion/kernel/sched.c
--- linux-2.4.23/kernel/sched.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/sched.c	2004-01-11 15:29:37.000000000 +0100
@@ -302,6 +302,17 @@ send_now_idle:
 		if (tsk->processor != this_cpu)
 			smp_send_reschedule(tsk->processor);
 	}
+#if LOWLATENCY_NEEDED
+	if (enable_lowlatency && (p->policy != SCHED_OTHER)) {
+		struct task_struct *t;
+		for (i = 0; i < smp_num_cpus; i++) {
+			cpu = cpu_logical_map(i);
+			t = cpu_curr(cpu);
+			if (t != tsk)
+				t->need_resched = 1;
+		}
+	}
+#endif
 	return;
 		
 
@@ -489,7 +500,7 @@ static inline void __schedule_tail(struc
 	task_lock(prev);
 	task_release_cpu(prev);
 	mb();
-	if (prev->state == TASK_RUNNING)
+	if (task_on_runqueue(prev))
 		goto needs_resched;
 
 out_unlock:
@@ -519,7 +530,7 @@ needs_resched:
 			goto out_unlock;
 
 		spin_lock_irqsave(&runqueue_lock, flags);
-		if ((prev->state == TASK_RUNNING) && !task_has_cpu(prev))
+		if (task_on_runqueue(prev) && !task_has_cpu(prev))
 			reschedule_idle(prev);
 		spin_unlock_irqrestore(&runqueue_lock, flags);
 		goto out_unlock;
@@ -532,6 +543,10 @@ needs_resched:
 asmlinkage void schedule_tail(struct task_struct *prev)
 {
 	__schedule_tail(prev);
+ 	preempt_enable();
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_enter_process();
+#endif /* CONFIG_ADEOS_CORE */
 }
 
 /*
@@ -551,9 +566,15 @@ asmlinkage void schedule(void)
 	struct list_head *tmp;
 	int this_cpu, c;
 
+#ifdef CONFIG_ADEOS_CORE
+	if (adp_current != adp_root) /* Let's be helpful and conservative. */
+	    return;
+#endif /* CONFIG_ADEOS_CORE */
 
 	spin_lock_prefetch(&runqueue_lock);
 
+ 	preempt_disable();
+
 	BUG_ON(!current->active_mm);
 need_resched_back:
 	prev = current;
@@ -581,6 +602,14 @@ need_resched_back:
 			move_last_runqueue(prev);
 		}
 
+#ifdef CONFIG_PREEMPT
+	/*
+	 * entering from preempt_schedule, off a kernel preemption,
+	 * go straight to picking the next task.
+	 */
+	if (unlikely(preempt_get_count() & PREEMPT_ACTIVE))
+		goto treat_like_run;
+#endif
 	switch (prev->state) {
 		case TASK_INTERRUPTIBLE:
 			if (signal_pending(prev)) {
@@ -591,6 +620,9 @@ need_resched_back:
 			del_from_runqueue(prev);
 		case TASK_RUNNING:;
 	}
+#ifdef CONFIG_PREEMPT
+	treat_like_run:
+#endif
 	prev->need_resched = 0;
 
 	/*
@@ -625,6 +657,11 @@ repeat_schedule:
 		goto repeat_schedule;
 	}
 
+	if (unlikely(prev->need_resched)) {
+		prev->need_resched = 0;
+		goto repeat_schedule;
+	}
+
 	/*
 	 * from this point on nothing can prevent us from
 	 * switching to the next task, save this fact in
@@ -668,7 +705,11 @@ repeat_schedule:
 	 * but prev is set to (the just run) 'last' process by switch_to().
 	 * This might sound slightly confusing but makes tons of sense.
 	 */
+#ifdef CONFIG_ADEOS_CORE
+	prepare_to_switch(prev,next);
+#else /* !CONFIG_ADEOS_CORE */
 	prepare_to_switch();
+#endif /* CONFIG_ADEOS_CORE */
 	{
 		struct mm_struct *mm = next->mm;
 		struct mm_struct *oldmm = prev->active_mm;
@@ -693,15 +734,51 @@ repeat_schedule:
 	 * stack.
 	 */
 	switch_to(prev, next, prev);
+#ifdef CONFIG_ADEOS_CORE
+	if (__adeos_schedule_tail(prev) > 0)
+	    {
+#ifdef CONFIG_PREEMPT
+	    /* Decrement the preemption count even as part of a
+	       truncated scheduling tail. We cannot use
+	       preempt_enable_no_resched() here since we might be
+	       running on behalf of a non-root domain, recycling the
+	       root domain code and stack context. */
+	    current->preempt_count--;
+#endif /* CONFIG_PREEMPT */
+	    return;
+	    }
+#endif /* CONFIG_ADEOS_CORE */
 	__schedule_tail(prev);
 
 same_process:
 	reacquire_kernel_lock(current);
 	if (current->need_resched)
 		goto need_resched_back;
+	preempt_enable_no_resched();
 	return;
 }
 
+#ifdef CONFIG_PREEMPT
+/*
+ * this is is the entry point to schedule() from in-kernel preemption
+ */
+asmlinkage void preempt_schedule(void)
+{
+	if (unlikely(irqs_disabled()))
+			return;
+
+need_resched:
+	current->preempt_count += PREEMPT_ACTIVE;
+	schedule();
+	current->preempt_count -= PREEMPT_ACTIVE;
+
+	/* we could miss a preemption opportunity between schedule and now */
+	barrier();
+	if (unlikely(current->need_resched))
+		goto need_resched;
+}
+#endif /* CONFIG_PREEMPT */
+
 /*
  * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just wake everything
  * up.  If it's an exclusive wakeup (nr_exclusive == small +ve number) then we wake all the
@@ -989,6 +1066,13 @@ static int setscheduler(pid_t pid, int p
 		goto out_unlock;
 
 	retval = 0;
+#ifdef CONFIG_ADEOS_CORE
+	{
+	struct { struct task_struct *task; int policy; struct sched_param *param; } evdata = { p, policy, &lp };
+	if (__adeos_renice_process(&evdata))
+	    goto out_unlock;
+	}
+#endif /* CONFIG_ADEOS_CORE */
 	p->policy = policy;
 	p->rt_priority = lp.sched_priority;
 
@@ -1365,6 +1449,13 @@ void __init init_idle(void)
 	sched_data->curr = current;
 	sched_data->last_schedule = get_cycles();
 	clear_bit(current->processor, &wait_init_idle);
+#ifdef CONFIG_PREEMPT
+	/*
+	 * fix up the preempt_count for non-CPU0 idle threads
+	 */
+	if (current->processor)
+		current->preempt_count = 0;
+#endif
 }
 
 extern void init_timervecs (void);
@@ -1395,3 +1486,93 @@ void __init sched_init(void)
 	atomic_inc(&init_mm.mm_count);
 	enter_lazy_tlb(&init_mm, current, cpu);
 }
+
+#if LOWLATENCY_NEEDED
+#if LOWLATENCY_DEBUG
+
+static struct lolat_stats_t *lolat_stats_head;
+static spinlock_t lolat_stats_lock = SPIN_LOCK_UNLOCKED;
+
+void set_running_and_schedule(struct lolat_stats_t *stats)
+{
+	spin_lock(&lolat_stats_lock);
+	if (stats->visited == 0) {
+		stats->visited = 1;
+		stats->next = lolat_stats_head;
+		lolat_stats_head = stats;
+	}
+	stats->count++;
+	spin_unlock(&lolat_stats_lock);
+
+	if (current->state != TASK_RUNNING)
+		set_current_state(TASK_RUNNING);
+	schedule();
+}
+
+void show_lolat_stats(void)
+{
+	struct lolat_stats_t *stats = lolat_stats_head;
+
+	printk("Low latency scheduling stats:\n");
+	while (stats) {
+		printk("%s:%d: %lu\n", stats->file, stats->line, stats->count);
+		stats->count = 0;
+		stats = stats->next;
+	}
+}
+
+#else	/* LOWLATENCY_DEBUG */
+
+void set_running_and_schedule()
+{
+	if (current->state != TASK_RUNNING)
+		__set_current_state(TASK_RUNNING);
+	schedule();
+}
+
+#endif	/* LOWLATENCY_DEBUG */
+
+int ll_copy_to_user(void *to_user, const void *from, unsigned long len)
+{
+	while (len) {
+		unsigned long n_to_copy = len;
+		unsigned long remainder;
+
+		if (n_to_copy > 4096)
+			n_to_copy = 4096;
+		remainder = copy_to_user(to_user, from, n_to_copy);
+		if (remainder)
+			return remainder + len;
+		to_user = ((char *)to_user) + n_to_copy;
+		from = ((char *)from) + n_to_copy;
+		len -= n_to_copy;
+		conditional_schedule();
+	}
+	return 0;
+}
+
+int ll_copy_from_user(void *to, const void *from_user, unsigned long len)
+{
+	while (len) {
+		unsigned long n_to_copy = len;
+		unsigned long remainder;
+
+		if (n_to_copy > 4096)
+			n_to_copy = 4096;
+		remainder = copy_from_user(to, from_user, n_to_copy);
+		if (remainder)
+			return remainder + len;
+		to = ((char *)to) + n_to_copy;
+		from_user = ((char *)from_user) + n_to_copy;
+		len -= n_to_copy;
+		conditional_schedule();
+	}
+	return 0;
+}
+
+#ifdef CONFIG_LOLAT_SYSCTL
+struct low_latency_enable_struct __enable_lowlatency = { 0, };
+#endif
+
+#endif	/* LOWLATENCY_NEEDED */
+
diff -uNrp linux-2.4.23/kernel/signal.c linux-2.4.23-fusion/kernel/signal.c
--- linux-2.4.23/kernel/signal.c	2003-06-13 16:51:39.000000000 +0200
+++ linux-2.4.23-fusion/kernel/signal.c	2004-01-11 15:29:37.000000000 +0100
@@ -554,6 +554,17 @@ printk("SIG queue (%s:%d): %d ", t->comm
 	if (!sig || !t->sig)
 		goto out_nolock;
 
+#ifdef CONFIG_ADEOS_CORE
+	/* If some domain handler in the pipeline doesn't ask for
+	   propagation, return success pretending that 'sig' was
+	   delivered. */
+	{
+	struct { struct task_struct *task; int sig; } evdata = { t, sig };
+	if (__adeos_signal_process(&evdata))
+	    goto out_nolock;
+	}
+#endif /* CONFIG_ADEOS_CORE */
+
 	spin_lock_irqsave(&t->sigmask_lock, flags);
 	handle_stop_signal(sig, t);
 
diff -uNrp linux-2.4.23/kernel/softirq.c linux-2.4.23-fusion/kernel/softirq.c
--- linux-2.4.23/kernel/softirq.c	2002-11-29 00:53:15.000000000 +0100
+++ linux-2.4.23-fusion/kernel/softirq.c	2004-01-11 15:29:37.000000000 +0100
@@ -60,7 +60,7 @@ static inline void wakeup_softirqd(unsig
 
 asmlinkage void do_softirq()
 {
-	int cpu = smp_processor_id();
+	int cpu;
 	__u32 pending;
 	unsigned long flags;
 	__u32 mask;
@@ -70,6 +70,8 @@ asmlinkage void do_softirq()
 
 	local_irq_save(flags);
 
+	cpu = smp_processor_id();
+
 	pending = softirq_pending(cpu);
 
 	if (pending) {
@@ -151,10 +153,11 @@ struct tasklet_head tasklet_hi_vec[NR_CP
 
 void __tasklet_schedule(struct tasklet_struct *t)
 {
-	int cpu = smp_processor_id();
+	int cpu;
 	unsigned long flags;
 
 	local_irq_save(flags);
+	cpu = smp_processor_id();
 	t->next = tasklet_vec[cpu].list;
 	tasklet_vec[cpu].list = t;
 	cpu_raise_softirq(cpu, TASKLET_SOFTIRQ);
@@ -175,10 +178,11 @@ void __tasklet_hi_schedule(struct taskle
 
 static void tasklet_action(struct softirq_action *a)
 {
-	int cpu = smp_processor_id();
+	int cpu;
 	struct tasklet_struct *list;
 
 	local_irq_disable();
+	cpu = smp_processor_id();
 	list = tasklet_vec[cpu].list;
 	tasklet_vec[cpu].list = NULL;
 	local_irq_enable();
@@ -209,10 +213,11 @@ static void tasklet_action(struct softir
 
 static void tasklet_hi_action(struct softirq_action *a)
 {
-	int cpu = smp_processor_id();
+	int cpu;
 	struct tasklet_struct *list;
 
 	local_irq_disable();
+	cpu = smp_processor_id();
 	list = tasklet_hi_vec[cpu].list;
 	tasklet_hi_vec[cpu].list = NULL;
 	local_irq_enable();
diff -uNrp linux-2.4.23/kernel/sys.c linux-2.4.23-fusion/kernel/sys.c
--- linux-2.4.23/kernel/sys.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/sys.c	2004-01-11 15:29:37.000000000 +0100
@@ -320,6 +320,7 @@ asmlinkage long sys_reboot(int magic1, i
 		notifier_call_chain(&reboot_notifier_list, SYS_HALT, NULL);
 		printk(KERN_EMERG "System halted.\n");
 		machine_halt();
+		unlock_kernel();
 		do_exit(0);
 		break;
 
@@ -327,6 +328,7 @@ asmlinkage long sys_reboot(int magic1, i
 		notifier_call_chain(&reboot_notifier_list, SYS_POWER_OFF, NULL);
 		printk(KERN_EMERG "Power down.\n");
 		machine_power_off();
+		unlock_kernel();
 		do_exit(0);
 		break;
 
diff -uNrp linux-2.4.23/kernel/sysctl.c linux-2.4.23-fusion/kernel/sysctl.c
--- linux-2.4.23/kernel/sysctl.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/kernel/sysctl.c	2004-01-11 15:29:37.000000000 +0100
@@ -275,6 +275,10 @@ static ctl_table kern_table[] = {
 	{KERN_EXCEPTION_TRACE,"exception-trace",
 	 &exception_trace,sizeof(int),0644,NULL,&proc_dointvec},
 #endif	
+#ifdef CONFIG_LOLAT_SYSCTL
+	{KERN_LOWLATENCY, "lowlatency", &enable_lowlatency, sizeof (int),
+	 0644, NULL, &proc_dointvec},
+#endif
 	{0}
 };
 
@@ -360,6 +364,9 @@ void __init sysctl_init(void)
 #ifdef CONFIG_PROC_FS
 	register_proc_table(root_table, proc_sys_root);
 	init_irq_proc();
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_init_proc();
+#endif /* CONFIG_ADEOS_CORE */
 #endif
 }
 
diff -uNrp linux-2.4.23/lib/dec_and_lock.c linux-2.4.23-fusion/lib/dec_and_lock.c
--- linux-2.4.23/lib/dec_and_lock.c	2001-10-03 18:11:26.000000000 +0200
+++ linux-2.4.23-fusion/lib/dec_and_lock.c	2004-01-11 15:29:37.000000000 +0100
@@ -1,5 +1,6 @@
 #include <linux/module.h>
 #include <linux/spinlock.h>
+#include <linux/sched.h>
 #include <asm/atomic.h>
 
 /*
diff -uNrp linux-2.4.23/mm/filemap.c linux-2.4.23-fusion/mm/filemap.c
--- linux-2.4.23/mm/filemap.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/mm/filemap.c	2004-01-11 15:29:37.000000000 +0100
@@ -183,7 +183,9 @@ void invalidate_inode_pages(struct inode
 {
 	struct list_head *head, *curr;
 	struct page * page;
+	int ll_count = 100;
 
+restart:
 	head = &inode->i_mapping->clean_pages;
 
 	spin_lock(&pagemap_lru_lock);
@@ -194,6 +196,14 @@ void invalidate_inode_pages(struct inode
 		page = list_entry(curr, struct page, list);
 		curr = curr->next;
 
+		if (conditional_schedule_needed() && ll_count) {
+			spin_unlock(&pagecache_lock);
+			spin_unlock(&pagemap_lru_lock);
+			unconditional_schedule();
+			ll_count--;
+			goto restart;
+		}
+
 		/* We cannot invalidate something in dirty.. */
 		if (PageDirty(page))
 			continue;
@@ -257,8 +267,7 @@ static void truncate_complete_page(struc
 	page_cache_release(page);
 }
 
-static int FASTCALL(truncate_list_pages(struct list_head *, unsigned long, unsigned *));
-static int truncate_list_pages(struct list_head *head, unsigned long start, unsigned *partial)
+static int truncate_list_pages(struct list_head *head, unsigned long start, unsigned *partial, int *restart_count)
 {
 	struct list_head *curr;
 	struct page * page;
@@ -269,6 +278,17 @@ static int truncate_list_pages(struct li
 	while (curr != head) {
 		unsigned long offset;
 
+		if (conditional_schedule_needed() && *restart_count) {
+			(*restart_count)--;
+			list_del(head);
+			list_add(head, curr);		/* Restart on this page */
+			spin_unlock(&pagecache_lock);
+			unconditional_schedule();
+			spin_lock(&pagecache_lock);
+			unlocked = 1;
+			goto restart;
+		}
+
 		page = list_entry(curr, struct page, list);
 		offset = page->index;
 
@@ -301,13 +321,11 @@ static int truncate_list_pages(struct li
 			} else
  				wait_on_page(page);
 
-			page_cache_release(page);
-
-			if (current->need_resched) {
-				__set_current_state(TASK_RUNNING);
-				schedule();
+			if (LOWLATENCY_NEEDED) {
+				*restart_count = 4;	/* We made progress */
 			}
 
+			page_cache_release(page);
 			spin_lock(&pagecache_lock);
 			goto restart;
 		}
@@ -330,13 +348,14 @@ void truncate_inode_pages(struct address
 {
 	unsigned long start = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
 	unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);
+	int restart_count = 4;
 	int unlocked;
 
 	spin_lock(&pagecache_lock);
 	do {
-		unlocked = truncate_list_pages(&mapping->clean_pages, start, &partial);
-		unlocked |= truncate_list_pages(&mapping->dirty_pages, start, &partial);
-		unlocked |= truncate_list_pages(&mapping->locked_pages, start, &partial);
+		unlocked = truncate_list_pages(&mapping->clean_pages, start, &partial, &restart_count);
+		unlocked |= truncate_list_pages(&mapping->dirty_pages, start, &partial, &restart_count);
+		unlocked |= truncate_list_pages(&mapping->locked_pages, start, &partial, &restart_count);
 	} while (unlocked);
 	/* Traversed all three lists without dropping the lock */
 	spin_unlock(&pagecache_lock);
@@ -481,6 +500,7 @@ static int do_buffer_fdatasync(struct li
 
 		page_cache_get(page);
 		spin_unlock(&pagecache_lock);
+		conditional_schedule();		/* sys_msync() (only used by minixfs, udf) */
 		lock_page(page);
 
 		/* The buffers could have been free'd while we waited for the page lock */
@@ -567,12 +587,14 @@ int filemap_fdatasync(struct address_spa
 		list_del(&page->list);
 		list_add(&page->list, &mapping->locked_pages);
 
-		if (!PageDirty(page))
-			continue;
-
 		page_cache_get(page);
 		spin_unlock(&pagecache_lock);
 
+		conditional_schedule();		/* sys_msync() */
+
+		if (!PageDirty(page))
+			goto clean;
+
 		lock_page(page);
 
 		if (PageDirty(page)) {
@@ -583,7 +605,7 @@ int filemap_fdatasync(struct address_spa
 				ret = err;
 		} else
 			UnlockPage(page);
-
+clean:
 		page_cache_release(page);
 		spin_lock(&pagecache_lock);
 	}
@@ -601,7 +623,8 @@ int filemap_fdatasync(struct address_spa
 int filemap_fdatawait(struct address_space * mapping)
 {
 	int ret = 0;
-
+	DEFINE_RESCHED_COUNT;
+restart:
 	spin_lock(&pagecache_lock);
 
         while (!list_empty(&mapping->locked_pages)) {
@@ -610,6 +633,17 @@ int filemap_fdatawait(struct address_spa
 		list_del(&page->list);
 		list_add(&page->list, &mapping->clean_pages);
 
+		if (TEST_RESCHED_COUNT(32)) {
+			RESET_RESCHED_COUNT();
+			if (conditional_schedule_needed()) {
+				page_cache_get(page);
+				spin_unlock(&pagecache_lock);
+				unconditional_schedule();
+				page_cache_release(page);
+				goto restart;
+			}
+		}
+
 		if (!PageLocked(page))
 			continue;
 
@@ -719,8 +753,10 @@ static int page_cache_read(struct file *
 	spin_lock(&pagecache_lock);
 	page = __find_page_nolock(mapping, offset, *hash);
 	spin_unlock(&pagecache_lock);
-	if (page)
+	if (page) {
+		conditional_schedule();
 		return 0;
+	}
 
 	page = page_cache_alloc(mapping);
 	if (!page)
@@ -990,6 +1026,11 @@ static struct page * __find_lock_page_he
 	 * the hash-list needs a held write-lock.
 	 */
 repeat:
+	if (conditional_schedule_needed()) {
+		spin_unlock(&pagecache_lock);
+		unconditional_schedule();
+		spin_lock(&pagecache_lock);
+	}
 	page = __find_page_nolock(mapping, offset, hash);
 	if (page) {
 		page_cache_get(page);
@@ -1440,6 +1481,8 @@ found_page:
 		page_cache_get(page);
 		spin_unlock(&pagecache_lock);
 
+		conditional_schedule();		/* sys_read() */
+
 		if (!Page_Uptodate(page))
 			goto page_not_up_to_date;
 		generic_file_readahead(reada_ok, filp, inode, page);
@@ -2190,6 +2233,12 @@ static inline int filemap_sync_pte_range
 		address += PAGE_SIZE;
 		pte++;
 	} while (address && (address < end));
+
+	if (conditional_schedule_needed()) {
+		spin_unlock(&vma->vm_mm->page_table_lock);
+		unconditional_schedule();		/* syncing large mapped files */
+		spin_lock(&vma->vm_mm->page_table_lock);
+	}
 	return error;
 }
 
@@ -2606,7 +2655,9 @@ static long madvise_dontneed(struct vm_a
 	if (vma->vm_flags & VM_LOCKED)
 		return -EINVAL;
 
-	zap_page_range(vma->vm_mm, start, end - start);
+        zap_page_range(vma->vm_mm, start, end - start,
+		ZPR_COND_RESCHED);        /* sys_madvise(MADV_DONTNEED) */
+
 	return 0;
 }
 
@@ -3176,6 +3227,9 @@ do_generic_file_write(struct file *file,
 			goto sync_failure;
 		page_fault = __copy_from_user(kaddr+offset, buf, bytes);
 		flush_dcache_page(page);
+
+                conditional_schedule();
+
 		status = mapping->a_ops->commit_write(file, page, offset, offset+bytes);
 		if (page_fault)
 			goto fail_write;
diff -uNrp linux-2.4.23/mm/memory.c linux-2.4.23-fusion/mm/memory.c
--- linux-2.4.23/mm/memory.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/mm/memory.c	2004-01-11 15:29:37.000000000 +0100
@@ -357,7 +357,7 @@ static inline int zap_pmd_range(mmu_gath
 /*
  * remove user pages in a given range.
  */
-void zap_page_range(struct mm_struct *mm, unsigned long address, unsigned long size)
+static void do_zap_page_range(struct mm_struct *mm, unsigned long address, unsigned long size)
 {
 	mmu_gather_t *tlb;
 	pgd_t * dir;
@@ -478,6 +478,10 @@ int get_user_pages(struct task_struct *t
 			struct page *map;
 			while (!(map = follow_page(mm, start, write))) {
 				spin_unlock(&mm->page_table_lock);
+
+				/* Pinning down many physical pages (kiobufs, mlockall) */
+				conditional_schedule();
+
 				switch (handle_mm_fault(mm, vma, start, write)) {
 				case 1:
 					tsk->min_flt++;
@@ -639,6 +643,21 @@ void unmap_kiobuf (struct kiobuf *iobuf)
 	iobuf->locked = 0;
 }
 
+#define MAX_ZAP_BYTES 256*PAGE_SIZE
+
+void zap_page_range(struct mm_struct *mm, unsigned long address, unsigned long size, int actions)
+{
+	while (size) {
+		unsigned long chunk = size;
+		if (actions & ZPR_COND_RESCHED && chunk > MAX_ZAP_BYTES)
+			chunk = MAX_ZAP_BYTES;
+		do_zap_page_range(mm, address, chunk);
+		if (actions & ZPR_COND_RESCHED)
+			conditional_schedule();
+		address += chunk;
+		size -= chunk;
+	}
+}
 
 /*
  * Lock down all of the pages of a kiovec for IO.
@@ -748,11 +767,18 @@ int unlock_kiovec(int nr, struct kiobuf 
 	return 0;
 }
 
-static inline void zeromap_pte_range(pte_t * pte, unsigned long address,
-                                     unsigned long size, pgprot_t prot)
+static inline void zeromap_pte_range(struct mm_struct *mm, pte_t * pte,
+				unsigned long address, unsigned long size,
+				pgprot_t prot)
 {
 	unsigned long end;
 
+	if (conditional_schedule_needed()) {
+		spin_unlock(&mm->page_table_lock);
+		unconditional_schedule();		/* mmap(/dev/zero) */
+		spin_lock(&mm->page_table_lock);
+	}
+
 	address &= ~PMD_MASK;
 	end = address + size;
 	if (end > PMD_SIZE)
@@ -780,7 +806,7 @@ static inline int zeromap_pmd_range(stru
 		pte_t * pte = pte_alloc(mm, pmd, address);
 		if (!pte)
 			return -ENOMEM;
-		zeromap_pte_range(pte, address, end - address, prot);
+		zeromap_pte_range(mm, pte, address, end - address, prot);
 		address = (address + PMD_SIZE) & PMD_MASK;
 		pmd++;
 	} while (address && (address < end));
@@ -1014,7 +1040,7 @@ static void vmtruncate_list(struct vm_ar
 
 		/* mapping wholly truncated? */
 		if (mpnt->vm_pgoff >= pgoff) {
-			zap_page_range(mm, start, len);
+                        zap_page_range(mm, start, len, 0);
 			continue;
 		}
 
@@ -1027,7 +1053,7 @@ static void vmtruncate_list(struct vm_ar
 		/* Ok, partially affected.. */
 		start += diff << PAGE_SHIFT;
 		len = (len - diff) << PAGE_SHIFT;
-		zap_page_range(mm, start, len);
+                zap_page_range(mm, start, len, 0);
 	} while ((mpnt = mpnt->vm_next_share) != NULL);
 }
 
diff -uNrp linux-2.4.23/mm/mmap.c linux-2.4.23-fusion/mm/mmap.c
--- linux-2.4.23/mm/mmap.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/mm/mmap.c	2004-01-11 15:29:37.000000000 +0100
@@ -594,7 +594,7 @@ unmap_and_free_vma:
 	fput(file);
 
 	/* Undo any partial mapping done by a device driver. */
-	zap_page_range(mm, vma->vm_start, vma->vm_end - vma->vm_start);
+        zap_page_range(mm, vma->vm_start, vma->vm_end - vma->vm_start, 0);
 free_vma:
 	kmem_cache_free(vm_area_cachep, vma);
 	return error;
@@ -994,7 +994,7 @@ int do_munmap(struct mm_struct *mm, unsi
 		remove_shared_vm_struct(mpnt);
 		mm->map_count--;
 
-		zap_page_range(mm, st, size);
+                zap_page_range(mm, st, size, ZPR_COND_RESCHED);   /* sys_munmap() */
 
 		/*
 		 * Fix the mapping, and free the old area if it wasn't reused.
@@ -1154,7 +1154,7 @@ void exit_mmap(struct mm_struct * mm)
 		}
 		mm->map_count--;
 		remove_shared_vm_struct(mpnt);
-		zap_page_range(mm, start, size);
+		zap_page_range(mm, start, size, ZPR_COND_RESCHED);      /* sys_exit() */
 		if (mpnt->vm_file)
 			fput(mpnt->vm_file);
 		kmem_cache_free(vm_area_cachep, mpnt);
diff -uNrp linux-2.4.23/mm/mremap.c linux-2.4.23-fusion/mm/mremap.c
--- linux-2.4.23/mm/mremap.c	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/mm/mremap.c	2004-01-11 15:29:37.000000000 +0100
@@ -118,7 +118,7 @@ oops_we_failed:
 	flush_cache_range(mm, new_addr, new_addr + len);
 	while ((offset += PAGE_SIZE) < len)
 		move_one_page(mm, new_addr + offset, old_addr + offset);
-	zap_page_range(mm, new_addr, len);
+        zap_page_range(mm, new_addr, len, 0);
 	return -1;
 }
 
diff -uNrp linux-2.4.23/mm/slab.c linux-2.4.23-fusion/mm/slab.c
--- linux-2.4.23/mm/slab.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/mm/slab.c	2004-01-11 15:29:38.000000000 +0100
@@ -49,7 +49,8 @@
  *  constructors and destructors are called without any locking.
  *  Several members in kmem_cache_t and slab_t never change, they
  *	are accessed without any locking.
- *  The per-cpu arrays are never accessed from the wrong cpu, no locking.
+ *  The per-cpu arrays are never accessed from the wrong cpu, no locking,
+ *  	and local interrupts are disabled so slab code is preempt-safe.
  *  The non-constant members are protected with a per-cache irq spinlock.
  *
  * Further notes from the original documentation:
@@ -858,12 +859,14 @@ static int is_chained_kmem_cache(kmem_ca
  */
 static void smp_call_function_all_cpus(void (*func) (void *arg), void *arg)
 {
+	preempt_disable();
 	local_irq_disable();
 	func(arg);
 	local_irq_enable();
 
 	if (smp_call_function(func, arg, 1, 1))
 		BUG();
+	preempt_enable();
 }
 typedef struct ccupdate_struct_s
 {
@@ -935,6 +938,7 @@ static int __kmem_cache_shrink_locked(km
 		list_del(&slabp->list);
 
 		spin_unlock_irq(&cachep->spinlock);
+		conditional_schedule();
 		kmem_slab_destroy(cachep, slabp);
 		ret++;
 		spin_lock_irq(&cachep->spinlock);
@@ -1851,6 +1855,7 @@ perfect:
 		 */
 		spin_unlock_irq(&best_cachep->spinlock);
 		kmem_slab_destroy(best_cachep, slabp);
+		conditional_schedule();		/* try_to_free_pages() */
 		spin_lock_irq(&best_cachep->spinlock);
 	}
 	spin_unlock_irq(&best_cachep->spinlock);
diff -uNrp linux-2.4.23/mm/swapfile.c linux-2.4.23-fusion/mm/swapfile.c
--- linux-2.4.23/mm/swapfile.c	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/mm/swapfile.c	2004-01-11 15:29:38.000000000 +0100
@@ -832,7 +832,7 @@ int get_swaparea_info(char *buf)
 				len += sprintf(buf + len, "partition\t");
 
 			usedswap = 0;
-			for (j = 0; j < ptr->max; ++j)
+			for (j = 0; j < ptr->max; ++j) {
 				switch (ptr->swap_map[j]) {
 					case SWAP_MAP_BAD:
 					case 0:
@@ -840,6 +840,8 @@ int get_swaparea_info(char *buf)
 					default:
 						usedswap++;
 				}
+				conditional_schedule();
+			}
 			len += sprintf(buf + len, "%d\t%d\t%d\n", ptr->pages << (PAGE_SHIFT - 10), 
 				usedswap << (PAGE_SHIFT - 10), ptr->prio);
 		}
@@ -1138,6 +1140,11 @@ void si_swapinfo(struct sysinfo *val)
 		if (swap_info[i].flags != SWP_USED)
 			continue;
 		for (j = 0; j < swap_info[i].max; ++j) {
+			if (conditional_schedule_needed()) {
+				swap_list_unlock();
+				conditional_schedule();
+				swap_list_lock();
+			}
 			switch (swap_info[i].swap_map[j]) {
 				case 0:
 				case SWAP_MAP_BAD:
diff -uNrp linux-2.4.23/mm/vmalloc.c linux-2.4.23-fusion/mm/vmalloc.c
--- linux-2.4.23/mm/vmalloc.c	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/mm/vmalloc.c	2004-01-11 15:29:38.000000000 +0100
@@ -165,6 +165,9 @@ static inline int __vmalloc_area_pages (
 	dir = pgd_offset_k(address);
 	spin_lock(&init_mm.page_table_lock);
 	do {
+#ifdef CONFIG_ADEOS_CORE
+		pgd_t olddir = *dir;
+#endif /* CONFIG_ADEOS_CORE */
 		pmd_t *pmd;
 		
 		pmd = pmd_alloc(&init_mm, dir, address);
@@ -176,6 +179,11 @@ static inline int __vmalloc_area_pages (
 		if (alloc_area_pmd(pmd, address, end - address, gfp_mask, prot, pages))
 			break;
 
+#ifdef CONFIG_ADEOS_CORE
+		if (pgd_val(olddir) != pgd_val(*dir))
+		    set_pgdir(address,*dir);
+#endif /* CONFIG_ADEOS_CORE */
+
 		address = (address + PGDIR_SIZE) & PGDIR_MASK;
 		dir++;
 
diff -uNrp linux-2.4.23/mm/vmscan.c linux-2.4.23-fusion/mm/vmscan.c
--- linux-2.4.23/mm/vmscan.c	2003-11-28 23:18:54.000000000 +0100
+++ linux-2.4.23-fusion/mm/vmscan.c	2004-01-11 15:29:38.000000000 +0100
@@ -189,6 +189,7 @@ static inline int swap_out_pmd(struct mm
 {
 	pte_t * pte;
 	unsigned long pmd_end;
+	DEFINE_RESCHED_COUNT;
 
 	if (pmd_none(*dir))
 		return count;
@@ -214,11 +215,17 @@ static inline int swap_out_pmd(struct mm
 					address += PAGE_SIZE;
 					break;
 				}
+                                if (TEST_RESCHED_COUNT(4)) {
+                                        if (conditional_schedule_needed())
+						goto out;
+                                        RESET_RESCHED_COUNT();
+                                }
 			}
 		}
 		address += PAGE_SIZE;
 		pte++;
 	} while (address && (address < end));
+out:
 	mm->swap_address = address;
 	return count;
 }
@@ -247,6 +254,8 @@ static inline int swap_out_pgd(struct mm
 		count = swap_out_pmd(mm, vma, pmd, address, end, count, classzone);
 		if (!count)
 			break;
+		if (conditional_schedule_needed())
+			return count;
 		address = (address + PMD_SIZE) & PMD_MASK;
 		pmd++;
 	} while (address && (address < end));
@@ -271,6 +280,8 @@ static inline int swap_out_vma(struct mm
 		count = swap_out_pgd(mm, vma, pgdir, address, end, count, classzone);
 		if (!count)
 			break;
+		if (conditional_schedule_needed())
+			return count;
 		address = (address + PGDIR_SIZE) & PGDIR_MASK;
 		pgdir++;
 	} while (address && (address < end));
@@ -292,6 +303,7 @@ static inline int swap_out_mm(struct mm_
 	 * Find the proper vm-area after freezing the vma chain 
 	 * and ptes.
 	 */
+continue_scan:
 	spin_lock(&mm->page_table_lock);
 	address = mm->swap_address;
 	if (address == TASK_SIZE || swap_mm != mm) {
@@ -309,6 +321,12 @@ static inline int swap_out_mm(struct mm_
 			vma = vma->vm_next;
 			if (!vma)
 				break;
+                        if (conditional_schedule_needed()) {    /* Scanning a large vma */
+                                spin_unlock(&mm->page_table_lock);
+                                unconditional_schedule();
+                                /* Continue from where we left off */
+                                goto continue_scan;
+                        }
 			if (!count)
 				goto out_unlock;
 			address = vma->vm_start;
diff -uNrp linux-2.4.23/net/core/dev.c linux-2.4.23-fusion/net/core/dev.c
--- linux-2.4.23/net/core/dev.c	2003-11-28 23:18:55.000000000 +0100
+++ linux-2.4.23-fusion/net/core/dev.c	2004-01-11 15:29:38.000000000 +0100
@@ -1093,9 +1093,15 @@ int dev_queue_xmit(struct sk_buff *skb)
 		int cpu = smp_processor_id();
 
 		if (dev->xmit_lock_owner != cpu) {
+			/*
+			 * The spin_lock effectivly does a preempt lock, but 
+			 * we are about to drop that...
+			 */
+			preempt_disable();
 			spin_unlock(&dev->queue_lock);
 			spin_lock(&dev->xmit_lock);
 			dev->xmit_lock_owner = cpu;
+			preempt_enable();
 
 			if (!netif_queue_stopped(dev)) {
 				if (netdev_nit)
@@ -1274,7 +1280,7 @@ static void sample_queue(unsigned long d
 
 int netif_rx(struct sk_buff *skb)
 {
-	int this_cpu = smp_processor_id();
+	int this_cpu;
 	struct softnet_data *queue;
 	unsigned long flags;
 
@@ -1284,9 +1290,10 @@ int netif_rx(struct sk_buff *skb)
 	/* The code is rearranged so that the path is the most
 	   short when CPU is congested, but is still operating.
 	 */
-	queue = &softnet_data[this_cpu];
 
 	local_irq_save(flags);
+	this_cpu = smp_processor_id();
+	queue = &softnet_data[this_cpu];
 
 	netdev_rx_stat[this_cpu].total++;
 	if (queue->input_pkt_queue.qlen <= netdev_max_backlog) {
diff -uNrp linux-2.4.23/net/core/iovec.c linux-2.4.23-fusion/net/core/iovec.c
--- linux-2.4.23/net/core/iovec.c	2001-09-10 16:57:00.000000000 +0200
+++ linux-2.4.23-fusion/net/core/iovec.c	2004-01-11 15:29:38.000000000 +0100
@@ -88,7 +88,7 @@ int memcpy_toiovec(struct iovec *iov, un
 		if(iov->iov_len)
 		{
 			int copy = min_t(unsigned int, iov->iov_len, len);
-			if (copy_to_user(iov->iov_base, kdata, copy))
+                        if (ll_copy_to_user(iov->iov_base, kdata, copy))
 				goto out;
 			kdata+=copy;
 			len-=copy;
diff -uNrp linux-2.4.23/net/core/skbuff.c linux-2.4.23-fusion/net/core/skbuff.c
--- linux-2.4.23/net/core/skbuff.c	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/net/core/skbuff.c	2004-01-11 15:29:38.000000000 +0100
@@ -111,33 +111,37 @@ void skb_under_panic(struct sk_buff *skb
 
 static __inline__ struct sk_buff *skb_head_from_pool(void)
 {
-	struct sk_buff_head *list = &skb_head_pool[smp_processor_id()].list;
+	struct sk_buff_head *list;
+	struct sk_buff *skb = NULL;
+	unsigned long flags;
 
-	if (skb_queue_len(list)) {
-		struct sk_buff *skb;
-		unsigned long flags;
+	local_irq_save(flags);
 
-		local_irq_save(flags);
+	list = &skb_head_pool[smp_processor_id()].list;
+
+	if (skb_queue_len(list))
 		skb = __skb_dequeue(list);
-		local_irq_restore(flags);
-		return skb;
-	}
-	return NULL;
+
+	local_irq_restore(flags);
+	return skb;
 }
 
 static __inline__ void skb_head_to_pool(struct sk_buff *skb)
 {
-	struct sk_buff_head *list = &skb_head_pool[smp_processor_id()].list;
+	struct sk_buff_head *list;
+	unsigned long flags;
 
-	if (skb_queue_len(list) < sysctl_hot_list_len) {
-		unsigned long flags;
+	local_irq_save(flags);
+	list = &skb_head_pool[smp_processor_id()].list;
 
-		local_irq_save(flags);
+	if (skb_queue_len(list) < sysctl_hot_list_len) {
 		__skb_queue_head(list, skb);
 		local_irq_restore(flags);
 
 		return;
 	}
+
+	local_irq_restore(flags);
 	kmem_cache_free(skbuff_head_cache, skb);
 }
 
diff -uNrp linux-2.4.23/net/ipv4/tcp_minisocks.c linux-2.4.23-fusion/net/ipv4/tcp_minisocks.c
--- linux-2.4.23/net/ipv4/tcp_minisocks.c	2003-08-25 13:44:44.000000000 +0200
+++ linux-2.4.23-fusion/net/ipv4/tcp_minisocks.c	2004-01-11 15:29:38.000000000 +0100
@@ -433,6 +433,9 @@ static void SMP_TIMER_NAME(tcp_twkill)(u
 {
 	struct tcp_tw_bucket *tw;
 	int killed = 0;
+#if LOWLATENCY_NEEDED
+	int max_killed = 0;
+#endif
 
 	/* NOTE: compare this to previous version where lock
 	 * was released after detaching chain. It was racy,
@@ -446,6 +449,13 @@ static void SMP_TIMER_NAME(tcp_twkill)(u
 		goto out;
 
 	while((tw = tcp_tw_death_row[tcp_tw_death_row_slot]) != NULL) {
+#if LOWLATENCY_NEEDED
+		/* This loop takes ~6 usecs per iteration. */
+		if (killed > 100) {
+			max_killed = 1;
+			break;
+		}
+#endif
 		tcp_tw_death_row[tcp_tw_death_row_slot] = tw->next_death;
 		if (tw->next_death)
 			tw->next_death->pprev_death = tw->pprev_death;
@@ -458,12 +468,24 @@ static void SMP_TIMER_NAME(tcp_twkill)(u
 		killed++;
 
 		spin_lock(&tw_death_lock);
+
+	}
+
+#if LOWLATENCY_NEEDED
+	if (max_killed) {	/* More to do: do it soon */
+		mod_timer(&tcp_tw_timer, jiffies+2);
+		tcp_tw_count -= killed;
+	}
+	else
+#endif
+	{
+		tcp_tw_death_row_slot =
+			((tcp_tw_death_row_slot + 1) & (TCP_TWKILL_SLOTS - 1));
+
+		if ((tcp_tw_count -= killed) != 0)
+			mod_timer(&tcp_tw_timer, jiffies+TCP_TWKILL_PERIOD);
 	}
-	tcp_tw_death_row_slot =
-		((tcp_tw_death_row_slot + 1) & (TCP_TWKILL_SLOTS - 1));
 
-	if ((tcp_tw_count -= killed) != 0)
-		mod_timer(&tcp_tw_timer, jiffies+TCP_TWKILL_PERIOD);
 	net_statistics[smp_processor_id()*2].TimeWaited += killed;
 out:
 	spin_unlock(&tw_death_lock);
diff -uNrp linux-2.4.23/net/socket.c linux-2.4.23-fusion/net/socket.c
--- linux-2.4.23/net/socket.c	2003-11-28 23:18:58.000000000 +0100
+++ linux-2.4.23-fusion/net/socket.c	2004-01-11 15:29:38.000000000 +0100
@@ -132,7 +132,7 @@ static struct file_operations socket_fil
 
 static struct net_proto_family *net_families[NPROTO];
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 static atomic_t net_family_lockct = ATOMIC_INIT(0);
 static spinlock_t net_family_lock = SPIN_LOCK_UNLOCKED;
 
diff -uNrp linux-2.4.23/net/sunrpc/pmap_clnt.c linux-2.4.23-fusion/net/sunrpc/pmap_clnt.c
--- linux-2.4.23/net/sunrpc/pmap_clnt.c	2002-08-03 02:39:46.000000000 +0200
+++ linux-2.4.23-fusion/net/sunrpc/pmap_clnt.c	2004-01-11 15:29:38.000000000 +0100
@@ -12,6 +12,7 @@
 #include <linux/config.h>
 #include <linux/types.h>
 #include <linux/socket.h>
+#include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/uio.h>
